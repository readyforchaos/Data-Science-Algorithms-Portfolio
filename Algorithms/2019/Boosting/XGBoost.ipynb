{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"assets/raalabs.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv('data/customer_churn.csv')\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize area codes\n",
    "churn_df['Area Code'] = churn_df['Area Code'].apply(str)\n",
    "pd.get_dummies(churn_df['Area Code']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "## Transform true/false yes/no text into numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df['State'].value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the outcome\n",
    "churn_df['Churn?'] = np.where(churn_df['Churn?'] == 'True.', 1, 0)\n",
    "churn_df[\"Int'l Plan\"] = np.where(churn_df[\"Int'l Plan\"] == 'yes', 1, 0)\n",
    "churn_df['VMail Plan'] = np.where(churn_df['VMail Plan'] == 'yes', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummify states\n",
    "pd.get_dummies(churn_df['State']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize categorical columns\n",
    "churn_df = pd.concat([churn_df, pd.get_dummies(churn_df['State'])], axis=1)\n",
    "churn_df = pd.concat([churn_df, pd.get_dummies(churn_df['Area Code'])], axis=1)\n",
    "\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls in data and impute if necessary\n",
    "for feat in list(churn_df):\n",
    "     if (len(churn_df[feat]) - churn_df[feat].count()) > 0:\n",
    "         print(feat)\n",
    "         print(len(churn_df[feat]) - churn_df[feat].count())\n",
    "         tmp_df.loc[tmp_df[feat].isnull(), feat] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(churn_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feat for feat in list(churn_df) if feat not in ['State', 'Churn?', 'Phone', 'Area Code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'Churn?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create XGBoost Classification prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into test (70%) and train (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(churn_df, \n",
    "                                                 churn_df[outcome], \n",
    "                                                 test_size=0.3, \n",
    "                                                 random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select evaluation metric (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_params = {\n",
    "    'max_depth':3, \n",
    "    'eta':0.05, \n",
    "    'silent':0, \n",
    "    'eval_metric':'auc',\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective':'binary:logistic',\n",
    "    'seed' : 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast to matrix (XGBoost requirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train[features], y_train, feature_names = features)\n",
    "dtest = xgb.DMatrix(X_test[features], y_test, feature_names = features)\n",
    "evals = [(dtrain,'train'),(dtest,'eval')]\n",
    "xgb_model = xgb.train (params = xgb_params,\n",
    "              dtrain = dtrain,\n",
    "              num_boost_round = 2000,\n",
    "              verbose_eval=50, \n",
    "              early_stopping_rounds = 500,\n",
    "              evals=evals,\n",
    "              #feval = f1_score_cust,\n",
    "              maximize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Learn about AUC before continuing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validating...\")\n",
    "check = xgb_model.predict(xgb.DMatrix(X_test[features]), ntree_limit=xgb_model.best_iteration+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "#area under the precision-recall curve\n",
    "score = average_precision_score(X_test[outcome].values, check)\n",
    "print('area under the precision-recall curve: {:.6f}'.format(score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc,recall_score,precision_score\n",
    "check2=check.round()\n",
    "score = precision_score(X_test[outcome].values, check2)\n",
    "print('precision score: {:.6f}'.format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = recall_score(X_test[outcome].values, check2)\n",
    "print('recall score: {:.6f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predict test set... \")\n",
    "test_prediction = xgb_model.predict(xgb.DMatrix(X_test[features],missing = -99), ntree_limit=xgb_model.best_iteration+1)\n",
    "score = average_precision_score(X_test[outcome].values, test_prediction)\n",
    "\n",
    "print('area under the precision-recall curve test set: {:.6f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(X_test[outcome].values, check)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "#xgb.plot_importance(gbm)\n",
    "#plt.show()\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's not stop here, let's get some actionable insight out of this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,9))\n",
    "xgb.plot_importance(xgb_model,  height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spit feature importance into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe version of important feature for model \n",
    "xgb_fea_imp=pd.DataFrame(list(xgb_model.get_fscore().items()),\n",
    "columns=['feature','importance']).sort_values('importance', ascending=False)\n",
    "xgb_fea_imp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating top/bottom percentiles to determine under/over use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is customer under 75 percentile = not using service enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df['Day Mins'].quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df['Day Mins'].quantile(0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting number of customers who are going to churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_churn = xgb_model.predict(dtest)\n",
    "plt.plot(sorted(pred_churn))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop thorugh each numerical feature and get 25 and 75 percentile for each customer with high risk of churning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all numerical features\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric_features = list(X_test.head().select_dtypes(include=numerics))\n",
    "features_to_ignore = ['Account Length', 'Area Code','Churn?', 'Will_Churn']\n",
    "numeric_features = [nf for nf in numeric_features if nf not in features_to_ignore]\n",
    "\n",
    "row_counter = 0\n",
    "X_test['Will_Churn'] = pred_churn\n",
    "new_df = []\n",
    "for index, row in X_test.iterrows():\n",
    "    if row['Will_Churn'] > 0.8:\n",
    "        row_counter += 1\n",
    "        new_df.append(row[list(churn_df)])\n",
    "        for feat in numeric_features:\n",
    "            # only consider high prob churns\n",
    "            if row[feat] < X_test[feat].quantile(0.25):\n",
    "                print('(ID:', row_counter, ')', feat,  ' is < than 25 percentile')\n",
    "            if row[feat] > X_test[feat].quantile(0.75):\n",
    "                print('(ID:', row_counter, ')', feat,  ' is > than 75 percentile')\n",
    "\n",
    "\n",
    "new_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject one \"high risk\" customer into a cluster of \"low risk\" customers and see how it fits.\n",
    "#### Create a new dataframe and inject high risk customer to the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all known not to churn\n",
    "not_churn = X_train[X_train['Churn?']==False].copy()\n",
    "\n",
    "find_closet_df = []\n",
    "\n",
    "# add row to find insights\n",
    "find_closet_df.append(new_df[0])\n",
    "\n",
    "for index, row in not_churn.iterrows():\n",
    "    find_closet_df.append(row[list(churn_df)])\n",
    "    \n",
    "find_closet_df = pd.DataFrame(find_closet_df)\n",
    "find_closet_df['ID'] = [idx for idx in range(1,len(find_closet_df)+1)]\n",
    "find_closet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run KMeans unsupervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.brandidea.com/images/datascience/kmeansxmeans.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Closest Clusters to the Embedded Churn Risk\n",
    "#### Create 20 clusters and show me the closest customers to starting customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "num_clusters = 20\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(find_closet_df[features])\n",
    "labels = kmeans.labels_\n",
    "find_closet_df['clusters'] = labels\n",
    "find_closet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We compare the row with high-probability of churn against non-churns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find 13 rows of non-churn resembling row 0 with the high-probability of churn, thus we recommend offering day-time credits to this customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closet_df[find_closet_df['clusters']==6][features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get better understanding by plotting into a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_compare(cluster_df, cluster_number, var1, var2):\n",
    "    mydat = find_closet_df.copy()\n",
    "    mydat = mydat[mydat['clusters'] == cluster_number]\n",
    "    mydat = mydat[[var1, var2, 'clusters']]\n",
    "    # differentiate high-risk churn customer\n",
    "    mydat.iat[0, 2] = 0\n",
    "\n",
    "    sns.lmplot(var1, var2, data=mydat,\n",
    "               fit_reg=False, hue=\"clusters\", \n",
    "               scatter_kws={\"marker\": \"D\", \"s\": 100})\n",
    "\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass cluster number into function and what columns we want to cluster on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_compare(find_closet_df.copy(), 6, 'Night Mins', 'Night Calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_compare(find_closet_df.copy(), 6, 'Day Mins', 'Eve Mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
