{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data prep\n",
    "Data = [[[(i + j)/100]for i in range(5)] for j in range(100)]\n",
    "target = [(i+5)/100 for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(Data, dtype=float)\n",
    "target = np.array(target, dtype=float)\n",
    "data.shape\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0704 03:49:07.637147 28516 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0704 03:49:07.653107 28516 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0704 03:49:07.656123 28516 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RNN Model\n",
    "model = Sequential()\n",
    "model.add(LSTM((1), batch_input_shape=(None, None, 1), return_sequences=True)) \n",
    "#input (NONE = you don't know input size, batch size 1)\n",
    "model.add(LSTM((1),return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0704 03:49:13.519283 28516 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 1)           12        \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 24\n",
      "Trainable params: 24\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"MAE\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0704 03:49:24.425247 28516 deprecation.py:323] From C:\\Users\\gill\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0704 03:49:25.277962 28516 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0704 03:49:25.345751 28516 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/400\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.5524 - acc: 0.0000e+00 - val_loss: 0.4432 - val_acc: 0.0000e+00\n",
      "Epoch 2/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5485 - acc: 0.0000e+00 - val_loss: 0.4395 - val_acc: 0.0000e+00\n",
      "Epoch 3/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5446 - acc: 0.0000e+00 - val_loss: 0.4357 - val_acc: 0.0000e+00\n",
      "Epoch 4/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5406 - acc: 0.0000e+00 - val_loss: 0.4318 - val_acc: 0.0000e+00\n",
      "Epoch 5/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.5365 - acc: 0.0000e+00 - val_loss: 0.4278 - val_acc: 0.0000e+00\n",
      "Epoch 6/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5325 - acc: 0.0000e+00 - val_loss: 0.4238 - val_acc: 0.0000e+00\n",
      "Epoch 7/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5283 - acc: 0.0000e+00 - val_loss: 0.4197 - val_acc: 0.0000e+00\n",
      "Epoch 8/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5239 - acc: 0.0000e+00 - val_loss: 0.4156 - val_acc: 0.0000e+00\n",
      "Epoch 9/400\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.5196 - acc: 0.0000e+00 - val_loss: 0.4113 - val_acc: 0.0000e+00\n",
      "Epoch 10/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5151 - acc: 0.0000e+00 - val_loss: 0.4070 - val_acc: 0.0000e+00\n",
      "Epoch 11/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.5106 - acc: 0.0000e+00 - val_loss: 0.4025 - val_acc: 0.0000e+00\n",
      "Epoch 12/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.5059 - acc: 0.0000e+00 - val_loss: 0.3980 - val_acc: 0.0000e+00\n",
      "Epoch 13/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5012 - acc: 0.0000e+00 - val_loss: 0.3934 - val_acc: 0.0000e+00\n",
      "Epoch 14/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4963 - acc: 0.0000e+00 - val_loss: 0.3886 - val_acc: 0.0000e+00\n",
      "Epoch 15/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.4913 - acc: 0.0000e+00 - val_loss: 0.3838 - val_acc: 0.0000e+00\n",
      "Epoch 16/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4864 - acc: 0.0000e+00 - val_loss: 0.3789 - val_acc: 0.0000e+00\n",
      "Epoch 17/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.4813 - acc: 0.0000e+00 - val_loss: 0.3739 - val_acc: 0.0000e+00\n",
      "Epoch 18/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4763 - acc: 0.0000e+00 - val_loss: 0.3689 - val_acc: 0.0000e+00\n",
      "Epoch 19/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4712 - acc: 0.0000e+00 - val_loss: 0.3638 - val_acc: 0.0000e+00\n",
      "Epoch 20/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.4660 - acc: 0.0000e+00 - val_loss: 0.3590 - val_acc: 0.0000e+00\n",
      "Epoch 21/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4606 - acc: 0.0000e+00 - val_loss: 0.3541 - val_acc: 0.0000e+00\n",
      "Epoch 22/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.4553 - acc: 0.0000e+00 - val_loss: 0.3491 - val_acc: 0.0000e+00\n",
      "Epoch 23/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4499 - acc: 0.0000e+00 - val_loss: 0.3440 - val_acc: 0.0000e+00\n",
      "Epoch 24/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.4444 - acc: 0.0000e+00 - val_loss: 0.3389 - val_acc: 0.0000e+00\n",
      "Epoch 25/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.4390 - acc: 0.0000e+00 - val_loss: 0.3337 - val_acc: 0.0000e+00\n",
      "Epoch 26/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4336 - acc: 0.0000e+00 - val_loss: 0.3284 - val_acc: 0.0000e+00\n",
      "Epoch 27/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.4282 - acc: 0.0000e+00 - val_loss: 0.3231 - val_acc: 0.0000e+00\n",
      "Epoch 28/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.4228 - acc: 0.0000e+00 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 29/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.4173 - acc: 0.0000e+00 - val_loss: 0.3124 - val_acc: 0.0000e+00\n",
      "Epoch 30/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4121 - acc: 0.0000e+00 - val_loss: 0.3069 - val_acc: 0.0000e+00\n",
      "Epoch 31/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.4066 - acc: 0.0000e+00 - val_loss: 0.3014 - val_acc: 0.0000e+00\n",
      "Epoch 32/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.4015 - acc: 0.0000e+00 - val_loss: 0.2959 - val_acc: 0.0000e+00\n",
      "Epoch 33/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.3962 - acc: 0.0000e+00 - val_loss: 0.2904 - val_acc: 0.0000e+00\n",
      "Epoch 34/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3910 - acc: 0.0000e+00 - val_loss: 0.2849 - val_acc: 0.0000e+00\n",
      "Epoch 35/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3859 - acc: 0.0000e+00 - val_loss: 0.2799 - val_acc: 0.0000e+00\n",
      "Epoch 36/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.3806 - acc: 0.0000e+00 - val_loss: 0.2748 - val_acc: 0.0000e+00\n",
      "Epoch 37/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3756 - acc: 0.0000e+00 - val_loss: 0.2697 - val_acc: 0.0000e+00\n",
      "Epoch 38/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3706 - acc: 0.0000e+00 - val_loss: 0.2646 - val_acc: 0.0000e+00\n",
      "Epoch 39/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3656 - acc: 0.0000e+00 - val_loss: 0.2596 - val_acc: 0.0000e+00\n",
      "Epoch 40/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3608 - acc: 0.0000e+00 - val_loss: 0.2551 - val_acc: 0.0000e+00\n",
      "Epoch 41/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.3558 - acc: 0.0000e+00 - val_loss: 0.2508 - val_acc: 0.0000e+00\n",
      "Epoch 42/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.3509 - acc: 0.0000e+00 - val_loss: 0.2468 - val_acc: 0.0000e+00\n",
      "Epoch 43/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.3461 - acc: 0.0000e+00 - val_loss: 0.2428 - val_acc: 0.0000e+00\n",
      "Epoch 44/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3412 - acc: 0.0000e+00 - val_loss: 0.2390 - val_acc: 0.0000e+00\n",
      "Epoch 45/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3365 - acc: 0.0000e+00 - val_loss: 0.2355 - val_acc: 0.0000e+00\n",
      "Epoch 46/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3316 - acc: 0.0000e+00 - val_loss: 0.2321 - val_acc: 0.0000e+00\n",
      "Epoch 47/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3272 - acc: 0.0000e+00 - val_loss: 0.2287 - val_acc: 0.0000e+00\n",
      "Epoch 48/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.3224 - acc: 0.0000e+00 - val_loss: 0.2254 - val_acc: 0.0000e+00\n",
      "Epoch 49/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3183 - acc: 0.0000e+00 - val_loss: 0.2220 - val_acc: 0.0000e+00\n",
      "Epoch 50/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3141 - acc: 0.0000e+00 - val_loss: 0.2188 - val_acc: 0.0000e+00\n",
      "Epoch 51/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3098 - acc: 0.0000e+00 - val_loss: 0.2161 - val_acc: 0.0000e+00\n",
      "Epoch 52/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3059 - acc: 0.0000e+00 - val_loss: 0.2135 - val_acc: 0.0000e+00\n",
      "Epoch 53/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3019 - acc: 0.0000e+00 - val_loss: 0.2109 - val_acc: 0.0000e+00\n",
      "Epoch 54/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2982 - acc: 0.0000e+00 - val_loss: 0.2083 - val_acc: 0.0000e+00\n",
      "Epoch 55/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2949 - acc: 0.0000e+00 - val_loss: 0.2058 - val_acc: 0.0000e+00\n",
      "Epoch 56/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2912 - acc: 0.0000e+00 - val_loss: 0.2035 - val_acc: 0.0000e+00\n",
      "Epoch 57/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2882 - acc: 0.0000e+00 - val_loss: 0.2017 - val_acc: 0.0000e+00\n",
      "Epoch 58/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.2849 - acc: 0.0000e+00 - val_loss: 0.1999 - val_acc: 0.0000e+00\n",
      "Epoch 59/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2819 - acc: 0.0000e+00 - val_loss: 0.1981 - val_acc: 0.0000e+00\n",
      "Epoch 60/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2790 - acc: 0.0000e+00 - val_loss: 0.1967 - val_acc: 0.0000e+00\n",
      "Epoch 61/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2762 - acc: 0.0000e+00 - val_loss: 0.1955 - val_acc: 0.0000e+00\n",
      "Epoch 62/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2732 - acc: 0.0000e+00 - val_loss: 0.1942 - val_acc: 0.0000e+00\n",
      "Epoch 63/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2705 - acc: 0.0000e+00 - val_loss: 0.1931 - val_acc: 0.0000e+00\n",
      "Epoch 64/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2681 - acc: 0.0000e+00 - val_loss: 0.1919 - val_acc: 0.0000e+00\n",
      "Epoch 65/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2657 - acc: 0.0000e+00 - val_loss: 0.1910 - val_acc: 0.0000e+00\n",
      "Epoch 66/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2634 - acc: 0.0000e+00 - val_loss: 0.1903 - val_acc: 0.0000e+00\n",
      "Epoch 67/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2611 - acc: 0.0000e+00 - val_loss: 0.1896 - val_acc: 0.0000e+00\n",
      "Epoch 68/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2588 - acc: 0.0000e+00 - val_loss: 0.1890 - val_acc: 0.0000e+00\n",
      "Epoch 69/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2568 - acc: 0.0000e+00 - val_loss: 0.1883 - val_acc: 0.0000e+00\n",
      "Epoch 70/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2548 - acc: 0.0000e+00 - val_loss: 0.1877 - val_acc: 0.0000e+00\n",
      "Epoch 71/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2529 - acc: 0.0000e+00 - val_loss: 0.1870 - val_acc: 0.0000e+00\n",
      "Epoch 72/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.2512 - acc: 0.0000e+00 - val_loss: 0.1864 - val_acc: 0.0000e+00\n",
      "Epoch 73/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2493 - acc: 0.0000e+00 - val_loss: 0.1858 - val_acc: 0.0000e+00\n",
      "Epoch 74/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2476 - acc: 0.0000e+00 - val_loss: 0.1852 - val_acc: 0.0000e+00\n",
      "Epoch 75/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2462 - acc: 0.0000e+00 - val_loss: 0.1845 - val_acc: 0.0000e+00\n",
      "Epoch 76/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2446 - acc: 0.0000e+00 - val_loss: 0.1839 - val_acc: 0.0000e+00\n",
      "Epoch 77/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2431 - acc: 0.0000e+00 - val_loss: 0.1833 - val_acc: 0.0000e+00\n",
      "Epoch 78/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2418 - acc: 0.0000e+00 - val_loss: 0.1828 - val_acc: 0.0000e+00\n",
      "Epoch 79/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2402 - acc: 0.0000e+00 - val_loss: 0.1825 - val_acc: 0.0000e+00\n",
      "Epoch 80/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2389 - acc: 0.0000e+00 - val_loss: 0.1822 - val_acc: 0.0000e+00\n",
      "Epoch 81/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2378 - acc: 0.0000e+00 - val_loss: 0.1819 - val_acc: 0.0000e+00\n",
      "Epoch 82/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2363 - acc: 0.0000e+00 - val_loss: 0.1815 - val_acc: 0.0000e+00\n",
      "Epoch 83/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2350 - acc: 0.0000e+00 - val_loss: 0.1812 - val_acc: 0.0000e+00\n",
      "Epoch 84/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2338 - acc: 0.0000e+00 - val_loss: 0.1810 - val_acc: 0.0000e+00\n",
      "Epoch 85/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2326 - acc: 0.0000e+00 - val_loss: 0.1809 - val_acc: 0.0000e+00\n",
      "Epoch 86/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2313 - acc: 0.0000e+00 - val_loss: 0.1808 - val_acc: 0.0000e+00\n",
      "Epoch 87/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2302 - acc: 0.0000e+00 - val_loss: 0.1806 - val_acc: 0.0000e+00\n",
      "Epoch 88/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2290 - acc: 0.0000e+00 - val_loss: 0.1804 - val_acc: 0.0000e+00\n",
      "Epoch 89/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2278 - acc: 0.0000e+00 - val_loss: 0.1802 - val_acc: 0.0500\n",
      "Epoch 90/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2268 - acc: 0.0000e+00 - val_loss: 0.1799 - val_acc: 0.0500\n",
      "Epoch 91/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2258 - acc: 0.0000e+00 - val_loss: 0.1797 - val_acc: 0.0500\n",
      "Epoch 92/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2247 - acc: 0.0000e+00 - val_loss: 0.1794 - val_acc: 0.0500\n",
      "Epoch 93/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2236 - acc: 0.0000e+00 - val_loss: 0.1791 - val_acc: 0.0500\n",
      "Epoch 94/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2227 - acc: 0.0000e+00 - val_loss: 0.1787 - val_acc: 0.0500\n",
      "Epoch 95/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2216 - acc: 0.0000e+00 - val_loss: 0.1784 - val_acc: 0.0500\n",
      "Epoch 96/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2206 - acc: 0.0000e+00 - val_loss: 0.1780 - val_acc: 0.0500\n",
      "Epoch 97/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2196 - acc: 0.0000e+00 - val_loss: 0.1777 - val_acc: 0.0500\n",
      "Epoch 98/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.2186 - acc: 0.0000e+00 - val_loss: 0.1773 - val_acc: 0.0500\n",
      "Epoch 99/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2176 - acc: 0.0000e+00 - val_loss: 0.1768 - val_acc: 0.0500\n",
      "Epoch 100/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2166 - acc: 0.0000e+00 - val_loss: 0.1765 - val_acc: 0.0500\n",
      "Epoch 101/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2156 - acc: 0.0000e+00 - val_loss: 0.1762 - val_acc: 0.0500\n",
      "Epoch 102/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2145 - acc: 0.0000e+00 - val_loss: 0.1757 - val_acc: 0.0500\n",
      "Epoch 103/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2136 - acc: 0.0000e+00 - val_loss: 0.1753 - val_acc: 0.0500\n",
      "Epoch 104/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2124 - acc: 0.0000e+00 - val_loss: 0.1748 - val_acc: 0.0500\n",
      "Epoch 105/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2114 - acc: 0.0000e+00 - val_loss: 0.1744 - val_acc: 0.0500\n",
      "Epoch 106/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2103 - acc: 0.0000e+00 - val_loss: 0.1739 - val_acc: 0.0500\n",
      "Epoch 107/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2093 - acc: 0.0000e+00 - val_loss: 0.1734 - val_acc: 0.0500\n",
      "Epoch 108/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2082 - acc: 0.0000e+00 - val_loss: 0.1729 - val_acc: 0.0500\n",
      "Epoch 109/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2072 - acc: 0.0000e+00 - val_loss: 0.1723 - val_acc: 0.0500\n",
      "Epoch 110/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2061 - acc: 0.0000e+00 - val_loss: 0.1716 - val_acc: 0.0500\n",
      "Epoch 111/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2050 - acc: 0.0000e+00 - val_loss: 0.1709 - val_acc: 0.0500\n",
      "Epoch 112/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2039 - acc: 0.0000e+00 - val_loss: 0.1701 - val_acc: 0.0500\n",
      "Epoch 113/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2028 - acc: 0.0000e+00 - val_loss: 0.1693 - val_acc: 0.0500\n",
      "Epoch 114/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2017 - acc: 0.0000e+00 - val_loss: 0.1686 - val_acc: 0.0500\n",
      "Epoch 115/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2005 - acc: 0.0000e+00 - val_loss: 0.1678 - val_acc: 0.0500\n",
      "Epoch 116/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1994 - acc: 0.0000e+00 - val_loss: 0.1671 - val_acc: 0.0500\n",
      "Epoch 117/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1982 - acc: 0.0000e+00 - val_loss: 0.1664 - val_acc: 0.0500\n",
      "Epoch 118/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1970 - acc: 0.0000e+00 - val_loss: 0.1655 - val_acc: 0.0500\n",
      "Epoch 119/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1958 - acc: 0.0000e+00 - val_loss: 0.1647 - val_acc: 0.0500\n",
      "Epoch 120/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1945 - acc: 0.0000e+00 - val_loss: 0.1640 - val_acc: 0.0500\n",
      "Epoch 121/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1933 - acc: 0.0000e+00 - val_loss: 0.1632 - val_acc: 0.0500\n",
      "Epoch 122/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1920 - acc: 0.0000e+00 - val_loss: 0.1622 - val_acc: 0.0500\n",
      "Epoch 123/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1907 - acc: 0.0000e+00 - val_loss: 0.1614 - val_acc: 0.0500\n",
      "Epoch 124/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.1894 - acc: 0.0000e+00 - val_loss: 0.1603 - val_acc: 0.0500\n",
      "Epoch 125/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1880 - acc: 0.0000e+00 - val_loss: 0.1592 - val_acc: 0.0500\n",
      "Epoch 126/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1867 - acc: 0.0000e+00 - val_loss: 0.1580 - val_acc: 0.0500\n",
      "Epoch 127/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1853 - acc: 0.0000e+00 - val_loss: 0.1568 - val_acc: 0.0500\n",
      "Epoch 128/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.1839 - acc: 0.0000e+00 - val_loss: 0.1556 - val_acc: 0.0500\n",
      "Epoch 129/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1825 - acc: 0.0000e+00 - val_loss: 0.1542 - val_acc: 0.0500\n",
      "Epoch 130/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1810 - acc: 0.0000e+00 - val_loss: 0.1528 - val_acc: 0.0500\n",
      "Epoch 131/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1796 - acc: 0.0000e+00 - val_loss: 0.1516 - val_acc: 0.0500\n",
      "Epoch 132/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1780 - acc: 0.0000e+00 - val_loss: 0.1502 - val_acc: 0.0500\n",
      "Epoch 133/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1765 - acc: 0.0000e+00 - val_loss: 0.1486 - val_acc: 0.0500\n",
      "Epoch 134/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1749 - acc: 0.0000e+00 - val_loss: 0.1470 - val_acc: 0.0500\n",
      "Epoch 135/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1733 - acc: 0.0000e+00 - val_loss: 0.1455 - val_acc: 0.0500\n",
      "Epoch 136/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1716 - acc: 0.0000e+00 - val_loss: 0.1437 - val_acc: 0.0500\n",
      "Epoch 137/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1700 - acc: 0.0000e+00 - val_loss: 0.1419 - val_acc: 0.0500\n",
      "Epoch 138/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0500\n",
      "Epoch 139/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1380 - val_acc: 0.0500\n",
      "Epoch 140/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1646 - acc: 0.0000e+00 - val_loss: 0.1359 - val_acc: 0.0500\n",
      "Epoch 141/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1627 - acc: 0.0000e+00 - val_loss: 0.1340 - val_acc: 0.0500\n",
      "Epoch 142/400\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.1608 - acc: 0.0000e+00 - val_loss: 0.1319 - val_acc: 0.0500\n",
      "Epoch 143/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1588 - acc: 0.0000e+00 - val_loss: 0.1298 - val_acc: 0.0500\n",
      "Epoch 144/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1568 - acc: 0.0000e+00 - val_loss: 0.1277 - val_acc: 0.0500\n",
      "Epoch 145/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1547 - acc: 0.0000e+00 - val_loss: 0.1256 - val_acc: 0.0500\n",
      "Epoch 146/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1526 - acc: 0.0000e+00 - val_loss: 0.1234 - val_acc: 0.0500\n",
      "Epoch 147/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1505 - acc: 0.0000e+00 - val_loss: 0.1208 - val_acc: 0.0500\n",
      "Epoch 148/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1482 - acc: 0.0000e+00 - val_loss: 0.1182 - val_acc: 0.0500\n",
      "Epoch 149/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1460 - acc: 0.0000e+00 - val_loss: 0.1156 - val_acc: 0.0500\n",
      "Epoch 150/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1436 - acc: 0.0000e+00 - val_loss: 0.1129 - val_acc: 0.0500\n",
      "Epoch 151/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1413 - acc: 0.0000e+00 - val_loss: 0.1101 - val_acc: 0.0500\n",
      "Epoch 152/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1388 - acc: 0.0000e+00 - val_loss: 0.1075 - val_acc: 0.0500\n",
      "Epoch 153/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1363 - acc: 0.0000e+00 - val_loss: 0.1046 - val_acc: 0.0500\n",
      "Epoch 154/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1336 - acc: 0.0000e+00 - val_loss: 0.1017 - val_acc: 0.0500\n",
      "Epoch 155/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1310 - acc: 0.0000e+00 - val_loss: 0.0988 - val_acc: 0.0500\n",
      "Epoch 156/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1283 - acc: 0.0000e+00 - val_loss: 0.0959 - val_acc: 0.0500\n",
      "Epoch 157/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1254 - acc: 0.0000e+00 - val_loss: 0.0928 - val_acc: 0.0500\n",
      "Epoch 158/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1225 - acc: 0.0000e+00 - val_loss: 0.0895 - val_acc: 0.0500\n",
      "Epoch 159/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1195 - acc: 0.0000e+00 - val_loss: 0.0861 - val_acc: 0.0500\n",
      "Epoch 160/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.1166 - acc: 0.0000e+00 - val_loss: 0.0826 - val_acc: 0.0500\n",
      "Epoch 161/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1133 - acc: 0.0000e+00 - val_loss: 0.0792 - val_acc: 0.0500\n",
      "Epoch 162/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1101 - acc: 0.0000e+00 - val_loss: 0.0757 - val_acc: 0.0500\n",
      "Epoch 163/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1069 - acc: 0.0000e+00 - val_loss: 0.0723 - val_acc: 0.0500\n",
      "Epoch 164/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1035 - acc: 0.0000e+00 - val_loss: 0.0689 - val_acc: 0.0500\n",
      "Epoch 165/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1000 - acc: 0.0000e+00 - val_loss: 0.0656 - val_acc: 0.0500\n",
      "Epoch 166/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0964 - acc: 0.0000e+00 - val_loss: 0.0622 - val_acc: 0.0500\n",
      "Epoch 167/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0930 - acc: 0.0000e+00 - val_loss: 0.0598 - val_acc: 0.0500\n",
      "Epoch 168/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0896 - acc: 0.0000e+00 - val_loss: 0.0583 - val_acc: 0.0500\n",
      "Epoch 169/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0871 - acc: 0.0000e+00 - val_loss: 0.0573 - val_acc: 0.0500\n",
      "Epoch 170/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0846 - acc: 0.0000e+00 - val_loss: 0.0564 - val_acc: 0.0500\n",
      "Epoch 171/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.0825 - acc: 0.0000e+00 - val_loss: 0.0561 - val_acc: 0.0500\n",
      "Epoch 172/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0809 - acc: 0.0000e+00 - val_loss: 0.0560 - val_acc: 0.0500\n",
      "Epoch 173/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0796 - acc: 0.0000e+00 - val_loss: 0.0562 - val_acc: 0.0500\n",
      "Epoch 174/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0781 - acc: 0.0000e+00 - val_loss: 0.0566 - val_acc: 0.0500\n",
      "Epoch 175/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0769 - acc: 0.0000e+00 - val_loss: 0.0570 - val_acc: 0.0500\n",
      "Epoch 176/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0758 - acc: 0.0000e+00 - val_loss: 0.0573 - val_acc: 0.0500\n",
      "Epoch 177/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0748 - acc: 0.0000e+00 - val_loss: 0.0575 - val_acc: 0.0500\n",
      "Epoch 178/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0738 - acc: 0.0000e+00 - val_loss: 0.0576 - val_acc: 0.0500\n",
      "Epoch 179/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0729 - acc: 0.0000e+00 - val_loss: 0.0576 - val_acc: 0.0500\n",
      "Epoch 180/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0721 - acc: 0.0000e+00 - val_loss: 0.0576 - val_acc: 0.0500\n",
      "Epoch 181/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0713 - acc: 0.0000e+00 - val_loss: 0.0575 - val_acc: 0.0500\n",
      "Epoch 182/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0705 - acc: 0.0000e+00 - val_loss: 0.0574 - val_acc: 0.0500\n",
      "Epoch 183/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0698 - acc: 0.0000e+00 - val_loss: 0.0572 - val_acc: 0.0500\n",
      "Epoch 184/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0691 - acc: 0.0000e+00 - val_loss: 0.0569 - val_acc: 0.0500\n",
      "Epoch 185/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0683 - acc: 0.0000e+00 - val_loss: 0.0566 - val_acc: 0.0500\n",
      "Epoch 186/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0675 - acc: 0.0000e+00 - val_loss: 0.0564 - val_acc: 0.0500\n",
      "Epoch 187/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0668 - acc: 0.0000e+00 - val_loss: 0.0562 - val_acc: 0.0500\n",
      "Epoch 188/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0661 - acc: 0.0000e+00 - val_loss: 0.0560 - val_acc: 0.0500\n",
      "Epoch 189/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0655 - acc: 0.0000e+00 - val_loss: 0.0556 - val_acc: 0.0500\n",
      "Epoch 190/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0649 - acc: 0.0000e+00 - val_loss: 0.0552 - val_acc: 0.0500\n",
      "Epoch 191/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0641 - acc: 0.0000e+00 - val_loss: 0.0548 - val_acc: 0.0500\n",
      "Epoch 192/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0635 - acc: 0.0000e+00 - val_loss: 0.0544 - val_acc: 0.0500\n",
      "Epoch 193/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0629 - acc: 0.0000e+00 - val_loss: 0.0539 - val_acc: 0.0500\n",
      "Epoch 194/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0623 - acc: 0.0000e+00 - val_loss: 0.0535 - val_acc: 0.0500\n",
      "Epoch 195/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0615 - acc: 0.0000e+00 - val_loss: 0.0530 - val_acc: 0.0500\n",
      "Epoch 196/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0610 - acc: 0.0000e+00 - val_loss: 0.0525 - val_acc: 0.0500\n",
      "Epoch 197/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0605 - acc: 0.0000e+00 - val_loss: 0.0521 - val_acc: 0.0500\n",
      "Epoch 198/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0600 - acc: 0.0000e+00 - val_loss: 0.0519 - val_acc: 0.0500\n",
      "Epoch 199/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0594 - acc: 0.0000e+00 - val_loss: 0.0517 - val_acc: 0.0500\n",
      "Epoch 200/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0587 - acc: 0.0000e+00 - val_loss: 0.0515 - val_acc: 0.0500\n",
      "Epoch 201/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0582 - acc: 0.0000e+00 - val_loss: 0.0512 - val_acc: 0.0500\n",
      "Epoch 202/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0576 - acc: 0.0000e+00 - val_loss: 0.0508 - val_acc: 0.0500\n",
      "Epoch 203/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0570 - acc: 0.0000e+00 - val_loss: 0.0504 - val_acc: 0.0500\n",
      "Epoch 204/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0565 - acc: 0.0000e+00 - val_loss: 0.0500 - val_acc: 0.0500\n",
      "Epoch 205/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0560 - acc: 0.0000e+00 - val_loss: 0.0495 - val_acc: 0.0500\n",
      "Epoch 206/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0555 - acc: 0.0000e+00 - val_loss: 0.0491 - val_acc: 0.0500\n",
      "Epoch 207/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0550 - acc: 0.0000e+00 - val_loss: 0.0489 - val_acc: 0.0500\n",
      "Epoch 208/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0544 - acc: 0.0000e+00 - val_loss: 0.0487 - val_acc: 0.0500\n",
      "Epoch 209/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0539 - acc: 0.0000e+00 - val_loss: 0.0485 - val_acc: 0.0500\n",
      "Epoch 210/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0534 - acc: 0.0000e+00 - val_loss: 0.0482 - val_acc: 0.0500\n",
      "Epoch 211/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0531 - acc: 0.0000e+00 - val_loss: 0.0481 - val_acc: 0.0500\n",
      "Epoch 212/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0525 - acc: 0.0000e+00 - val_loss: 0.0479 - val_acc: 0.0500\n",
      "Epoch 213/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0519 - acc: 0.0000e+00 - val_loss: 0.0478 - val_acc: 0.0500\n",
      "Epoch 214/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0515 - acc: 0.0000e+00 - val_loss: 0.0476 - val_acc: 0.0500\n",
      "Epoch 215/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0511 - acc: 0.0000e+00 - val_loss: 0.0475 - val_acc: 0.0500\n",
      "Epoch 216/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0505 - acc: 0.0000e+00 - val_loss: 0.0473 - val_acc: 0.0500\n",
      "Epoch 217/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0501 - acc: 0.0000e+00 - val_loss: 0.0471 - val_acc: 0.0500\n",
      "Epoch 218/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0496 - acc: 0.0000e+00 - val_loss: 0.0468 - val_acc: 0.0500\n",
      "Epoch 219/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0493 - acc: 0.0000e+00 - val_loss: 0.0466 - val_acc: 0.0500\n",
      "Epoch 220/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0489 - acc: 0.0000e+00 - val_loss: 0.0462 - val_acc: 0.0500\n",
      "Epoch 221/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0485 - acc: 0.0000e+00 - val_loss: 0.0460 - val_acc: 0.0500\n",
      "Epoch 222/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0480 - acc: 0.0000e+00 - val_loss: 0.0458 - val_acc: 0.0500\n",
      "Epoch 223/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0476 - acc: 0.0000e+00 - val_loss: 0.0455 - val_acc: 0.0500\n",
      "Epoch 224/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0472 - acc: 0.0000e+00 - val_loss: 0.0453 - val_acc: 0.0500\n",
      "Epoch 225/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0467 - acc: 0.0000e+00 - val_loss: 0.0450 - val_acc: 0.0500\n",
      "Epoch 226/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0463 - acc: 0.0000e+00 - val_loss: 0.0446 - val_acc: 0.0500\n",
      "Epoch 227/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0460 - acc: 0.0000e+00 - val_loss: 0.0443 - val_acc: 0.0500\n",
      "Epoch 228/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0456 - acc: 0.0000e+00 - val_loss: 0.0438 - val_acc: 0.0500\n",
      "Epoch 229/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0453 - acc: 0.0000e+00 - val_loss: 0.0435 - val_acc: 0.0500\n",
      "Epoch 230/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0449 - acc: 0.0000e+00 - val_loss: 0.0432 - val_acc: 0.0500\n",
      "Epoch 231/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0445 - acc: 0.0000e+00 - val_loss: 0.0429 - val_acc: 0.0500\n",
      "Epoch 232/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0441 - acc: 0.0000e+00 - val_loss: 0.0426 - val_acc: 0.0500\n",
      "Epoch 233/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0438 - acc: 0.0000e+00 - val_loss: 0.0424 - val_acc: 0.0500\n",
      "Epoch 234/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0435 - acc: 0.0000e+00 - val_loss: 0.0421 - val_acc: 0.0500\n",
      "Epoch 235/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0431 - acc: 0.0000e+00 - val_loss: 0.0418 - val_acc: 0.0500\n",
      "Epoch 236/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0428 - acc: 0.0000e+00 - val_loss: 0.0416 - val_acc: 0.0500\n",
      "Epoch 237/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0425 - acc: 0.0000e+00 - val_loss: 0.0414 - val_acc: 0.0500\n",
      "Epoch 238/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0421 - acc: 0.0000e+00 - val_loss: 0.0411 - val_acc: 0.0500\n",
      "Epoch 239/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0418 - acc: 0.0000e+00 - val_loss: 0.0409 - val_acc: 0.0500\n",
      "Epoch 240/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0415 - acc: 0.0000e+00 - val_loss: 0.0406 - val_acc: 0.0500\n",
      "Epoch 241/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0411 - acc: 0.0000e+00 - val_loss: 0.0403 - val_acc: 0.0500\n",
      "Epoch 242/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0409 - acc: 0.0000e+00 - val_loss: 0.0401 - val_acc: 0.0500\n",
      "Epoch 243/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0406 - acc: 0.0000e+00 - val_loss: 0.0398 - val_acc: 0.0500\n",
      "Epoch 244/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0402 - acc: 0.0000e+00 - val_loss: 0.0396 - val_acc: 0.0500\n",
      "Epoch 245/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0399 - acc: 0.0000e+00 - val_loss: 0.0394 - val_acc: 0.0500\n",
      "Epoch 246/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0397 - acc: 0.0000e+00 - val_loss: 0.0392 - val_acc: 0.0500\n",
      "Epoch 247/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0394 - acc: 0.0000e+00 - val_loss: 0.0389 - val_acc: 0.0500\n",
      "Epoch 248/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0392 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0500\n",
      "Epoch 249/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0388 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0500\n",
      "Epoch 250/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0386 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0500\n",
      "Epoch 251/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0383 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0500\n",
      "Epoch 252/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0381 - acc: 0.0000e+00 - val_loss: 0.0384 - val_acc: 0.0500\n",
      "Epoch 253/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0377 - acc: 0.0000e+00 - val_loss: 0.0382 - val_acc: 0.0500\n",
      "Epoch 254/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0375 - acc: 0.0000e+00 - val_loss: 0.0379 - val_acc: 0.0500\n",
      "Epoch 255/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0373 - acc: 0.0000e+00 - val_loss: 0.0377 - val_acc: 0.0500\n",
      "Epoch 256/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0370 - acc: 0.0000e+00 - val_loss: 0.0375 - val_acc: 0.0500\n",
      "Epoch 257/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0368 - acc: 0.0000e+00 - val_loss: 0.0375 - val_acc: 0.0500\n",
      "Epoch 258/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0365 - acc: 0.0000e+00 - val_loss: 0.0374 - val_acc: 0.0500\n",
      "Epoch 259/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.0362 - acc: 0.0000e+00 - val_loss: 0.0373 - val_acc: 0.0500\n",
      "Epoch 260/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.0360 - acc: 0.0000e+00 - val_loss: 0.0370 - val_acc: 0.0500\n",
      "Epoch 261/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0358 - acc: 0.0000e+00 - val_loss: 0.0367 - val_acc: 0.0500\n",
      "Epoch 262/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0356 - acc: 0.0000e+00 - val_loss: 0.0366 - val_acc: 0.0500\n",
      "Epoch 263/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0354 - acc: 0.0000e+00 - val_loss: 0.0363 - val_acc: 0.0500\n",
      "Epoch 264/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0352 - acc: 0.0000e+00 - val_loss: 0.0360 - val_acc: 0.0500\n",
      "Epoch 265/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0350 - acc: 0.0000e+00 - val_loss: 0.0360 - val_acc: 0.0500\n",
      "Epoch 266/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0347 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0500\n",
      "Epoch 267/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0345 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0500\n",
      "Epoch 268/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0343 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0500\n",
      "Epoch 269/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0343 - acc: 0.0000e+00 - val_loss: 0.0362 - val_acc: 0.0500\n",
      "Epoch 270/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0340 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0500\n",
      "Epoch 271/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0337 - acc: 0.0000e+00 - val_loss: 0.0356 - val_acc: 0.0500\n",
      "Epoch 272/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0335 - acc: 0.0000e+00 - val_loss: 0.0351 - val_acc: 0.0500\n",
      "Epoch 273/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0333 - acc: 0.0000e+00 - val_loss: 0.0348 - val_acc: 0.0500\n",
      "Epoch 274/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0331 - acc: 0.0000e+00 - val_loss: 0.0348 - val_acc: 0.0500\n",
      "Epoch 275/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0330 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0500\n",
      "Epoch 276/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.0327 - acc: 0.0000e+00 - val_loss: 0.0345 - val_acc: 0.0500\n",
      "Epoch 277/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0327 - acc: 0.0000e+00 - val_loss: 0.0345 - val_acc: 0.0500\n",
      "Epoch 278/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0324 - acc: 0.0000e+00 - val_loss: 0.0341 - val_acc: 0.0500\n",
      "Epoch 279/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0322 - acc: 0.0000e+00 - val_loss: 0.0338 - val_acc: 0.0500\n",
      "Epoch 280/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0320 - acc: 0.0000e+00 - val_loss: 0.0338 - val_acc: 0.0500\n",
      "Epoch 281/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0318 - acc: 0.0000e+00 - val_loss: 0.0340 - val_acc: 0.0500\n",
      "Epoch 282/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0317 - acc: 0.0000e+00 - val_loss: 0.0344 - val_acc: 0.0500\n",
      "Epoch 283/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0316 - acc: 0.0000e+00 - val_loss: 0.0343 - val_acc: 0.0500\n",
      "Epoch 284/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0314 - acc: 0.0000e+00 - val_loss: 0.0344 - val_acc: 0.0500\n",
      "Epoch 285/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0313 - acc: 0.0000e+00 - val_loss: 0.0341 - val_acc: 0.0500\n",
      "Epoch 286/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0312 - acc: 0.0000e+00 - val_loss: 0.0333 - val_acc: 0.0500\n",
      "Epoch 287/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0309 - acc: 0.0000e+00 - val_loss: 0.0330 - val_acc: 0.0500\n",
      "Epoch 288/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0307 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0500\n",
      "Epoch 289/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0307 - acc: 0.0000e+00 - val_loss: 0.0325 - val_acc: 0.0500\n",
      "Epoch 290/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0305 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0500\n",
      "Epoch 291/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0303 - acc: 0.0000e+00 - val_loss: 0.0326 - val_acc: 0.0500\n",
      "Epoch 292/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0302 - acc: 0.0000e+00 - val_loss: 0.0328 - val_acc: 0.0500\n",
      "Epoch 293/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0300 - acc: 0.0000e+00 - val_loss: 0.0328 - val_acc: 0.0500\n",
      "Epoch 294/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0299 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0500\n",
      "Epoch 295/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0298 - acc: 0.0000e+00 - val_loss: 0.0329 - val_acc: 0.0500\n",
      "Epoch 296/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0297 - acc: 0.0000e+00 - val_loss: 0.0329 - val_acc: 0.0500\n",
      "Epoch 297/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0296 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0500\n",
      "Epoch 298/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0295 - acc: 0.0000e+00 - val_loss: 0.0324 - val_acc: 0.0500\n",
      "Epoch 299/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0293 - acc: 0.0000e+00 - val_loss: 0.0323 - val_acc: 0.0500\n",
      "Epoch 300/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0291 - acc: 0.0000e+00 - val_loss: 0.0317 - val_acc: 0.0500\n",
      "Epoch 301/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0290 - acc: 0.0000e+00 - val_loss: 0.0311 - val_acc: 0.0500\n",
      "Epoch 302/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0289 - acc: 0.0000e+00 - val_loss: 0.0308 - val_acc: 0.0500\n",
      "Epoch 303/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0288 - acc: 0.0000e+00 - val_loss: 0.0306 - val_acc: 0.0500\n",
      "Epoch 304/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0287 - acc: 0.0000e+00 - val_loss: 0.0305 - val_acc: 0.0500\n",
      "Epoch 305/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0286 - acc: 0.0000e+00 - val_loss: 0.0303 - val_acc: 0.0500\n",
      "Epoch 306/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0284 - acc: 0.0000e+00 - val_loss: 0.0304 - val_acc: 0.0500\n",
      "Epoch 307/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0283 - acc: 0.0000e+00 - val_loss: 0.0304 - val_acc: 0.0500\n",
      "Epoch 308/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.0281 - acc: 0.0000e+00 - val_loss: 0.0307 - val_acc: 0.0500\n",
      "Epoch 309/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0280 - acc: 0.0000e+00 - val_loss: 0.0308 - val_acc: 0.0500\n",
      "Epoch 310/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0279 - acc: 0.0000e+00 - val_loss: 0.0308 - val_acc: 0.0500\n",
      "Epoch 311/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0278 - acc: 0.0000e+00 - val_loss: 0.0307 - val_acc: 0.0500\n",
      "Epoch 312/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0277 - acc: 0.0000e+00 - val_loss: 0.0309 - val_acc: 0.0500\n",
      "Epoch 313/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0276 - acc: 0.0000e+00 - val_loss: 0.0307 - val_acc: 0.0500\n",
      "Epoch 314/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0275 - acc: 0.0000e+00 - val_loss: 0.0305 - val_acc: 0.0500\n",
      "Epoch 315/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0274 - acc: 0.0000e+00 - val_loss: 0.0300 - val_acc: 0.0500\n",
      "Epoch 316/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0273 - acc: 0.0000e+00 - val_loss: 0.0295 - val_acc: 0.0500\n",
      "Epoch 317/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0272 - acc: 0.0000e+00 - val_loss: 0.0294 - val_acc: 0.0500\n",
      "Epoch 318/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0271 - acc: 0.0000e+00 - val_loss: 0.0295 - val_acc: 0.0500\n",
      "Epoch 319/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0270 - acc: 0.0000e+00 - val_loss: 0.0299 - val_acc: 0.0500\n",
      "Epoch 320/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0269 - acc: 0.0000e+00 - val_loss: 0.0303 - val_acc: 0.0500\n",
      "Epoch 321/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0268 - acc: 0.0000e+00 - val_loss: 0.0303 - val_acc: 0.0500\n",
      "Epoch 322/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0267 - acc: 0.0000e+00 - val_loss: 0.0300 - val_acc: 0.0500\n",
      "Epoch 323/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0266 - acc: 0.0000e+00 - val_loss: 0.0300 - val_acc: 0.0500\n",
      "Epoch 324/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0265 - acc: 0.0000e+00 - val_loss: 0.0297 - val_acc: 0.0500\n",
      "Epoch 325/400\n",
      "80/80 [==============================] - 0s 174us/step - loss: 0.0264 - acc: 0.0000e+00 - val_loss: 0.0293 - val_acc: 0.0500\n",
      "Epoch 326/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0263 - acc: 0.0000e+00 - val_loss: 0.0292 - val_acc: 0.0500\n",
      "Epoch 327/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0262 - acc: 0.0000e+00 - val_loss: 0.0290 - val_acc: 0.0500\n",
      "Epoch 328/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0262 - acc: 0.0000e+00 - val_loss: 0.0291 - val_acc: 0.0500\n",
      "Epoch 329/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0261 - acc: 0.0000e+00 - val_loss: 0.0293 - val_acc: 0.0500\n",
      "Epoch 330/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0260 - acc: 0.0000e+00 - val_loss: 0.0289 - val_acc: 0.0500\n",
      "Epoch 331/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0259 - acc: 0.0000e+00 - val_loss: 0.0287 - val_acc: 0.0500\n",
      "Epoch 332/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0258 - acc: 0.0000e+00 - val_loss: 0.0287 - val_acc: 0.0500\n",
      "Epoch 333/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0257 - acc: 0.0000e+00 - val_loss: 0.0286 - val_acc: 0.0500\n",
      "Epoch 334/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0256 - acc: 0.0000e+00 - val_loss: 0.0283 - val_acc: 0.0500\n",
      "Epoch 335/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0256 - acc: 0.0000e+00 - val_loss: 0.0281 - val_acc: 0.0500\n",
      "Epoch 336/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0255 - acc: 0.0000e+00 - val_loss: 0.0279 - val_acc: 0.0500\n",
      "Epoch 337/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0254 - acc: 0.0000e+00 - val_loss: 0.0278 - val_acc: 0.0500\n",
      "Epoch 338/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0253 - acc: 0.0000e+00 - val_loss: 0.0279 - val_acc: 0.0500\n",
      "Epoch 339/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0252 - acc: 0.0000e+00 - val_loss: 0.0278 - val_acc: 0.0500\n",
      "Epoch 340/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0252 - acc: 0.0000e+00 - val_loss: 0.0279 - val_acc: 0.0500\n",
      "Epoch 341/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0251 - acc: 0.0000e+00 - val_loss: 0.0275 - val_acc: 0.0500\n",
      "Epoch 342/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0250 - acc: 0.0000e+00 - val_loss: 0.0273 - val_acc: 0.0500\n",
      "Epoch 343/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0250 - acc: 0.0000e+00 - val_loss: 0.0273 - val_acc: 0.0500\n",
      "Epoch 344/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0249 - acc: 0.0000e+00 - val_loss: 0.0273 - val_acc: 0.0500\n",
      "Epoch 345/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0248 - acc: 0.0000e+00 - val_loss: 0.0274 - val_acc: 0.0500\n",
      "Epoch 346/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0247 - acc: 0.0000e+00 - val_loss: 0.0273 - val_acc: 0.0500\n",
      "Epoch 347/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0247 - acc: 0.0000e+00 - val_loss: 0.0272 - val_acc: 0.0500\n",
      "Epoch 348/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0246 - acc: 0.0000e+00 - val_loss: 0.0271 - val_acc: 0.0500\n",
      "Epoch 349/400\n",
      "80/80 [==============================] - 0s 186us/step - loss: 0.0245 - acc: 0.0000e+00 - val_loss: 0.0271 - val_acc: 0.0500\n",
      "Epoch 350/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0245 - acc: 0.0000e+00 - val_loss: 0.0272 - val_acc: 0.0500\n",
      "Epoch 351/400\n",
      "80/80 [==============================] - 0s 213us/step - loss: 0.0244 - acc: 0.0000e+00 - val_loss: 0.0272 - val_acc: 0.0500\n",
      "Epoch 352/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0243 - acc: 0.0000e+00 - val_loss: 0.0272 - val_acc: 0.0500\n",
      "Epoch 353/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0242 - acc: 0.0000e+00 - val_loss: 0.0270 - val_acc: 0.0500\n",
      "Epoch 354/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0242 - acc: 0.0000e+00 - val_loss: 0.0267 - val_acc: 0.0500\n",
      "Epoch 355/400\n",
      "80/80 [==============================] - 0s 225us/step - loss: 0.0241 - acc: 0.0000e+00 - val_loss: 0.0266 - val_acc: 0.0500\n",
      "Epoch 356/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0241 - acc: 0.0000e+00 - val_loss: 0.0264 - val_acc: 0.0500\n",
      "Epoch 357/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0240 - acc: 0.0000e+00 - val_loss: 0.0264 - val_acc: 0.0500\n",
      "Epoch 358/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0239 - acc: 0.0000e+00 - val_loss: 0.0265 - val_acc: 0.0500\n",
      "Epoch 359/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0238 - acc: 0.0000e+00 - val_loss: 0.0268 - val_acc: 0.0500\n",
      "Epoch 360/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0239 - acc: 0.0000e+00 - val_loss: 0.0271 - val_acc: 0.0500\n",
      "Epoch 361/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0238 - acc: 0.0000e+00 - val_loss: 0.0269 - val_acc: 0.0500\n",
      "Epoch 362/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0237 - acc: 0.0000e+00 - val_loss: 0.0266 - val_acc: 0.0500\n",
      "Epoch 363/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0237 - acc: 0.0000e+00 - val_loss: 0.0261 - val_acc: 0.0500\n",
      "Epoch 364/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0237 - acc: 0.0000e+00 - val_loss: 0.0257 - val_acc: 0.0500\n",
      "Epoch 365/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0236 - acc: 0.0000e+00 - val_loss: 0.0257 - val_acc: 0.0500\n",
      "Epoch 366/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0236 - acc: 0.0000e+00 - val_loss: 0.0260 - val_acc: 0.0500\n",
      "Epoch 367/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0234 - acc: 0.0000e+00 - val_loss: 0.0260 - val_acc: 0.0500\n",
      "Epoch 368/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0233 - acc: 0.0000e+00 - val_loss: 0.0261 - val_acc: 0.0500\n",
      "Epoch 369/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0233 - acc: 0.0000e+00 - val_loss: 0.0261 - val_acc: 0.0500\n",
      "Epoch 370/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0232 - acc: 0.0000e+00 - val_loss: 0.0259 - val_acc: 0.0500\n",
      "Epoch 371/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.0231 - acc: 0.0000e+00 - val_loss: 0.0256 - val_acc: 0.0500\n",
      "Epoch 372/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0233 - acc: 0.0000e+00 - val_loss: 0.0254 - val_acc: 0.0500\n",
      "Epoch 373/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0232 - acc: 0.0000e+00 - val_loss: 0.0258 - val_acc: 0.0500\n",
      "Epoch 374/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0232 - acc: 0.0000e+00 - val_loss: 0.0263 - val_acc: 0.0500\n",
      "Epoch 375/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0230 - acc: 0.0000e+00 - val_loss: 0.0261 - val_acc: 0.0500\n",
      "Epoch 376/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0257 - val_acc: 0.0500\n",
      "Epoch 377/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0229 - acc: 0.0000e+00 - val_loss: 0.0254 - val_acc: 0.0500\n",
      "Epoch 378/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0228 - acc: 0.0000e+00 - val_loss: 0.0253 - val_acc: 0.0500\n",
      "Epoch 379/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0227 - acc: 0.0000e+00 - val_loss: 0.0254 - val_acc: 0.0500\n",
      "Epoch 380/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0227 - acc: 0.0000e+00 - val_loss: 0.0255 - val_acc: 0.0500\n",
      "Epoch 381/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0227 - acc: 0.0000e+00 - val_loss: 0.0253 - val_acc: 0.0500\n",
      "Epoch 382/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.0226 - acc: 0.0000e+00 - val_loss: 0.0252 - val_acc: 0.0500\n",
      "Epoch 383/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0225 - acc: 0.0000e+00 - val_loss: 0.0251 - val_acc: 0.0500\n",
      "Epoch 384/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0225 - acc: 0.0000e+00 - val_loss: 0.0250 - val_acc: 0.0500\n",
      "Epoch 385/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.0224 - acc: 0.0000e+00 - val_loss: 0.0251 - val_acc: 0.0500\n",
      "Epoch 386/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.0224 - acc: 0.0000e+00 - val_loss: 0.0254 - val_acc: 0.0500\n",
      "Epoch 387/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0225 - acc: 0.0000e+00 - val_loss: 0.0257 - val_acc: 0.0500\n",
      "Epoch 388/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0224 - acc: 0.0000e+00 - val_loss: 0.0252 - val_acc: 0.0500\n",
      "Epoch 389/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0222 - acc: 0.0000e+00 - val_loss: 0.0248 - val_acc: 0.0500\n",
      "Epoch 390/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0223 - acc: 0.0000e+00 - val_loss: 0.0246 - val_acc: 0.0500\n",
      "Epoch 391/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0222 - acc: 0.0000e+00 - val_loss: 0.0247 - val_acc: 0.0500\n",
      "Epoch 392/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0221 - acc: 0.0000e+00 - val_loss: 0.0247 - val_acc: 0.0500\n",
      "Epoch 393/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0221 - acc: 0.0000e+00 - val_loss: 0.0246 - val_acc: 0.0500\n",
      "Epoch 394/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0220 - acc: 0.0000e+00 - val_loss: 0.0246 - val_acc: 0.0500\n",
      "Epoch 395/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0220 - acc: 0.0000e+00 - val_loss: 0.0247 - val_acc: 0.0500\n",
      "Epoch 396/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0219 - acc: 0.0000e+00 - val_loss: 0.0248 - val_acc: 0.0500\n",
      "Epoch 397/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0220 - acc: 0.0000e+00 - val_loss: 0.0252 - val_acc: 0.0500\n",
      "Epoch 398/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0219 - acc: 0.0000e+00 - val_loss: 0.0249 - val_acc: 0.0500\n",
      "Epoch 399/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0218 - acc: 0.0000e+00 - val_loss: 0.0246 - val_acc: 0.0500\n",
      "Epoch 400/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0218 - acc: 0.0000e+00 - val_loss: 0.0243 - val_acc: 0.0500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=400, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24029528],\n",
       "       [0.17147903],\n",
       "       [0.9320947 ],\n",
       "       [0.21027163],\n",
       "       [0.6983819 ],\n",
       "       [0.27374437],\n",
       "       [0.5896153 ],\n",
       "       [0.9355133 ],\n",
       "       [0.44971   ],\n",
       "       [0.51976675],\n",
       "       [0.47290084],\n",
       "       [0.13045068],\n",
       "       [0.92852664],\n",
       "       [0.2917429 ],\n",
       "       [0.18996269],\n",
       "       [0.404351  ],\n",
       "       [0.19652592],\n",
       "       [0.32027793],\n",
       "       [0.38237977],\n",
       "       [0.6010347 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.predict(x_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2d946138a90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(20), results, c=\"r\")\n",
    "plt.scatter(range(20), y_test, c=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2d94a1a95f8>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Variable size input - Simple solution => padding to make inputs of same size or.,.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = [[[(i + j)/100] for i in range(7)] for j in range(100)] #7 input sequence\n",
    "target = [(i+7)/100 for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(Data, dtype=float)\n",
    "target = np.array(target, dtype=float)\n",
    "data.shape\n",
    "target.shape\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size = 0.2, random_state = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 7, 1)              12        \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 24\n",
      "Trainable params: 24\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#RNN Model\n",
    "model = Sequential()\n",
    "model.add(LSTM((1), batch_input_shape=(None, 7, 1), return_sequences=True)) \n",
    "#input (NONE = you don't know input size, batch size 1)\n",
    "model.add(LSTM((1),return_sequences=False))\n",
    "model.compile(loss=\"MAE\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/400\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.5878 - acc: 0.0000e+00 - val_loss: 0.4762 - val_acc: 0.0000e+00\n",
      "Epoch 2/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.5851 - acc: 0.0000e+00 - val_loss: 0.4734 - val_acc: 0.0000e+00\n",
      "Epoch 3/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.5822 - acc: 0.0000e+00 - val_loss: 0.4705 - val_acc: 0.0000e+00\n",
      "Epoch 4/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.5793 - acc: 0.0000e+00 - val_loss: 0.4676 - val_acc: 0.0000e+00\n",
      "Epoch 5/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.5764 - acc: 0.0000e+00 - val_loss: 0.4647 - val_acc: 0.0000e+00\n",
      "Epoch 6/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.5734 - acc: 0.0000e+00 - val_loss: 0.4617 - val_acc: 0.0000e+00\n",
      "Epoch 7/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.5704 - acc: 0.0000e+00 - val_loss: 0.4586 - val_acc: 0.0000e+00\n",
      "Epoch 8/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5673 - acc: 0.0000e+00 - val_loss: 0.4554 - val_acc: 0.0000e+00\n",
      "Epoch 9/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5641 - acc: 0.0000e+00 - val_loss: 0.4522 - val_acc: 0.0000e+00\n",
      "Epoch 10/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.5607 - acc: 0.0000e+00 - val_loss: 0.4488 - val_acc: 0.0000e+00\n",
      "Epoch 11/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5573 - acc: 0.0000e+00 - val_loss: 0.4453 - val_acc: 0.0000e+00\n",
      "Epoch 12/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.5538 - acc: 0.0000e+00 - val_loss: 0.4417 - val_acc: 0.0000e+00\n",
      "Epoch 13/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5501 - acc: 0.0000e+00 - val_loss: 0.4379 - val_acc: 0.0000e+00\n",
      "Epoch 14/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.5463 - acc: 0.0000e+00 - val_loss: 0.4340 - val_acc: 0.0000e+00\n",
      "Epoch 15/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.5423 - acc: 0.0000e+00 - val_loss: 0.4300 - val_acc: 0.0000e+00\n",
      "Epoch 16/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5382 - acc: 0.0000e+00 - val_loss: 0.4258 - val_acc: 0.0000e+00\n",
      "Epoch 17/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5339 - acc: 0.0000e+00 - val_loss: 0.4214 - val_acc: 0.0000e+00\n",
      "Epoch 18/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.5294 - acc: 0.0000e+00 - val_loss: 0.4169 - val_acc: 0.0000e+00\n",
      "Epoch 19/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5248 - acc: 0.0000e+00 - val_loss: 0.4121 - val_acc: 0.0000e+00\n",
      "Epoch 20/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.5199 - acc: 0.0000e+00 - val_loss: 0.4072 - val_acc: 0.0000e+00\n",
      "Epoch 21/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5149 - acc: 0.0000e+00 - val_loss: 0.4020 - val_acc: 0.0000e+00\n",
      "Epoch 22/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.5098 - acc: 0.0000e+00 - val_loss: 0.3966 - val_acc: 0.0000e+00\n",
      "Epoch 23/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.5043 - acc: 0.0000e+00 - val_loss: 0.3911 - val_acc: 0.0000e+00\n",
      "Epoch 24/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4989 - acc: 0.0000e+00 - val_loss: 0.3853 - val_acc: 0.0000e+00\n",
      "Epoch 25/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.4933 - acc: 0.0000e+00 - val_loss: 0.3798 - val_acc: 0.0000e+00\n",
      "Epoch 26/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.4874 - acc: 0.0000e+00 - val_loss: 0.3741 - val_acc: 0.0000e+00\n",
      "Epoch 27/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4813 - acc: 0.0000e+00 - val_loss: 0.3682 - val_acc: 0.0000e+00\n",
      "Epoch 28/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4749 - acc: 0.0000e+00 - val_loss: 0.3620 - val_acc: 0.0000e+00\n",
      "Epoch 29/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.4685 - acc: 0.0000e+00 - val_loss: 0.3556 - val_acc: 0.0000e+00\n",
      "Epoch 30/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.4620 - acc: 0.0000e+00 - val_loss: 0.3489 - val_acc: 0.0000e+00\n",
      "Epoch 31/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.4552 - acc: 0.0000e+00 - val_loss: 0.3419 - val_acc: 0.0000e+00\n",
      "Epoch 32/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.4484 - acc: 0.0000e+00 - val_loss: 0.3347 - val_acc: 0.0000e+00\n",
      "Epoch 33/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.4415 - acc: 0.0000e+00 - val_loss: 0.3273 - val_acc: 0.0000e+00\n",
      "Epoch 34/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.4346 - acc: 0.0000e+00 - val_loss: 0.3195 - val_acc: 0.0000e+00\n",
      "Epoch 35/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.4272 - acc: 0.0000e+00 - val_loss: 0.3116 - val_acc: 0.0000e+00\n",
      "Epoch 36/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4200 - acc: 0.0000e+00 - val_loss: 0.3038 - val_acc: 0.0000e+00\n",
      "Epoch 37/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4127 - acc: 0.0000e+00 - val_loss: 0.2961 - val_acc: 0.0000e+00\n",
      "Epoch 38/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4050 - acc: 0.0000e+00 - val_loss: 0.2882 - val_acc: 0.0000e+00\n",
      "Epoch 39/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.3974 - acc: 0.0000e+00 - val_loss: 0.2804 - val_acc: 0.0000e+00\n",
      "Epoch 40/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3895 - acc: 0.0000e+00 - val_loss: 0.2732 - val_acc: 0.0000e+00\n",
      "Epoch 41/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.3814 - acc: 0.0000e+00 - val_loss: 0.2665 - val_acc: 0.0000e+00\n",
      "Epoch 42/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.3733 - acc: 0.0000e+00 - val_loss: 0.2599 - val_acc: 0.0000e+00\n",
      "Epoch 43/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3650 - acc: 0.0000e+00 - val_loss: 0.2538 - val_acc: 0.0000e+00\n",
      "Epoch 44/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.3564 - acc: 0.0000e+00 - val_loss: 0.2475 - val_acc: 0.0000e+00\n",
      "Epoch 45/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.3478 - acc: 0.0000e+00 - val_loss: 0.2410 - val_acc: 0.0000e+00\n",
      "Epoch 46/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.3398 - acc: 0.0000e+00 - val_loss: 0.2355 - val_acc: 0.0000e+00\n",
      "Epoch 47/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3312 - acc: 0.0000e+00 - val_loss: 0.2299 - val_acc: 0.0000e+00\n",
      "Epoch 48/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3242 - acc: 0.0000e+00 - val_loss: 0.2242 - val_acc: 0.0000e+00\n",
      "Epoch 49/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.3156 - acc: 0.0000e+00 - val_loss: 0.2196 - val_acc: 0.0000e+00\n",
      "Epoch 50/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3078 - acc: 0.0000e+00 - val_loss: 0.2155 - val_acc: 0.0000e+00\n",
      "Epoch 51/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.3006 - acc: 0.0000e+00 - val_loss: 0.2123 - val_acc: 0.0000e+00\n",
      "Epoch 52/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.2934 - acc: 0.0000e+00 - val_loss: 0.2092 - val_acc: 0.0000e+00\n",
      "Epoch 53/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.2865 - acc: 0.0000e+00 - val_loss: 0.2071 - val_acc: 0.0000e+00\n",
      "Epoch 54/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.2798 - acc: 0.0000e+00 - val_loss: 0.2052 - val_acc: 0.0000e+00\n",
      "Epoch 55/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2736 - acc: 0.0000e+00 - val_loss: 0.2032 - val_acc: 0.0000e+00\n",
      "Epoch 56/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2675 - acc: 0.0000e+00 - val_loss: 0.2013 - val_acc: 0.0000e+00\n",
      "Epoch 57/400\n",
      "80/80 [==============================] - 0s 361us/step - loss: 0.2629 - acc: 0.0000e+00 - val_loss: 0.2000 - val_acc: 0.0000e+00\n",
      "Epoch 58/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.2578 - acc: 0.0000e+00 - val_loss: 0.1993 - val_acc: 0.0000e+00\n",
      "Epoch 59/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.2533 - acc: 0.0000e+00 - val_loss: 0.1997 - val_acc: 0.0000e+00\n",
      "Epoch 60/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.2492 - acc: 0.0000e+00 - val_loss: 0.2002 - val_acc: 0.0000e+00\n",
      "Epoch 61/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2456 - acc: 0.0125 - val_loss: 0.2006 - val_acc: 0.0000e+00\n",
      "Epoch 62/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.2424 - acc: 0.0125 - val_loss: 0.2014 - val_acc: 0.0000e+00\n",
      "Epoch 63/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.2392 - acc: 0.0125 - val_loss: 0.2026 - val_acc: 0.0000e+00\n",
      "Epoch 64/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2365 - acc: 0.0125 - val_loss: 0.2040 - val_acc: 0.0000e+00\n",
      "Epoch 65/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2342 - acc: 0.0125 - val_loss: 0.2059 - val_acc: 0.0000e+00\n",
      "Epoch 66/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2317 - acc: 0.0125 - val_loss: 0.2077 - val_acc: 0.0000e+00\n",
      "Epoch 67/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2296 - acc: 0.0125 - val_loss: 0.2094 - val_acc: 0.0000e+00\n",
      "Epoch 68/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2280 - acc: 0.0125 - val_loss: 0.2111 - val_acc: 0.0000e+00\n",
      "Epoch 69/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2264 - acc: 0.0125 - val_loss: 0.2130 - val_acc: 0.0000e+00\n",
      "Epoch 70/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.2246 - acc: 0.0125 - val_loss: 0.2149 - val_acc: 0.0000e+00\n",
      "Epoch 71/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.2235 - acc: 0.0125 - val_loss: 0.2167 - val_acc: 0.0000e+00\n",
      "Epoch 72/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2221 - acc: 0.0125 - val_loss: 0.2182 - val_acc: 0.0000e+00\n",
      "Epoch 73/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.2212 - acc: 0.0125 - val_loss: 0.2196 - val_acc: 0.0000e+00\n",
      "Epoch 74/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.2203 - acc: 0.0125 - val_loss: 0.2209 - val_acc: 0.0000e+00\n",
      "Epoch 75/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.2194 - acc: 0.0125 - val_loss: 0.2218 - val_acc: 0.0000e+00\n",
      "Epoch 76/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.2185 - acc: 0.0125 - val_loss: 0.2224 - val_acc: 0.0000e+00\n",
      "Epoch 77/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.2178 - acc: 0.0125 - val_loss: 0.2226 - val_acc: 0.0000e+00\n",
      "Epoch 78/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2171 - acc: 0.0125 - val_loss: 0.2227 - val_acc: 0.0000e+00\n",
      "Epoch 79/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2166 - acc: 0.0125 - val_loss: 0.2230 - val_acc: 0.0000e+00\n",
      "Epoch 80/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.2157 - acc: 0.0125 - val_loss: 0.2226 - val_acc: 0.0000e+00\n",
      "Epoch 81/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.2150 - acc: 0.0125 - val_loss: 0.2221 - val_acc: 0.0000e+00\n",
      "Epoch 82/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2144 - acc: 0.0125 - val_loss: 0.2217 - val_acc: 0.0000e+00\n",
      "Epoch 83/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2137 - acc: 0.0125 - val_loss: 0.2213 - val_acc: 0.0000e+00\n",
      "Epoch 84/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.2129 - acc: 0.0125 - val_loss: 0.2210 - val_acc: 0.0000e+00\n",
      "Epoch 85/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2122 - acc: 0.0125 - val_loss: 0.2204 - val_acc: 0.0000e+00\n",
      "Epoch 86/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.2114 - acc: 0.0125 - val_loss: 0.2200 - val_acc: 0.0000e+00\n",
      "Epoch 87/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2107 - acc: 0.0125 - val_loss: 0.2195 - val_acc: 0.0000e+00\n",
      "Epoch 88/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2100 - acc: 0.0125 - val_loss: 0.2192 - val_acc: 0.0000e+00\n",
      "Epoch 89/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.2091 - acc: 0.0125 - val_loss: 0.2187 - val_acc: 0.0000e+00\n",
      "Epoch 90/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2083 - acc: 0.0125 - val_loss: 0.2182 - val_acc: 0.0000e+00\n",
      "Epoch 91/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2075 - acc: 0.0125 - val_loss: 0.2177 - val_acc: 0.0000e+00\n",
      "Epoch 92/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2066 - acc: 0.0125 - val_loss: 0.2168 - val_acc: 0.0000e+00\n",
      "Epoch 93/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2058 - acc: 0.0125 - val_loss: 0.2160 - val_acc: 0.0000e+00\n",
      "Epoch 94/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2049 - acc: 0.0125 - val_loss: 0.2147 - val_acc: 0.0000e+00\n",
      "Epoch 95/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2040 - acc: 0.0125 - val_loss: 0.2133 - val_acc: 0.0000e+00\n",
      "Epoch 96/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2030 - acc: 0.0125 - val_loss: 0.2122 - val_acc: 0.0000e+00\n",
      "Epoch 97/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.2021 - acc: 0.0125 - val_loss: 0.2114 - val_acc: 0.0000e+00\n",
      "Epoch 98/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2011 - acc: 0.0125 - val_loss: 0.2103 - val_acc: 0.0000e+00\n",
      "Epoch 99/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2001 - acc: 0.0125 - val_loss: 0.2092 - val_acc: 0.0000e+00\n",
      "Epoch 100/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1991 - acc: 0.0125 - val_loss: 0.2081 - val_acc: 0.0000e+00\n",
      "Epoch 101/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.1980 - acc: 0.0125 - val_loss: 0.2073 - val_acc: 0.0000e+00\n",
      "Epoch 102/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1970 - acc: 0.0125 - val_loss: 0.2066 - val_acc: 0.0000e+00\n",
      "Epoch 103/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1958 - acc: 0.0125 - val_loss: 0.2056 - val_acc: 0.0000e+00\n",
      "Epoch 104/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1947 - acc: 0.0125 - val_loss: 0.2047 - val_acc: 0.0000e+00\n",
      "Epoch 105/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.1936 - acc: 0.0125 - val_loss: 0.2038 - val_acc: 0.0000e+00\n",
      "Epoch 106/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.1923 - acc: 0.0125 - val_loss: 0.2023 - val_acc: 0.0000e+00\n",
      "Epoch 107/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.1911 - acc: 0.0125 - val_loss: 0.2007 - val_acc: 0.0000e+00\n",
      "Epoch 108/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.1899 - acc: 0.0125 - val_loss: 0.1991 - val_acc: 0.0000e+00\n",
      "Epoch 109/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1885 - acc: 0.0125 - val_loss: 0.1971 - val_acc: 0.0000e+00\n",
      "Epoch 110/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1872 - acc: 0.0125 - val_loss: 0.1950 - val_acc: 0.0000e+00\n",
      "Epoch 111/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.1858 - acc: 0.0125 - val_loss: 0.1932 - val_acc: 0.0000e+00\n",
      "Epoch 112/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1844 - acc: 0.0125 - val_loss: 0.1918 - val_acc: 0.0000e+00\n",
      "Epoch 113/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1830 - acc: 0.0125 - val_loss: 0.1903 - val_acc: 0.0000e+00\n",
      "Epoch 114/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1814 - acc: 0.0125 - val_loss: 0.1886 - val_acc: 0.0000e+00\n",
      "Epoch 115/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1799 - acc: 0.0125 - val_loss: 0.1868 - val_acc: 0.0000e+00\n",
      "Epoch 116/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1783 - acc: 0.0125 - val_loss: 0.1850 - val_acc: 0.0000e+00\n",
      "Epoch 117/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.1766 - acc: 0.0125 - val_loss: 0.1828 - val_acc: 0.0000e+00\n",
      "Epoch 118/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1749 - acc: 0.0125 - val_loss: 0.1805 - val_acc: 0.0000e+00\n",
      "Epoch 119/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1732 - acc: 0.0125 - val_loss: 0.1784 - val_acc: 0.0000e+00\n",
      "Epoch 120/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1714 - acc: 0.0125 - val_loss: 0.1766 - val_acc: 0.0000e+00\n",
      "Epoch 121/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.1695 - acc: 0.0125 - val_loss: 0.1747 - val_acc: 0.0000e+00\n",
      "Epoch 122/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1675 - acc: 0.0125 - val_loss: 0.1726 - val_acc: 0.0000e+00\n",
      "Epoch 123/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1656 - acc: 0.0125 - val_loss: 0.1702 - val_acc: 0.0000e+00\n",
      "Epoch 124/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1635 - acc: 0.0125 - val_loss: 0.1680 - val_acc: 0.0000e+00\n",
      "Epoch 125/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1613 - acc: 0.0125 - val_loss: 0.1656 - val_acc: 0.0000e+00\n",
      "Epoch 126/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1591 - acc: 0.0125 - val_loss: 0.1631 - val_acc: 0.0000e+00\n",
      "Epoch 127/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1568 - acc: 0.0125 - val_loss: 0.1608 - val_acc: 0.0000e+00\n",
      "Epoch 128/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1545 - acc: 0.0125 - val_loss: 0.1577 - val_acc: 0.0000e+00\n",
      "Epoch 129/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.1520 - acc: 0.0125 - val_loss: 0.1549 - val_acc: 0.0000e+00\n",
      "Epoch 130/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1494 - acc: 0.0125 - val_loss: 0.1523 - val_acc: 0.0000e+00\n",
      "Epoch 131/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.1468 - acc: 0.0125 - val_loss: 0.1492 - val_acc: 0.0000e+00\n",
      "Epoch 132/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.1442 - acc: 0.0125 - val_loss: 0.1454 - val_acc: 0.0000e+00\n",
      "Epoch 133/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1413 - acc: 0.0125 - val_loss: 0.1422 - val_acc: 0.0000e+00\n",
      "Epoch 134/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1383 - acc: 0.0125 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
      "Epoch 135/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1353 - acc: 0.0125 - val_loss: 0.1360 - val_acc: 0.0000e+00\n",
      "Epoch 136/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1322 - acc: 0.0125 - val_loss: 0.1318 - val_acc: 0.0000e+00\n",
      "Epoch 137/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1290 - acc: 0.0125 - val_loss: 0.1273 - val_acc: 0.0000e+00\n",
      "Epoch 138/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1256 - acc: 0.0125 - val_loss: 0.1231 - val_acc: 0.0000e+00\n",
      "Epoch 139/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.1222 - acc: 0.0125 - val_loss: 0.1184 - val_acc: 0.0000e+00\n",
      "Epoch 140/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1186 - acc: 0.0125 - val_loss: 0.1139 - val_acc: 0.0000e+00\n",
      "Epoch 141/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.1149 - acc: 0.0125 - val_loss: 0.1093 - val_acc: 0.0000e+00\n",
      "Epoch 142/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1112 - acc: 0.0125 - val_loss: 0.1046 - val_acc: 0.0000e+00\n",
      "Epoch 143/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.1072 - acc: 0.0125 - val_loss: 0.1000 - val_acc: 0.0000e+00\n",
      "Epoch 144/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1031 - acc: 0.0125 - val_loss: 0.0959 - val_acc: 0.0000e+00\n",
      "Epoch 145/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0988 - acc: 0.0125 - val_loss: 0.0918 - val_acc: 0.0000e+00\n",
      "Epoch 146/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0945 - acc: 0.0125 - val_loss: 0.0877 - val_acc: 0.0000e+00\n",
      "Epoch 147/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0901 - acc: 0.0125 - val_loss: 0.0829 - val_acc: 0.0000e+00\n",
      "Epoch 148/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0854 - acc: 0.0125 - val_loss: 0.0785 - val_acc: 0.0000e+00\n",
      "Epoch 149/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0804 - acc: 0.0125 - val_loss: 0.0733 - val_acc: 0.0000e+00\n",
      "Epoch 150/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0755 - acc: 0.0125 - val_loss: 0.0668 - val_acc: 0.0000e+00\n",
      "Epoch 151/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0701 - acc: 0.0125 - val_loss: 0.0605 - val_acc: 0.0000e+00\n",
      "Epoch 152/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0649 - acc: 0.0125 - val_loss: 0.0540 - val_acc: 0.0000e+00\n",
      "Epoch 153/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0593 - acc: 0.0125 - val_loss: 0.0466 - val_acc: 0.0000e+00\n",
      "Epoch 154/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0536 - acc: 0.0125 - val_loss: 0.0396 - val_acc: 0.0000e+00\n",
      "Epoch 155/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0478 - acc: 0.0125 - val_loss: 0.0337 - val_acc: 0.0000e+00\n",
      "Epoch 156/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0427 - acc: 0.0125 - val_loss: 0.0318 - val_acc: 0.0000e+00\n",
      "Epoch 157/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0379 - acc: 0.0125 - val_loss: 0.0326 - val_acc: 0.0000e+00\n",
      "Epoch 158/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0354 - acc: 0.0125 - val_loss: 0.0332 - val_acc: 0.0000e+00\n",
      "Epoch 159/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0338 - acc: 0.0125 - val_loss: 0.0334 - val_acc: 0.0000e+00\n",
      "Epoch 160/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0322 - acc: 0.0125 - val_loss: 0.0342 - val_acc: 0.0000e+00\n",
      "Epoch 161/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0319 - acc: 0.0125 - val_loss: 0.0350 - val_acc: 0.0000e+00\n",
      "Epoch 162/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0316 - acc: 0.0125 - val_loss: 0.0356 - val_acc: 0.0000e+00\n",
      "Epoch 163/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0316 - acc: 0.0125 - val_loss: 0.0358 - val_acc: 0.0000e+00\n",
      "Epoch 164/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0314 - acc: 0.0125 - val_loss: 0.0354 - val_acc: 0.0000e+00\n",
      "Epoch 165/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0309 - acc: 0.0125 - val_loss: 0.0350 - val_acc: 0.0000e+00\n",
      "Epoch 166/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0306 - acc: 0.0125 - val_loss: 0.0347 - val_acc: 0.0000e+00\n",
      "Epoch 167/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0304 - acc: 0.0125 - val_loss: 0.0343 - val_acc: 0.0000e+00\n",
      "Epoch 168/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0303 - acc: 0.0125 - val_loss: 0.0338 - val_acc: 0.0000e+00\n",
      "Epoch 169/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0299 - acc: 0.0125 - val_loss: 0.0335 - val_acc: 0.0000e+00\n",
      "Epoch 170/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0295 - acc: 0.0125 - val_loss: 0.0332 - val_acc: 0.0000e+00\n",
      "Epoch 171/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0292 - acc: 0.0125 - val_loss: 0.0329 - val_acc: 0.0000e+00\n",
      "Epoch 172/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0290 - acc: 0.0125 - val_loss: 0.0325 - val_acc: 0.0000e+00\n",
      "Epoch 173/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0288 - acc: 0.0125 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
      "Epoch 174/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0286 - acc: 0.0125 - val_loss: 0.0316 - val_acc: 0.0000e+00\n",
      "Epoch 175/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0283 - acc: 0.0125 - val_loss: 0.0314 - val_acc: 0.0000e+00\n",
      "Epoch 176/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0282 - acc: 0.0125 - val_loss: 0.0314 - val_acc: 0.0000e+00\n",
      "Epoch 177/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0279 - acc: 0.0125 - val_loss: 0.0311 - val_acc: 0.0000e+00\n",
      "Epoch 178/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0277 - acc: 0.0125 - val_loss: 0.0308 - val_acc: 0.0000e+00\n",
      "Epoch 179/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0275 - acc: 0.0125 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 180/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0273 - acc: 0.0125 - val_loss: 0.0303 - val_acc: 0.0000e+00\n",
      "Epoch 181/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0272 - acc: 0.0125 - val_loss: 0.0302 - val_acc: 0.0000e+00\n",
      "Epoch 182/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0270 - acc: 0.0125 - val_loss: 0.0301 - val_acc: 0.0000e+00\n",
      "Epoch 183/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0267 - acc: 0.0125 - val_loss: 0.0296 - val_acc: 0.0000e+00\n",
      "Epoch 184/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0267 - acc: 0.0125 - val_loss: 0.0293 - val_acc: 0.0000e+00\n",
      "Epoch 185/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0264 - acc: 0.0125 - val_loss: 0.0295 - val_acc: 0.0000e+00\n",
      "Epoch 186/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0263 - acc: 0.0125 - val_loss: 0.0297 - val_acc: 0.0000e+00\n",
      "Epoch 187/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0261 - acc: 0.0125 - val_loss: 0.0295 - val_acc: 0.0000e+00\n",
      "Epoch 188/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0259 - acc: 0.0125 - val_loss: 0.0290 - val_acc: 0.0000e+00\n",
      "Epoch 189/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0258 - acc: 0.0125 - val_loss: 0.0286 - val_acc: 0.0000e+00\n",
      "Epoch 190/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0257 - acc: 0.0125 - val_loss: 0.0285 - val_acc: 0.0000e+00\n",
      "Epoch 191/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0255 - acc: 0.0125 - val_loss: 0.0287 - val_acc: 0.0000e+00\n",
      "Epoch 192/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0257 - acc: 0.0125 - val_loss: 0.0292 - val_acc: 0.0000e+00\n",
      "Epoch 193/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0252 - acc: 0.0125 - val_loss: 0.0288 - val_acc: 0.0000e+00\n",
      "Epoch 194/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.0251 - acc: 0.0125 - val_loss: 0.0282 - val_acc: 0.0000e+00\n",
      "Epoch 195/400\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.0249 - acc: 0.0125 - val_loss: 0.0279 - val_acc: 0.0000e+00\n",
      "Epoch 196/400\n",
      "80/80 [==============================] - 0s 362us/step - loss: 0.0249 - acc: 0.0125 - val_loss: 0.0279 - val_acc: 0.0000e+00\n",
      "Epoch 197/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0247 - acc: 0.0125 - val_loss: 0.0280 - val_acc: 0.0000e+00\n",
      "Epoch 198/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0246 - acc: 0.0125 - val_loss: 0.0282 - val_acc: 0.0000e+00\n",
      "Epoch 199/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0246 - acc: 0.0125 - val_loss: 0.0285 - val_acc: 0.0000e+00\n",
      "Epoch 200/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0244 - acc: 0.0125 - val_loss: 0.0280 - val_acc: 0.0000e+00\n",
      "Epoch 201/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0243 - acc: 0.0125 - val_loss: 0.0272 - val_acc: 0.0000e+00\n",
      "Epoch 202/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0243 - acc: 0.0125 - val_loss: 0.0269 - val_acc: 0.0000e+00\n",
      "Epoch 203/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.0240 - acc: 0.0125 - val_loss: 0.0274 - val_acc: 0.0000e+00\n",
      "Epoch 204/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0239 - acc: 0.0125 - val_loss: 0.0280 - val_acc: 0.0000e+00\n",
      "Epoch 205/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0237 - acc: 0.0125 - val_loss: 0.0275 - val_acc: 0.0000e+00\n",
      "Epoch 206/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0237 - acc: 0.0125 - val_loss: 0.0267 - val_acc: 0.0000e+00\n",
      "Epoch 207/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0235 - acc: 0.0125 - val_loss: 0.0266 - val_acc: 0.0000e+00\n",
      "Epoch 208/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0235 - acc: 0.0125 - val_loss: 0.0261 - val_acc: 0.0000e+00\n",
      "Epoch 209/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0233 - acc: 0.0125 - val_loss: 0.0263 - val_acc: 0.0000e+00\n",
      "Epoch 210/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0231 - acc: 0.0125 - val_loss: 0.0261 - val_acc: 0.0000e+00\n",
      "Epoch 211/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0230 - acc: 0.0125 - val_loss: 0.0259 - val_acc: 0.0000e+00\n",
      "Epoch 212/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0230 - acc: 0.0125 - val_loss: 0.0256 - val_acc: 0.0000e+00\n",
      "Epoch 213/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0228 - acc: 0.0125 - val_loss: 0.0254 - val_acc: 0.0000e+00\n",
      "Epoch 214/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0228 - acc: 0.0125 - val_loss: 0.0253 - val_acc: 0.0000e+00\n",
      "Epoch 215/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0227 - acc: 0.0125 - val_loss: 0.0250 - val_acc: 0.0000e+00\n",
      "Epoch 216/400\n",
      "80/80 [==============================] - 0s 300us/step - loss: 0.0226 - acc: 0.0125 - val_loss: 0.0259 - val_acc: 0.0000e+00\n",
      "Epoch 217/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0225 - acc: 0.0125 - val_loss: 0.0262 - val_acc: 0.0000e+00\n",
      "Epoch 218/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0225 - acc: 0.0125 - val_loss: 0.0259 - val_acc: 0.0000e+00\n",
      "Epoch 219/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0224 - acc: 0.0125 - val_loss: 0.0255 - val_acc: 0.0000e+00\n",
      "Epoch 220/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0222 - acc: 0.0125 - val_loss: 0.0244 - val_acc: 0.0000e+00\n",
      "Epoch 221/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0222 - acc: 0.0125 - val_loss: 0.0242 - val_acc: 0.0000e+00\n",
      "Epoch 222/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0220 - acc: 0.0125 - val_loss: 0.0248 - val_acc: 0.0000e+00\n",
      "Epoch 223/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0219 - acc: 0.0125 - val_loss: 0.0253 - val_acc: 0.0000e+00\n",
      "Epoch 224/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0219 - acc: 0.0125 - val_loss: 0.0254 - val_acc: 0.0000e+00\n",
      "Epoch 225/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0218 - acc: 0.0125 - val_loss: 0.0247 - val_acc: 0.0000e+00\n",
      "Epoch 226/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0218 - acc: 0.0125 - val_loss: 0.0242 - val_acc: 0.0000e+00\n",
      "Epoch 227/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0219 - acc: 0.0125 - val_loss: 0.0247 - val_acc: 0.0000e+00\n",
      "Epoch 228/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0216 - acc: 0.0125 - val_loss: 0.0243 - val_acc: 0.0000e+00\n",
      "Epoch 229/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0219 - acc: 0.0125 - val_loss: 0.0232 - val_acc: 0.0000e+00\n",
      "Epoch 230/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0217 - acc: 0.0125 - val_loss: 0.0234 - val_acc: 0.0000e+00\n",
      "Epoch 231/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0214 - acc: 0.0125 - val_loss: 0.0244 - val_acc: 0.0000e+00\n",
      "Epoch 232/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0219 - acc: 0.0125 - val_loss: 0.0255 - val_acc: 0.0000e+00\n",
      "Epoch 233/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0214 - acc: 0.0125 - val_loss: 0.0248 - val_acc: 0.0000e+00\n",
      "Epoch 234/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0211 - acc: 0.0125 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 235/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0212 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 236/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0213 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 237/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0213 - acc: 0.0125 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 238/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0210 - acc: 0.0125 - val_loss: 0.0240 - val_acc: 0.0000e+00\n",
      "Epoch 239/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0209 - acc: 0.0125 - val_loss: 0.0244 - val_acc: 0.0000e+00\n",
      "Epoch 240/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0209 - acc: 0.0125 - val_loss: 0.0242 - val_acc: 0.0000e+00\n",
      "Epoch 241/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0208 - acc: 0.0125 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
      "Epoch 242/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0207 - acc: 0.0125 - val_loss: 0.0236 - val_acc: 0.0000e+00\n",
      "Epoch 243/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0207 - acc: 0.0125 - val_loss: 0.0235 - val_acc: 0.0000e+00\n",
      "Epoch 244/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0206 - acc: 0.0125 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 245/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0206 - acc: 0.0125 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 246/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0206 - acc: 0.0125 - val_loss: 0.0233 - val_acc: 0.0000e+00\n",
      "Epoch 247/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0205 - acc: 0.0125 - val_loss: 0.0233 - val_acc: 0.0000e+00\n",
      "Epoch 248/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0205 - acc: 0.0125 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 249/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0208 - acc: 0.0125 - val_loss: 0.0234 - val_acc: 0.0000e+00\n",
      "Epoch 250/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0204 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 251/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0205 - acc: 0.0125 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 252/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0206 - acc: 0.0125 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 253/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0203 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 254/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0204 - acc: 0.0125 - val_loss: 0.0232 - val_acc: 0.0000e+00\n",
      "Epoch 255/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0203 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 256/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0202 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 257/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0202 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 258/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.0202 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 259/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0203 - acc: 0.0125 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 260/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0202 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 261/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0201 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 262/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0201 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 263/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0201 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 264/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0200 - acc: 0.0125 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 265/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0201 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 266/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0201 - acc: 0.0125 - val_loss: 0.0232 - val_acc: 0.0000e+00\n",
      "Epoch 267/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0201 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 268/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0199 - acc: 0.0125 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 269/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0200 - acc: 0.0125 - val_loss: 0.0217 - val_acc: 0.0000e+00\n",
      "Epoch 270/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0199 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 271/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0198 - acc: 0.0125 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 272/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0200 - acc: 0.0125 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 273/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0201 - acc: 0.0125 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 274/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0199 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 275/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 276/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0232 - val_acc: 0.0000e+00\n",
      "Epoch 277/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0198 - acc: 0.0125 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 278/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 279/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 280/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 281/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 282/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 283/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0195 - acc: 0.0125 - val_loss: 0.0234 - val_acc: 0.0000e+00\n",
      "Epoch 284/400\n",
      "80/80 [==============================] - 0s 275us/step - loss: 0.0198 - acc: 0.0125 - val_loss: 0.0236 - val_acc: 0.0000e+00\n",
      "Epoch 285/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 286/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 287/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 288/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 289/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0235 - val_acc: 0.0000e+00\n",
      "Epoch 290/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0199 - acc: 0.0125 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 291/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0198 - acc: 0.0125 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 292/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0198 - acc: 0.0125 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 293/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0199 - acc: 0.0125 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 294/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 295/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 296/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 297/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0194 - acc: 0.0125 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 298/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 299/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0192 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 300/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 301/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0197 - acc: 0.0125 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 302/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0194 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 303/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 304/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0194 - acc: 0.0125 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 305/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0194 - acc: 0.0125 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 306/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 307/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 308/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 309/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0192 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 310/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 311/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 312/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 313/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0192 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 314/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0192 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 315/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 316/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 317/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 318/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 319/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 320/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 321/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.0192 - acc: 0.0125 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 322/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 323/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 324/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 325/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 326/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 327/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 328/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 329/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 330/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 331/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 332/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 333/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 334/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 335/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 336/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 337/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 338/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 339/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 340/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 341/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 342/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 343/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 344/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 345/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 346/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 347/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 348/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 349/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 350/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 351/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 352/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 353/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 354/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0190 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 355/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 356/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0231 - val_acc: 0.0000e+00\n",
      "Epoch 357/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 358/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 359/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 360/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 361/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 362/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 363/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 364/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 365/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 366/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 367/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 368/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 369/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 370/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 371/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 372/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 373/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 374/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 375/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 376/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0191 - acc: 0.0125 - val_loss: 0.0232 - val_acc: 0.0000e+00\n",
      "Epoch 377/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 378/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 379/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 380/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 381/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 382/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 383/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 384/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 385/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 386/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 387/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0188 - acc: 0.0125 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 388/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 389/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0233 - val_acc: 0.0000e+00\n",
      "Epoch 390/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 391/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 392/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
      "Epoch 393/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0187 - acc: 0.0125 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 394/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 395/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 396/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 397/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 398/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0185 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 399/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 400/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=400, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2645804 ],\n",
       "       [0.19049145],\n",
       "       [0.95064354],\n",
       "       [0.23295449],\n",
       "       [0.71159893],\n",
       "       [0.2988639 ],\n",
       "       [0.60428977],\n",
       "       [0.95243526],\n",
       "       [0.47025046],\n",
       "       [0.5370891 ],\n",
       "       [0.49236214],\n",
       "       [0.14271836],\n",
       "       [0.94877625],\n",
       "       [0.31697822],\n",
       "       [0.21098492],\n",
       "       [0.426889  ],\n",
       "       [0.21814254],\n",
       "       [0.34531143],\n",
       "       [0.40576923],\n",
       "       [0.6153786 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.predict(x_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2d94f6a2240>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAW60lEQVR4nO3df4zk9X3f8ef7jrtGWzuH4a6tC+wutkhV0mtttCJu3CZE55of6h2lilLoRHHBzihtaCGNq1BthW2q+SOJWkNS6nRLieNoauy0JT2isy4OdRKpKpTFxZyBEJ/p7XKGmgskS61Vcwe8+8fMnmeXmd3Z2fn1ne/zIZ1mvp/5zs77Pvfd133m8/0VmYkkafLtGnUBkqThMPAlqSQMfEkqCQNfkkrCwJekkrhgVB+8f//+nJ2dHdXHS1IhPfnkk3+UmQd6ee/IAn92dpbFxcVRfbwkFVJELPX6Xqd0JKkkDHxJKgkDX5JKwsCXpJIw8CWpJAx8SSoJA1+SSsLAl6SSMPA1VPUTdWbvnWXXp3Yxe+8s9RP1UZcklcbIzrRV+dRP1Kk+UmX13CoASytLVB+pAlA5WBllaVIpOMLvkSPV7Zt/dP582K9ZPbfK/KPzI6pIKhdH+D1wpNqb5ZXlbbVL6i9H+D1wpNqb6X3T22rXen6r1E4Z+D1wpNqb2qEaU3um1rVN7Zmidqg2ooqKY+1b5dLKEkme/1Zp6Gs7DPweOFLtTeVghYXDC8zsmyEIZvbNsHB4wWmwLvitUv3gHH4Paodq6+bwwZFqtyoHKwZ8D/xWqX7YcoQfEQ9GxCsR8fUOr0dE/FJEnIyIpyPiqv6XOV4cqWrY/FapfuhmhP9Z4N8An+vw+vXAFc0/PwB8pvk40Rypapj8Vql+2HKEn5m/D7y2ySo3Ap/LhseACyPi3f0qUJLfKtUf/ZjDvwR4sWX5dLPt5Y0rRkQVqAJMT/tVVNoOv1Vqp/pxlE60act2K2bmQmbOZebcgQM93XRdktSjfgT+aeCyluVLgZf68HMlSX3Uj8A/CvxE82idDwArmfm26RxJ0mhtOYcfEZ8HrgH2R8Rp4BPAHoDM/BXgGHADcBJYBW4dVLGSpN5tGfiZecsWryfw032rSJI0EF5aQZJKwsDvVb0Os7Owa1fjse5FrLpiv/XOvtMOeS2dXtTrUK3CavOsx6WlxjJAxeOkO7LfemffqQ+iMQU/fHNzc7m4uDiSz96x2dnGL9xGMzNw6tSwqykO+6139p2aIuLJzJzr5b1O6fRiucMVCju1q8F+6519pz4w8HvR6bIQXi5ic/Zb7+w79YGB34taDabW37mJqalGuzqz33pn36kPDPxeVCqwsNCYP41oPC4suPNsK/Zb7+w79YE7bSWpQNxpK0nakoEvSSVh4EtSSRj4klQSBr4klYSBL0klYeBLUkkY+JJUEga+JJWEgS9JJWHgS1JJGPiSVBIGviSVhIEvSSVh4EtSSRj4klQSBr4klYSBL0klYeBLUkkY+JJUEga+JJWEgS9JJdFV4EfEdRHxfEScjIi72rw+HRFfiYj/FRFPR8QN/S9VkrQTWwZ+ROwG7geuB64EbomIKzes9i+AL2bm+4GbgX/b70IlSTvTzQj/auBkZr6QmWeBh4AbN6yTwPc2n+8DXupfiZKkfugm8C8BXmxZPt1sa/VJ4Mcj4jRwDPjH7X5QRFQjYjEiFs+cOdNDuZKkXnUT+NGmLTcs3wJ8NjMvBW4Afj0i3vazM3MhM+cyc+7AgQPbr1aS1LNuAv80cFnL8qW8fcrmo8AXATLzfwDfA+zvR4GSpP7oJvCfAK6IiMsjYi+NnbJHN6yzDBwCiIi/TCPwnbORpDGyZeBn5hvA7cBx4DkaR+M8ExH3RMSR5mo/C/xkRHwN+DzwDzJz47SPJGmELuhmpcw8RmNnbGvb3S3PnwU+2N/SJEn95Jm2klQSBr4klYSBL0klYeBLUkkY+JJUEga+JJWEgS9JJWHgS1JJGPiSVBIGfhHV6zA7C7t2NR7r9VFXJKkAurq0gsZIvQ7VKqyuNpaXlhrLAJXK6OqSNPYc4RfN/Px3w37N6mqjXZI2YeAXzfIy9YMweyfs+kTjsX6w0S5Jm3FKp2DqP3wR1R98ldW9jeWlC6F6GLj4IpzQkbQZR/gFM/8hzof9mtW9jXZJ2oyBXzDLb7y2rXZJWmPgF8z0vulttUvSGgO/YGqHakztmVrXNrVnitqh2ogqklQUBn7BVA5WWDi8wMy+GYJgZt8MC4cXqBx0l62kzcWo7jU+NzeXi4uLI/lsSSqqiHgyM+d6ea8jfEkqCQNfkkrCwJekkjDwJakkDHxJKgkDX5JKwsCXpJIw8CWpJAx8SSoJA1+SBmmM7kFt4EvSoNTr1D99K7M3LbHr7mT2piXqn751ZKFv4Gt7xmi0Io27+gN3UL32HEsXQkbzDnXXnqP+wB0jqaerwI+I6yLi+Yg4GRF3dVjnxyLi2Yh4JiL+Y3/L1FgYs9GKNO7m3/dq+zvUve/VkdSzZeBHxG7gfuB64Ergloi4csM6VwD/HPhgZn4/cOcAatWIjdtoRRp3y/u21z5o3YzwrwZOZuYLmXkWeAi4ccM6Pwncn5l/DJCZr/S3TI2DcRutSONues/F22oftG4C/xLgxZbl0822Vt8HfF9E/PeIeCwirmv3gyKiGhGLEbF45syZ3irWyIzbaEUad7Uj9zEV60dJU7GX2pH7RlJPN4Efbdo23jXlAuAK4BrgFuCBiLjwbW/KXMjMucycO3DgwHZr1YiN22hFGneVgxUWbnpw/R3qbnpwZHeou6CLdU4Dl7UsXwq81GadxzLzHPC/I+J5Gv8BPNGXKjUWakfuo/rwbazm2fNtoxytSEVQOVgZm1uQdjPCfwK4IiIuj4i9wM3A0Q3r/CbwIwARsZ/GFM8L/SxUozduoxVJ27PlCD8z34iI24HjwG7gwcx8JiLuARYz82jztQ9HxLPAm8A/y0z35E2gcRqtSNoeb2IuSQXiTcwlSVsy8CWpJAx8SSoJA1+lUj9RZ/beWXZ9ahez985SP+F1gFQe3RyHL02E+on6uvMIllaWqD58G4BHHqkUHOGrNOaP3rHupDGA1TzL/FEv/qZyMPBVGsvn2p8a0qldmjQGvkpjemV77dKkMfBVGrWnLmZq/YwOU2cb7VIZGPgqjcrH7mPh+B5m/gQiYeZPYOH4Hiof8+JvKgeP0lF5VCpUgMr8PCwvw/Q01GpQ8QgdlYMjfJVLpQKnTsFbbzUetxH2HsOvonOEL3WhfqJO9ZEqq+dWgeYx/I9UAY/hV3E4wpe6MP/o/PmwX7N6bpX5R+dHVJG0fQa+1IXllaVttUvjyMBXoYxqHn36O7u31S6NIwNfhbE2j760skSS5+fRhxH6teNvtj+G//ibA/9sqV8MfBXGKOfRK6/PsPAI64/hf6TRLhWFR+moMEY6j16rUalWqZxo+Q9nagoWaoP/bKlPHOGrMEY6j16pwMICzMxARONxYcGTtlQoBr4KY+Tz6Ds4aUsaBwa+CsN5dGlnnMNXcTiPLu2II3wVh/PoheV1iMaDI3wVS6ViwBeM1yEaH47wJQ2U1yEaHwa+pIFaXlneVrsGx8CXNFDTF1y0rfZxM0n7Hwx8SQNV+x3anz/xO6OpZztGef2mQTDwJQ1U5fdea3/+xO+9NurStjRp+x88SkfSYE1PUzmxROXEhvaZ6ZGUsx2Ttv+hqxF+RFwXEc9HxMmIuGuT9X40IjIi5vpXoqRCq9UaJ8i1mppqtI+5ou9/2GjLwI+I3cD9wPXAlcAtEXFlm/XeCfwT4PF+FympwAp8wlyR9z+0080I/2rgZGa+kJlngYeAG9us9y+BXwD+Xx/rkzQJCnrhuSLvf2inmzn8S4AXW5ZPAz/QukJEvB+4LDN/KyI+3sf6JGl0Crz/oZ1uRvjRpi3PvxixC/g08LNb/qCIakQsRsTimTNnuq9SkkahwPsf2ukm8E8Dl7UsXwq81LL8TuCvAL8bEaeADwBH2+24zcyFzJzLzLkDBw70XrUkDUOB9z+0082UzhPAFRFxOfAt4Gbg76+9mJkrwP615Yj4XeDjmbnY31IlaQQm6IJ9W47wM/MN4HbgOPAc8MXMfCYi7omII4MuUJLUH12deJWZx4BjG9ru7rDuNTsvS5LUb15aQdJEm6SLn+2Ul1aQNLG8+cp6jvAlTaxJu/jZThn4kibWpF38bKcMfEkTa9IufrZTBr6kiTVpFz/bKQNfKoGyHqkyaRc/2ymP0pEmXKmPVJmwi5/tlCN8acKV+kiVCbv42U4Z+NKEW15Z2lb7RJmwi5/tlIFfQmWdzy2r6e/s3lb7xCnozVcGwcAvmbX53KWVJZI8P59r6E+u2vE32x+pcvzN0RSkkTHwS6bU87klVXl9pv2RKq/PjLo0DZlH6ZRMqedzy6pWo1KtUjnR8h/91BQslHPHZZk5wi+Z0s/nlpE7LtVk4JeM87kl5Y5LYeCXjvO5Unk5h182zudKpVXYEb7HkvfI+VyptAo5wq+fqFN9+DZWszEZvbSyRPXh24ASXBukHyoVA14qoUKO8OeP3nE+7Nes5lnmj94xoookafwVMvCXz726rXZJUkEDf3ple+2SpIIGfu2pi9sfS/7UxaMpSJIKoJCBX/nYfSwc37P+WPLje6h87L5RlyZJY6uQgU+lQuVnfpVTD8/w1j3BqYdnqPzMrxbnyJN6HWZnYdeuxmPdQ0olDV4hD8sEintoYb1O/dO3Mn/TOZb3wfTKErVP30oFivn3kVQYxRzhF1j9gTuoXnuOpQshA5YuhOq156g/4CGlkgbLwB+y+fe9yure9W2rexvtkjpwGrQvijulU1DL+7bXLpVevQ7VKqw2r/+0tNRYBqdBt8kR/pBN72l/6GindmksjHKEPT9P/b2rzN4Juz4Bs3dC/b2rMO9d2rbLwB+y2pH7mIr1czpTsZfaEQ8p1ZhaG2EvLUHmd0fYQwr9+vcuUT3M+v1ehxvt2p6uAj8irouI5yPiZETc1eb1fxoRz0bE0xHxaER4cfUOKgcrLNz0IDP7ZgiCmX0zLNz0oBd90/ga8Qh7/trd7fd7Xetd2rYrMnPzFSJ2A38I/C3gNPAEcEtmPtuyzo8Aj2fmakT8Q+CazPx7m/3cubm5XFxc3Gn9kgas/leD6mHWhe7U2eaNc57ePD/6YdengnafEsBbnxj854+biHgyM+d6eW83I/yrgZOZ+UJmngUeAm5sXSEzv5KZa3fUeAy4tJdihsq9/lJXRj3Cnt7XfsKgU7s66ybwLwFebFk+3Wzr5KPAl9q9EBHViFiMiMUzZ850X2W/jXhOUiqS5Xe0v99xp/Z+qx2qMbVnal3b1J4paoe8S9t2dRP40aat7feoiPhxYA74xXavZ+ZCZs5l5tyBAwe6r7Lf5ue/e4jXmlX3+kvtjHqEXTlYYeHwwvr9XocX3O/Vg26Owz8NXNayfCnw0saVIuJDwDzww5n5p/0pb0CWl7fXLpVY7VCN6iNVVs99d5A07BF25WDFgO+Dbkb4TwBXRMTlEbEXuBk42rpCRLwf+HfAkcx8pf9l9tn0NPWDrD/q4GCjXdJ6jrAnx5Yj/Mx8IyJuB44Du4EHM/OZiLgHWMzMozSmcN4B/EZEACxn5pEB1r0j9Z+7geq3PsPqnsby0oVQPQJccgNuwtLbOcKeDF1dWiEzjwHHNrTd3fL8Q32ua6Dm//TY+bBfs7qn0e4mLWlSlfJM2+WV9nP1ndolaRKUMvCn97Wfq+/ULkmToJSB73G9ksqolIHvUQcaCc/u1ohteS2dQfFaOiqVjdd0B5iagoUFr+mubRn0tXQk7ZRnd2sMGPjSMHh2t8aAgS8NQ6ezuD27W0Nk4EvDUKs15uxbTU012qUhMfClYahUGjtoZ2YgovHoDlsNWVeXVpDUB5WKAa+RcoQvSSVh4EtSSRj40pDUT9SZvXeWXZ/axey9s9RPeKathss5fGkI6ifq6+4atbSyRPWRKoCX9NDQOMKXhmD+0fl1twgEWD23yvyjnmmr4THwpSHwHgwaBwa+NATeg0HjwMCXhsB7MGgcGPjSEHgPBo0Dr4cvSQXi9fAlSVsy8CWpJAx8SSoJA1+SSsLAl6SSMPAlqSQMfEkqCQNfkkpiZCdeRcQZYKkPP2o/8Ed9+DmDMs71WVtvxrk2GO/6rK03rbXNZOaBXn7IyAK/XyJisdezzoZhnOuztt6Mc20w3vVZW2/6VZtTOpJUEga+JJXEJAT+wqgL2MI412dtvRnn2mC867O23vSltsLP4UuSujMJI3xJUhcMfEkqicIEfkRcFxHPR8TJiLirzet/JiK+0Hz98YiYHVJdl0XEVyLiuYh4JiLuaLPONRGxEhFPNf/cPYzaWj7/VEScaH722+46Ew2/1Oy7pyPiqiHV9Zda+uSpiHg9Iu7csM7Q+i4iHoyIVyLi6y1tF0XElyPiG83Hd3V470ea63wjIj4yxPp+MSL+oPnv9nBEXNjhvZtuAwOq7ZMR8a2Wf7sbOrx309/tAdX2hZa6TkXEUx3eO+h+a5sfA9vuMnPs/wC7gW8C7wH2Al8Drtywzj8CfqX5/GbgC0Oq7d3AVc3n7wT+sE1t1wC/NcL+OwXs3+T1G4AvAQF8AHh8RP/G/4fGSSUj6Tvgh4CrgK+3tP0CcFfz+V3Az7d530XAC83HdzWfv2tI9X0YuKD5/Ofb1dfNNjCg2j4JfLyLf/dNf7cHUduG1/8VcPeI+q1tfgxquyvKCP9q4GRmvpCZZ4GHgBs3rHMj8GvN5/8JOBQRMejCMvPlzPxq8/n/BZ4DLhn05/bZjcDnsuEx4MKIePeQazgEfDMz+3H2dU8y8/eB1zY0t25Xvwb8nTZvvRb4cma+lpl/DHwZuG4Y9WXmb2fmG83Fx4BL+/253ejQd93o5nd7YLU1M+LHgM/38zO7tUl+DGS7K0rgXwK82LJ8mreH6vl1mr8AK8DFQ6muqTmN9H7g8TYv//WI+FpEfCkivn+YdQEJ/HZEPBkR1Tavd9O/g3YznX/pRtl3fz4zX4bGLyfw59qsMw79B3AbjW9q7Wy1DQzK7c3ppgc7TEuMuu/+JvDtzPxGh9eH1m8b8mMg211RAr/dSH3j8aTdrDMwEfEO4D8Dd2bm6xte/iqNqYq/Bvwy8JvDqqvpg5l5FXA98NMR8UMbXh913+0FjgC/0eblUfddN0bafwARMQ+8AdQ7rLLVNjAInwHeC7wPeJnG1MlGo+67W9h8dD+UftsiPzq+rU3bpn1XlMA/DVzWsnwp8FKndSLiAmAfvX3F3LaI2EPjH6uemf9l4+uZ+Xpmfqf5/BiwJyL2D6O25me+1Hx8BXiYxtfoVt307yBdD3w1M7+98YVR9x3w7bXprebjK23WGWn/NXfW/W2gks3J3Y262Ab6LjO/nZlvZuZbwL/v8Jkj67tmTvxd4Aud1hlGv3XIj4Fsd0UJ/CeAKyLi8uZo8Gbg6IZ1jgJre6l/FPhvnTb+fmrOAf4H4LnM/Ncd1vkLa/sTIuJqGv3+6qBra37en42Id649p7GT7+sbVjsK/EQ0fABYWfs6OSQdR1mj7Lum1u3qI8B/bbPOceDDEfGu5rTFh5ttAxcR1wE/BxzJzNUO63SzDQyittb9QDd1+MxufrcH5UPAH2Tm6XYvDqPfNsmPwWx3g9r7PIC92TfQ2IP9TWC+2XYPjQ0d4HtoTAmcBP4n8J4h1fU3aHyNehp4qvnnBuCngJ9qrnM78AyNIxAeA35wiP32nubnfq1Zw1rftdYXwP3Nvj0BzA2xvikaAb6vpW0kfUfjP52XgXM0Rk8fpbEf6FHgG83Hi5rrzgEPtLz3tua2dxK4dYj1naQxj7u27a0dqfYXgWObbQNDqO3Xm9vT0zQC7N0ba2suv+13e9C1Nds/u7adtaw77H7rlB8D2e68tIIklURRpnQkSTtk4EtSSRj4klQSBr4klYSBL0klYeBLUkkY+JJUEv8ffY1q4EPFc54AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(20), results, c=\"r\")\n",
    "plt.scatter(range(20), y_test, c=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27, 0.17, 1.03, 0.23, 0.7 , 0.31, 0.6 , 1.04, 0.48, 0.54, 0.5 ,\n",
       "       0.09, 1.02, 0.33, 0.2 , 0.44, 0.21, 0.36, 0.42, 0.61])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2645804 ],\n",
       "       [0.19049145],\n",
       "       [0.95064354],\n",
       "       [0.23295449],\n",
       "       [0.71159893],\n",
       "       [0.2988639 ],\n",
       "       [0.60428977],\n",
       "       [0.95243526],\n",
       "       [0.47025046],\n",
       "       [0.5370891 ],\n",
       "       [0.49236214],\n",
       "       [0.14271836],\n",
       "       [0.94877625],\n",
       "       [0.31697822],\n",
       "       [0.21098492],\n",
       "       [0.426889  ],\n",
       "       [0.21814254],\n",
       "       [0.34531143],\n",
       "       [0.40576923],\n",
       "       [0.6153786 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Predictions\n",
      "0      0.264580\n",
      "1      0.190491\n",
      "2      0.950644\n",
      "3      0.232954\n",
      "4      0.711599\n",
      "5      0.298864\n",
      "6      0.604290\n",
      "7      0.952435\n",
      "8      0.470250\n",
      "9      0.537089\n",
      "10     0.492362\n",
      "11     0.142718\n",
      "12     0.948776\n",
      "13     0.316978\n",
      "14     0.210985\n",
      "15     0.426889\n",
      "16     0.218143\n",
      "17     0.345311\n",
      "18     0.405769\n",
      "19     0.615379\n"
     ]
    }
   ],
   "source": [
    "preds = pd.DataFrame({'Predictions':results[:,0]})\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    20.000000\n",
      "mean      0.022331\n",
      "std       0.026869\n",
      "min       0.002911\n",
      "25%       0.007083\n",
      "50%       0.011368\n",
      "75%       0.016139\n",
      "max       0.087565\n",
      "Name: Error, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Error</th>\n",
       "      <th>RPrediction</th>\n",
       "      <th>RError</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.264580</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.005420</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.000001e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.190491</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.950644</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.079356</td>\n",
       "      <td>0.95</td>\n",
       "      <td>8.000001e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.232954</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4.172325e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.711599</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.71</td>\n",
       "      <td>9.999979e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.298864</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.011136</td>\n",
       "      <td>0.30</td>\n",
       "      <td>9.999988e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.604290</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.384186e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.952435</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.087565</td>\n",
       "      <td>0.95</td>\n",
       "      <td>9.000001e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.470250</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.537089</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2.145767e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.492362</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.999990e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.142718</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.052718</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.948776</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.071224</td>\n",
       "      <td>0.95</td>\n",
       "      <td>7.000001e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.316978</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.013022</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.000001e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.210985</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.010985</td>\n",
       "      <td>0.21</td>\n",
       "      <td>9.999993e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.426889</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.013111</td>\n",
       "      <td>0.43</td>\n",
       "      <td>9.999993e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.218143</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.008143</td>\n",
       "      <td>0.22</td>\n",
       "      <td>9.999999e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.345311</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.014689</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.000001e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.405769</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.615379</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.000000e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prediction  Actual     Error  RPrediction        RError\n",
       "0     0.264580    0.27  0.005420         0.26  1.000001e-02\n",
       "1     0.190491    0.17  0.020491         0.19  2.000000e-02\n",
       "2     0.950644    1.03  0.079356         0.95  8.000001e-02\n",
       "3     0.232954    0.23  0.002954         0.23  4.172325e-09\n",
       "4     0.711599    0.70  0.011599         0.71  9.999979e-03\n",
       "5     0.298864    0.31  0.011136         0.30  9.999988e-03\n",
       "6     0.604290    0.60  0.004290         0.60  2.384186e-08\n",
       "7     0.952435    1.04  0.087565         0.95  9.000001e-02\n",
       "8     0.470250    0.48  0.009750         0.47  1.000000e-02\n",
       "9     0.537089    0.54  0.002911         0.54  2.145767e-08\n",
       "10    0.492362    0.50  0.007638         0.49  9.999990e-03\n",
       "11    0.142718    0.09  0.052718         0.14  5.000000e-02\n",
       "12    0.948776    1.02  0.071224         0.95  7.000001e-02\n",
       "13    0.316978    0.33  0.013022         0.32  1.000001e-02\n",
       "14    0.210985    0.20  0.010985         0.21  9.999993e-03\n",
       "15    0.426889    0.44  0.013111         0.43  9.999993e-03\n",
       "16    0.218143    0.21  0.008143         0.22  9.999999e-03\n",
       "17    0.345311    0.36  0.014689         0.35  1.000001e-02\n",
       "18    0.405769    0.42  0.014231         0.41  1.000000e-02\n",
       "19    0.615379    0.61  0.005379         0.62  1.000000e-02"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting predictions\n",
    "p_df = pd.DataFrame()\n",
    "p_df['Prediction'] = preds[\"Predictions\"]\n",
    "p_df['Actual'] = y_test\n",
    "p_df['Error'] = np.abs(p_df['Actual'] - p_df['Prediction'])\n",
    "p_df['RPrediction'] = np.round(results,2) \n",
    "p_df['RError'] = np.abs(p_df['Actual'] - p_df['RPrediction'])\n",
    "\n",
    "print(p_df['Error'].describe())\n",
    "\n",
    "p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2d94f7e7710>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
