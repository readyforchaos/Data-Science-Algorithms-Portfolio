{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data prep\n",
    "Data = [[[(i + j)/100]for i in range(5)] for j in range(100)]\n",
    "target = [(i+5)/100 for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(Data, dtype=float)\n",
    "target = np.array(target, dtype=float)\n",
    "data.shape\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0704 17:00:11.174322 13696 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0704 17:00:11.189308 13696 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0704 17:00:11.191278 13696 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RNN Model\n",
    "model = Sequential()\n",
    "model.add(LSTM((1), batch_input_shape=(None, None, 1), return_sequences=True)) \n",
    "#input (NONE = you don't know input size, batch size 1)\n",
    "model.add(LSTM((1),return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0704 17:00:12.954352 13696 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 1)           12        \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 24\n",
      "Trainable params: 24\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"MAE\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0704 17:00:14.320732 13696 deprecation.py:323] From C:\\Users\\gill\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0704 17:00:15.057746 13696 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0704 17:00:15.119559 13696 deprecation_wrapper.py:119] From C:\\Users\\gill\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/400\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.5318 - acc: 0.0000e+00 - val_loss: 0.4255 - val_acc: 0.0000e+00\n",
      "Epoch 2/400\n",
      "80/80 [==============================] - 0s 174us/step - loss: 0.5276 - acc: 0.0000e+00 - val_loss: 0.4213 - val_acc: 0.0000e+00\n",
      "Epoch 3/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5233 - acc: 0.0000e+00 - val_loss: 0.4171 - val_acc: 0.0000e+00\n",
      "Epoch 4/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.5190 - acc: 0.0000e+00 - val_loss: 0.4129 - val_acc: 0.0000e+00\n",
      "Epoch 5/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.5146 - acc: 0.0000e+00 - val_loss: 0.4086 - val_acc: 0.0000e+00\n",
      "Epoch 6/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.5102 - acc: 0.0000e+00 - val_loss: 0.4042 - val_acc: 0.0000e+00\n",
      "Epoch 7/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.5057 - acc: 0.0000e+00 - val_loss: 0.3998 - val_acc: 0.0000e+00\n",
      "Epoch 8/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5012 - acc: 0.0000e+00 - val_loss: 0.3953 - val_acc: 0.0000e+00\n",
      "Epoch 9/400\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.4966 - acc: 0.0000e+00 - val_loss: 0.3908 - val_acc: 0.0000e+00\n",
      "Epoch 10/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4919 - acc: 0.0000e+00 - val_loss: 0.3861 - val_acc: 0.0000e+00\n",
      "Epoch 11/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4871 - acc: 0.0000e+00 - val_loss: 0.3814 - val_acc: 0.0000e+00\n",
      "Epoch 12/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.4823 - acc: 0.0000e+00 - val_loss: 0.3766 - val_acc: 0.0000e+00\n",
      "Epoch 13/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4774 - acc: 0.0000e+00 - val_loss: 0.3718 - val_acc: 0.0000e+00\n",
      "Epoch 14/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4725 - acc: 0.0000e+00 - val_loss: 0.3669 - val_acc: 0.0000e+00\n",
      "Epoch 15/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.4676 - acc: 0.0000e+00 - val_loss: 0.3620 - val_acc: 0.0000e+00\n",
      "Epoch 16/400\n",
      "80/80 [==============================] - 0s 211us/step - loss: 0.4627 - acc: 0.0000e+00 - val_loss: 0.3570 - val_acc: 0.0000e+00\n",
      "Epoch 17/400\n",
      "80/80 [==============================] - 0s 188us/step - loss: 0.4578 - acc: 0.0000e+00 - val_loss: 0.3521 - val_acc: 0.0000e+00\n",
      "Epoch 18/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.4529 - acc: 0.0000e+00 - val_loss: 0.3475 - val_acc: 0.0000e+00\n",
      "Epoch 19/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4478 - acc: 0.0000e+00 - val_loss: 0.3428 - val_acc: 0.0000e+00\n",
      "Epoch 20/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.4429 - acc: 0.0000e+00 - val_loss: 0.3380 - val_acc: 0.0000e+00\n",
      "Epoch 21/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.4379 - acc: 0.0000e+00 - val_loss: 0.3332 - val_acc: 0.0000e+00\n",
      "Epoch 22/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.4330 - acc: 0.0000e+00 - val_loss: 0.3284 - val_acc: 0.0000e+00\n",
      "Epoch 23/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.4282 - acc: 0.0000e+00 - val_loss: 0.3236 - val_acc: 0.0000e+00\n",
      "Epoch 24/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.4233 - acc: 0.0000e+00 - val_loss: 0.3188 - val_acc: 0.0000e+00\n",
      "Epoch 25/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4186 - acc: 0.0000e+00 - val_loss: 0.3139 - val_acc: 0.0000e+00\n",
      "Epoch 26/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.4138 - acc: 0.0000e+00 - val_loss: 0.3091 - val_acc: 0.0000e+00\n",
      "Epoch 27/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4093 - acc: 0.0000e+00 - val_loss: 0.3042 - val_acc: 0.0000e+00\n",
      "Epoch 28/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4046 - acc: 0.0000e+00 - val_loss: 0.2994 - val_acc: 0.0000e+00\n",
      "Epoch 29/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.4001 - acc: 0.0000e+00 - val_loss: 0.2946 - val_acc: 0.0000e+00\n",
      "Epoch 30/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3959 - acc: 0.0000e+00 - val_loss: 0.2897 - val_acc: 0.0000e+00\n",
      "Epoch 31/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.3918 - acc: 0.0000e+00 - val_loss: 0.2849 - val_acc: 0.0000e+00\n",
      "Epoch 32/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3872 - acc: 0.0000e+00 - val_loss: 0.2807 - val_acc: 0.0000e+00\n",
      "Epoch 33/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3831 - acc: 0.0000e+00 - val_loss: 0.2765 - val_acc: 0.0000e+00\n",
      "Epoch 34/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3790 - acc: 0.0000e+00 - val_loss: 0.2722 - val_acc: 0.0000e+00\n",
      "Epoch 35/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3750 - acc: 0.0000e+00 - val_loss: 0.2680 - val_acc: 0.0000e+00\n",
      "Epoch 36/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3711 - acc: 0.0000e+00 - val_loss: 0.2638 - val_acc: 0.0000e+00\n",
      "Epoch 37/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3671 - acc: 0.0000e+00 - val_loss: 0.2600 - val_acc: 0.0000e+00\n",
      "Epoch 38/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3633 - acc: 0.0000e+00 - val_loss: 0.2563 - val_acc: 0.0000e+00\n",
      "Epoch 39/400\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.3502 - acc: 0.0000e+0 - 0s 187us/step - loss: 0.3596 - acc: 0.0000e+00 - val_loss: 0.2531 - val_acc: 0.0000e+00\n",
      "Epoch 40/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3555 - acc: 0.0000e+00 - val_loss: 0.2500 - val_acc: 0.0000e+00\n",
      "Epoch 41/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3519 - acc: 0.0000e+00 - val_loss: 0.2469 - val_acc: 0.0000e+00\n",
      "Epoch 42/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.3482 - acc: 0.0000e+00 - val_loss: 0.2439 - val_acc: 0.0000e+00\n",
      "Epoch 43/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.3447 - acc: 0.0000e+00 - val_loss: 0.2413 - val_acc: 0.0000e+00\n",
      "Epoch 44/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3411 - acc: 0.0000e+00 - val_loss: 0.2387 - val_acc: 0.0000e+00\n",
      "Epoch 45/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.3376 - acc: 0.0000e+00 - val_loss: 0.2362 - val_acc: 0.0000e+00\n",
      "Epoch 46/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3341 - acc: 0.0000e+00 - val_loss: 0.2337 - val_acc: 0.0000e+00\n",
      "Epoch 47/400\n",
      "80/80 [==============================] - 0s 225us/step - loss: 0.3310 - acc: 0.0000e+00 - val_loss: 0.2312 - val_acc: 0.0000e+00\n",
      "Epoch 48/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.3278 - acc: 0.0000e+00 - val_loss: 0.2287 - val_acc: 0.0000e+00\n",
      "Epoch 49/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.3248 - acc: 0.0000e+00 - val_loss: 0.2265 - val_acc: 0.0000e+00\n",
      "Epoch 50/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.3218 - acc: 0.0000e+00 - val_loss: 0.2246 - val_acc: 0.0000e+00\n",
      "Epoch 51/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3189 - acc: 0.0000e+00 - val_loss: 0.2228 - val_acc: 0.0000e+00\n",
      "Epoch 52/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.3162 - acc: 0.0000e+00 - val_loss: 0.2210 - val_acc: 0.0000e+00\n",
      "Epoch 53/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3136 - acc: 0.0000e+00 - val_loss: 0.2192 - val_acc: 0.0000e+00\n",
      "Epoch 54/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.3109 - acc: 0.0000e+00 - val_loss: 0.2175 - val_acc: 0.0000e+00\n",
      "Epoch 55/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3084 - acc: 0.0000e+00 - val_loss: 0.2158 - val_acc: 0.0000e+00\n",
      "Epoch 56/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3062 - acc: 0.0000e+00 - val_loss: 0.2142 - val_acc: 0.0000e+00\n",
      "Epoch 57/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3040 - acc: 0.0000e+00 - val_loss: 0.2126 - val_acc: 0.0000e+00\n",
      "Epoch 58/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3018 - acc: 0.0000e+00 - val_loss: 0.2114 - val_acc: 0.0000e+00\n",
      "Epoch 59/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2995 - acc: 0.0000e+00 - val_loss: 0.2101 - val_acc: 0.0000e+00\n",
      "Epoch 60/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2975 - acc: 0.0000e+00 - val_loss: 0.2089 - val_acc: 0.0000e+00\n",
      "Epoch 61/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2953 - acc: 0.0000e+00 - val_loss: 0.2078 - val_acc: 0.0000e+00\n",
      "Epoch 62/400\n",
      "80/80 [==============================] - 0s 225us/step - loss: 0.2933 - acc: 0.0000e+00 - val_loss: 0.2069 - val_acc: 0.0000e+00\n",
      "Epoch 63/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2914 - acc: 0.0000e+00 - val_loss: 0.2061 - val_acc: 0.0000e+00\n",
      "Epoch 64/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2894 - acc: 0.0000e+00 - val_loss: 0.2053 - val_acc: 0.0000e+00\n",
      "Epoch 65/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2876 - acc: 0.0000e+00 - val_loss: 0.2046 - val_acc: 0.0000e+00\n",
      "Epoch 66/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2859 - acc: 0.0000e+00 - val_loss: 0.2039 - val_acc: 0.0000e+00\n",
      "Epoch 67/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2840 - acc: 0.0000e+00 - val_loss: 0.2032 - val_acc: 0.0000e+00\n",
      "Epoch 68/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2826 - acc: 0.0000e+00 - val_loss: 0.2025 - val_acc: 0.0000e+00\n",
      "Epoch 69/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2810 - acc: 0.0000e+00 - val_loss: 0.2018 - val_acc: 0.0000e+00\n",
      "Epoch 70/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2793 - acc: 0.0000e+00 - val_loss: 0.2014 - val_acc: 0.0000e+00\n",
      "Epoch 71/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2779 - acc: 0.0000e+00 - val_loss: 0.2011 - val_acc: 0.0000e+00\n",
      "Epoch 72/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2765 - acc: 0.0000e+00 - val_loss: 0.2007 - val_acc: 0.0000e+00\n",
      "Epoch 73/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2749 - acc: 0.0000e+00 - val_loss: 0.2003 - val_acc: 0.0000e+00\n",
      "Epoch 74/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2735 - acc: 0.0000e+00 - val_loss: 0.2000 - val_acc: 0.0000e+00\n",
      "Epoch 75/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2721 - acc: 0.0000e+00 - val_loss: 0.1996 - val_acc: 0.0000e+00\n",
      "Epoch 76/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2708 - acc: 0.0000e+00 - val_loss: 0.1992 - val_acc: 0.0000e+00\n",
      "Epoch 77/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2694 - acc: 0.0000e+00 - val_loss: 0.1989 - val_acc: 0.0000e+00\n",
      "Epoch 78/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2681 - acc: 0.0000e+00 - val_loss: 0.1985 - val_acc: 0.0000e+00\n",
      "Epoch 79/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2669 - acc: 0.0000e+00 - val_loss: 0.1982 - val_acc: 0.0000e+00\n",
      "Epoch 80/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2658 - acc: 0.0000e+00 - val_loss: 0.1978 - val_acc: 0.0000e+00\n",
      "Epoch 81/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2647 - acc: 0.0000e+00 - val_loss: 0.1975 - val_acc: 0.0000e+00\n",
      "Epoch 82/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2635 - acc: 0.0000e+00 - val_loss: 0.1971 - val_acc: 0.0000e+00\n",
      "Epoch 83/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2626 - acc: 0.0000e+00 - val_loss: 0.1968 - val_acc: 0.0000e+00\n",
      "Epoch 84/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2616 - acc: 0.0000e+00 - val_loss: 0.1964 - val_acc: 0.0000e+00\n",
      "Epoch 85/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2605 - acc: 0.0000e+00 - val_loss: 0.1960 - val_acc: 0.0000e+00\n",
      "Epoch 86/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2597 - acc: 0.0000e+00 - val_loss: 0.1957 - val_acc: 0.0000e+00\n",
      "Epoch 87/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2587 - acc: 0.0000e+00 - val_loss: 0.1953 - val_acc: 0.0000e+00\n",
      "Epoch 88/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2580 - acc: 0.0000e+00 - val_loss: 0.1949 - val_acc: 0.0000e+00\n",
      "Epoch 89/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.2569 - acc: 0.0000e+00 - val_loss: 0.1946 - val_acc: 0.0000e+00\n",
      "Epoch 90/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.2561 - acc: 0.0000e+00 - val_loss: 0.1944 - val_acc: 0.0000e+00\n",
      "Epoch 91/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2553 - acc: 0.0000e+00 - val_loss: 0.1942 - val_acc: 0.0000e+00\n",
      "Epoch 92/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.2545 - acc: 0.0000e+00 - val_loss: 0.1940 - val_acc: 0.0000e+00\n",
      "Epoch 93/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2536 - acc: 0.0000e+00 - val_loss: 0.1937 - val_acc: 0.0000e+00\n",
      "Epoch 94/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2528 - acc: 0.0000e+00 - val_loss: 0.1935 - val_acc: 0.0000e+00\n",
      "Epoch 95/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2520 - acc: 0.0000e+00 - val_loss: 0.1933 - val_acc: 0.0000e+00\n",
      "Epoch 96/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.2511 - acc: 0.0000e+00 - val_loss: 0.1931 - val_acc: 0.0000e+00\n",
      "Epoch 97/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2504 - acc: 0.0000e+00 - val_loss: 0.1928 - val_acc: 0.0000e+00\n",
      "Epoch 98/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2495 - acc: 0.0000e+00 - val_loss: 0.1926 - val_acc: 0.0000e+00\n",
      "Epoch 99/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2488 - acc: 0.0000e+00 - val_loss: 0.1923 - val_acc: 0.0000e+00\n",
      "Epoch 100/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.2479 - acc: 0.0000e+00 - val_loss: 0.1922 - val_acc: 0.0000e+00\n",
      "Epoch 101/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2471 - acc: 0.0000e+00 - val_loss: 0.1921 - val_acc: 0.0000e+00\n",
      "Epoch 102/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2463 - acc: 0.0000e+00 - val_loss: 0.1921 - val_acc: 0.0000e+00\n",
      "Epoch 103/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2455 - acc: 0.0000e+00 - val_loss: 0.1920 - val_acc: 0.0000e+00\n",
      "Epoch 104/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2446 - acc: 0.0000e+00 - val_loss: 0.1919 - val_acc: 0.0000e+00\n",
      "Epoch 105/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2438 - acc: 0.0000e+00 - val_loss: 0.1917 - val_acc: 0.0000e+00\n",
      "Epoch 106/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2430 - acc: 0.0000e+00 - val_loss: 0.1916 - val_acc: 0.0000e+00\n",
      "Epoch 107/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2422 - acc: 0.0000e+00 - val_loss: 0.1914 - val_acc: 0.0000e+00\n",
      "Epoch 108/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2414 - acc: 0.0000e+00 - val_loss: 0.1912 - val_acc: 0.0000e+00\n",
      "Epoch 109/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2406 - acc: 0.0000e+00 - val_loss: 0.1911 - val_acc: 0.0000e+00\n",
      "Epoch 110/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2398 - acc: 0.0000e+00 - val_loss: 0.1909 - val_acc: 0.0000e+00\n",
      "Epoch 111/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2390 - acc: 0.0000e+00 - val_loss: 0.1907 - val_acc: 0.0000e+00\n",
      "Epoch 112/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2382 - acc: 0.0000e+00 - val_loss: 0.1905 - val_acc: 0.0000e+00\n",
      "Epoch 113/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2374 - acc: 0.0000e+00 - val_loss: 0.1903 - val_acc: 0.0000e+00\n",
      "Epoch 114/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2366 - acc: 0.0000e+00 - val_loss: 0.1900 - val_acc: 0.0000e+00\n",
      "Epoch 115/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2359 - acc: 0.0000e+00 - val_loss: 0.1897 - val_acc: 0.0000e+00\n",
      "Epoch 116/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2351 - acc: 0.0000e+00 - val_loss: 0.1894 - val_acc: 0.0000e+00\n",
      "Epoch 117/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2343 - acc: 0.0000e+00 - val_loss: 0.1890 - val_acc: 0.0000e+00\n",
      "Epoch 118/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2335 - acc: 0.0000e+00 - val_loss: 0.1886 - val_acc: 0.0000e+00\n",
      "Epoch 119/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2328 - acc: 0.0000e+00 - val_loss: 0.1882 - val_acc: 0.0000e+00\n",
      "Epoch 120/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2320 - acc: 0.0000e+00 - val_loss: 0.1877 - val_acc: 0.0000e+00\n",
      "Epoch 121/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2312 - acc: 0.0000e+00 - val_loss: 0.1872 - val_acc: 0.0000e+00\n",
      "Epoch 122/400\n",
      "80/80 [==============================] - 0s 225us/step - loss: 0.2304 - acc: 0.0000e+00 - val_loss: 0.1867 - val_acc: 0.0500\n",
      "Epoch 123/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2296 - acc: 0.0000e+00 - val_loss: 0.1860 - val_acc: 0.0500\n",
      "Epoch 124/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.2288 - acc: 0.0000e+00 - val_loss: 0.1854 - val_acc: 0.0500\n",
      "Epoch 125/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2279 - acc: 0.0000e+00 - val_loss: 0.1849 - val_acc: 0.0500\n",
      "Epoch 126/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2270 - acc: 0.0000e+00 - val_loss: 0.1842 - val_acc: 0.0500\n",
      "Epoch 127/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2262 - acc: 0.0000e+00 - val_loss: 0.1835 - val_acc: 0.0500\n",
      "Epoch 128/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2252 - acc: 0.0000e+00 - val_loss: 0.1828 - val_acc: 0.0500\n",
      "Epoch 129/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2243 - acc: 0.0000e+00 - val_loss: 0.1821 - val_acc: 0.0500\n",
      "Epoch 130/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2234 - acc: 0.0000e+00 - val_loss: 0.1814 - val_acc: 0.0500\n",
      "Epoch 131/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2224 - acc: 0.0000e+00 - val_loss: 0.1806 - val_acc: 0.0500\n",
      "Epoch 132/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2214 - acc: 0.0000e+00 - val_loss: 0.1799 - val_acc: 0.0500\n",
      "Epoch 133/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2204 - acc: 0.0000e+00 - val_loss: 0.1791 - val_acc: 0.0500\n",
      "Epoch 134/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2194 - acc: 0.0000e+00 - val_loss: 0.1782 - val_acc: 0.0500\n",
      "Epoch 135/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2183 - acc: 0.0000e+00 - val_loss: 0.1774 - val_acc: 0.0500\n",
      "Epoch 136/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2172 - acc: 0.0000e+00 - val_loss: 0.1763 - val_acc: 0.0500\n",
      "Epoch 137/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2161 - acc: 0.0000e+00 - val_loss: 0.1752 - val_acc: 0.0500\n",
      "Epoch 138/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2149 - acc: 0.0000e+00 - val_loss: 0.1741 - val_acc: 0.0500\n",
      "Epoch 139/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2138 - acc: 0.0000e+00 - val_loss: 0.1728 - val_acc: 0.0500\n",
      "Epoch 140/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2126 - acc: 0.0000e+00 - val_loss: 0.1715 - val_acc: 0.0500\n",
      "Epoch 141/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2114 - acc: 0.0000e+00 - val_loss: 0.1704 - val_acc: 0.0500\n",
      "Epoch 142/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2101 - acc: 0.0000e+00 - val_loss: 0.1693 - val_acc: 0.0500\n",
      "Epoch 143/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2088 - acc: 0.0000e+00 - val_loss: 0.1682 - val_acc: 0.0500\n",
      "Epoch 144/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2075 - acc: 0.0000e+00 - val_loss: 0.1670 - val_acc: 0.0500\n",
      "Epoch 145/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2062 - acc: 0.0000e+00 - val_loss: 0.1658 - val_acc: 0.0500\n",
      "Epoch 146/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2047 - acc: 0.0000e+00 - val_loss: 0.1643 - val_acc: 0.0500\n",
      "Epoch 147/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2033 - acc: 0.0000e+00 - val_loss: 0.1626 - val_acc: 0.0500\n",
      "Epoch 148/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2018 - acc: 0.0000e+00 - val_loss: 0.1609 - val_acc: 0.0500\n",
      "Epoch 149/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2003 - acc: 0.0000e+00 - val_loss: 0.1592 - val_acc: 0.0500\n",
      "Epoch 150/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1988 - acc: 0.0000e+00 - val_loss: 0.1574 - val_acc: 0.0500\n",
      "Epoch 151/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1971 - acc: 0.0000e+00 - val_loss: 0.1554 - val_acc: 0.0500\n",
      "Epoch 152/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1953 - acc: 0.0000e+00 - val_loss: 0.1537 - val_acc: 0.0500\n",
      "Epoch 153/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1938 - acc: 0.0000e+00 - val_loss: 0.1516 - val_acc: 0.0500\n",
      "Epoch 154/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1919 - acc: 0.0000e+00 - val_loss: 0.1498 - val_acc: 0.0500\n",
      "Epoch 155/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1901 - acc: 0.0000e+00 - val_loss: 0.1477 - val_acc: 0.0500\n",
      "Epoch 156/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1882 - acc: 0.0000e+00 - val_loss: 0.1458 - val_acc: 0.0500\n",
      "Epoch 157/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1863 - acc: 0.0000e+00 - val_loss: 0.1436 - val_acc: 0.0500\n",
      "Epoch 158/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.1843 - acc: 0.0000e+00 - val_loss: 0.1415 - val_acc: 0.0500\n",
      "Epoch 159/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1822 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0500\n",
      "Epoch 160/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1801 - acc: 0.0000e+00 - val_loss: 0.1367 - val_acc: 0.0500\n",
      "Epoch 161/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1777 - acc: 0.0000e+00 - val_loss: 0.1340 - val_acc: 0.0500\n",
      "Epoch 162/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1756 - acc: 0.0000e+00 - val_loss: 0.1309 - val_acc: 0.0500\n",
      "Epoch 163/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1731 - acc: 0.0000e+00 - val_loss: 0.1279 - val_acc: 0.0500\n",
      "Epoch 164/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1706 - acc: 0.0000e+00 - val_loss: 0.1249 - val_acc: 0.0500\n",
      "Epoch 165/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1218 - val_acc: 0.0500\n",
      "Epoch 166/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1656 - acc: 0.0000e+00 - val_loss: 0.1186 - val_acc: 0.0500\n",
      "Epoch 167/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1628 - acc: 0.0000e+00 - val_loss: 0.1156 - val_acc: 0.0500\n",
      "Epoch 168/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1602 - acc: 0.0000e+00 - val_loss: 0.1125 - val_acc: 0.0500\n",
      "Epoch 169/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1574 - acc: 0.0000e+00 - val_loss: 0.1094 - val_acc: 0.0500\n",
      "Epoch 170/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1545 - acc: 0.0000e+00 - val_loss: 0.1062 - val_acc: 0.0500\n",
      "Epoch 171/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.1515 - acc: 0.0000e+00 - val_loss: 0.1030 - val_acc: 0.0500\n",
      "Epoch 172/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1484 - acc: 0.0000e+00 - val_loss: 0.0997 - val_acc: 0.0500\n",
      "Epoch 173/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1453 - acc: 0.0000e+00 - val_loss: 0.0961 - val_acc: 0.0500\n",
      "Epoch 174/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1420 - acc: 0.0000e+00 - val_loss: 0.0925 - val_acc: 0.0500\n",
      "Epoch 175/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1385 - acc: 0.0000e+00 - val_loss: 0.0890 - val_acc: 0.0500\n",
      "Epoch 176/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1352 - acc: 0.0000e+00 - val_loss: 0.0854 - val_acc: 0.0500\n",
      "Epoch 177/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.1315 - acc: 0.0000e+00 - val_loss: 0.0820 - val_acc: 0.0500\n",
      "Epoch 178/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.1280 - acc: 0.0000e+00 - val_loss: 0.0792 - val_acc: 0.0500\n",
      "Epoch 179/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1253 - acc: 0.0000e+00 - val_loss: 0.0776 - val_acc: 0.0500\n",
      "Epoch 180/400\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1004 - acc: 0.0000e+0 - 0s 237us/step - loss: 0.1224 - acc: 0.0000e+00 - val_loss: 0.0770 - val_acc: 0.0500\n",
      "Epoch 181/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1203 - acc: 0.0000e+00 - val_loss: 0.0772 - val_acc: 0.0500\n",
      "Epoch 182/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1185 - acc: 0.0000e+00 - val_loss: 0.0769 - val_acc: 0.0500\n",
      "Epoch 183/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1166 - acc: 0.0000e+00 - val_loss: 0.0764 - val_acc: 0.0500\n",
      "Epoch 184/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1154 - acc: 0.0000e+00 - val_loss: 0.0762 - val_acc: 0.0500\n",
      "Epoch 185/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1140 - acc: 0.0000e+00 - val_loss: 0.0758 - val_acc: 0.0500\n",
      "Epoch 186/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1130 - acc: 0.0000e+00 - val_loss: 0.0760 - val_acc: 0.0500\n",
      "Epoch 187/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.1118 - acc: 0.0000e+00 - val_loss: 0.0764 - val_acc: 0.0500\n",
      "Epoch 188/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1108 - acc: 0.0000e+00 - val_loss: 0.0765 - val_acc: 0.0500\n",
      "Epoch 189/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1098 - acc: 0.0000e+00 - val_loss: 0.0761 - val_acc: 0.0500\n",
      "Epoch 190/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1089 - acc: 0.0000e+00 - val_loss: 0.0756 - val_acc: 0.0500\n",
      "Epoch 191/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1079 - acc: 0.0000e+00 - val_loss: 0.0752 - val_acc: 0.0500\n",
      "Epoch 192/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1070 - acc: 0.0000e+00 - val_loss: 0.0747 - val_acc: 0.0500\n",
      "Epoch 193/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1063 - acc: 0.0000e+00 - val_loss: 0.0744 - val_acc: 0.0500\n",
      "Epoch 194/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1053 - acc: 0.0000e+00 - val_loss: 0.0737 - val_acc: 0.0500\n",
      "Epoch 195/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1044 - acc: 0.0000e+00 - val_loss: 0.0730 - val_acc: 0.0500\n",
      "Epoch 196/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1036 - acc: 0.0000e+00 - val_loss: 0.0725 - val_acc: 0.0500\n",
      "Epoch 197/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.1029 - acc: 0.0000e+00 - val_loss: 0.0721 - val_acc: 0.0500\n",
      "Epoch 198/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1021 - acc: 0.0000e+00 - val_loss: 0.0718 - val_acc: 0.0500\n",
      "Epoch 199/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.1013 - acc: 0.0000e+00 - val_loss: 0.0716 - val_acc: 0.0500\n",
      "Epoch 200/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.1005 - acc: 0.0000e+00 - val_loss: 0.0713 - val_acc: 0.0500\n",
      "Epoch 201/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0997 - acc: 0.0000e+00 - val_loss: 0.0711 - val_acc: 0.0500\n",
      "Epoch 202/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0989 - acc: 0.0000e+00 - val_loss: 0.0708 - val_acc: 0.0500\n",
      "Epoch 203/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0983 - acc: 0.0000e+00 - val_loss: 0.0705 - val_acc: 0.0500\n",
      "Epoch 204/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0975 - acc: 0.0000e+00 - val_loss: 0.0700 - val_acc: 0.0500\n",
      "Epoch 205/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0967 - acc: 0.0000e+00 - val_loss: 0.0696 - val_acc: 0.0500\n",
      "Epoch 206/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0960 - acc: 0.0000e+00 - val_loss: 0.0691 - val_acc: 0.0500\n",
      "Epoch 207/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0952 - acc: 0.0000e+00 - val_loss: 0.0686 - val_acc: 0.0500\n",
      "Epoch 208/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0945 - acc: 0.0000e+00 - val_loss: 0.0681 - val_acc: 0.0500\n",
      "Epoch 209/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0939 - acc: 0.0000e+00 - val_loss: 0.0678 - val_acc: 0.0500\n",
      "Epoch 210/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0931 - acc: 0.0000e+00 - val_loss: 0.0673 - val_acc: 0.0500\n",
      "Epoch 211/400\n",
      "80/80 [==============================] - 0s 174us/step - loss: 0.0924 - acc: 0.0000e+00 - val_loss: 0.0670 - val_acc: 0.0500\n",
      "Epoch 212/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0916 - acc: 0.0000e+00 - val_loss: 0.0668 - val_acc: 0.0500\n",
      "Epoch 213/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0909 - acc: 0.0000e+00 - val_loss: 0.0667 - val_acc: 0.0500\n",
      "Epoch 214/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0902 - acc: 0.0000e+00 - val_loss: 0.0664 - val_acc: 0.0500\n",
      "Epoch 215/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0895 - acc: 0.0000e+00 - val_loss: 0.0661 - val_acc: 0.0500\n",
      "Epoch 216/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0888 - acc: 0.0000e+00 - val_loss: 0.0657 - val_acc: 0.0500\n",
      "Epoch 217/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0880 - acc: 0.0000e+00 - val_loss: 0.0654 - val_acc: 0.0500\n",
      "Epoch 218/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0874 - acc: 0.0000e+00 - val_loss: 0.0656 - val_acc: 0.0500\n",
      "Epoch 219/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0866 - acc: 0.0000e+00 - val_loss: 0.0654 - val_acc: 0.0500\n",
      "Epoch 220/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0859 - acc: 0.0000e+00 - val_loss: 0.0650 - val_acc: 0.0500\n",
      "Epoch 221/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0851 - acc: 0.0000e+00 - val_loss: 0.0646 - val_acc: 0.0500\n",
      "Epoch 222/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0844 - acc: 0.0000e+00 - val_loss: 0.0639 - val_acc: 0.0500\n",
      "Epoch 223/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0837 - acc: 0.0000e+00 - val_loss: 0.0633 - val_acc: 0.0500\n",
      "Epoch 224/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0830 - acc: 0.0000e+00 - val_loss: 0.0630 - val_acc: 0.0500\n",
      "Epoch 225/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0823 - acc: 0.0000e+00 - val_loss: 0.0627 - val_acc: 0.0500\n",
      "Epoch 226/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0816 - acc: 0.0000e+00 - val_loss: 0.0625 - val_acc: 0.0500\n",
      "Epoch 227/400\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.0809 - acc: 0.0000e+00 - val_loss: 0.0624 - val_acc: 0.0500\n",
      "Epoch 228/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0803 - acc: 0.0000e+00 - val_loss: 0.0622 - val_acc: 0.0500\n",
      "Epoch 229/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0796 - acc: 0.0000e+00 - val_loss: 0.0614 - val_acc: 0.0500\n",
      "Epoch 230/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0788 - acc: 0.0000e+00 - val_loss: 0.0608 - val_acc: 0.0500\n",
      "Epoch 231/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0782 - acc: 0.0000e+00 - val_loss: 0.0602 - val_acc: 0.0500\n",
      "Epoch 232/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0776 - acc: 0.0000e+00 - val_loss: 0.0598 - val_acc: 0.0500\n",
      "Epoch 233/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0769 - acc: 0.0000e+00 - val_loss: 0.0596 - val_acc: 0.0500\n",
      "Epoch 234/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0763 - acc: 0.0000e+00 - val_loss: 0.0592 - val_acc: 0.0500\n",
      "Epoch 235/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0755 - acc: 0.0000e+00 - val_loss: 0.0583 - val_acc: 0.0500\n",
      "Epoch 236/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0749 - acc: 0.0000e+00 - val_loss: 0.0577 - val_acc: 0.0500\n",
      "Epoch 237/400\n",
      "80/80 [==============================] - 0s 225us/step - loss: 0.0743 - acc: 0.0000e+00 - val_loss: 0.0572 - val_acc: 0.0500\n",
      "Epoch 238/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0736 - acc: 0.0000e+00 - val_loss: 0.0570 - val_acc: 0.0500\n",
      "Epoch 239/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0730 - acc: 0.0000e+00 - val_loss: 0.0570 - val_acc: 0.0500\n",
      "Epoch 240/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0723 - acc: 0.0000e+00 - val_loss: 0.0569 - val_acc: 0.0500\n",
      "Epoch 241/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0717 - acc: 0.0000e+00 - val_loss: 0.0563 - val_acc: 0.0500\n",
      "Epoch 242/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0711 - acc: 0.0000e+00 - val_loss: 0.0554 - val_acc: 0.0500\n",
      "Epoch 243/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0705 - acc: 0.0000e+00 - val_loss: 0.0550 - val_acc: 0.0500\n",
      "Epoch 244/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0699 - acc: 0.0000e+00 - val_loss: 0.0546 - val_acc: 0.0500\n",
      "Epoch 245/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0693 - acc: 0.0000e+00 - val_loss: 0.0544 - val_acc: 0.0500\n",
      "Epoch 246/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0687 - acc: 0.0000e+00 - val_loss: 0.0543 - val_acc: 0.0500\n",
      "Epoch 247/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0680 - acc: 0.0000e+00 - val_loss: 0.0538 - val_acc: 0.0500\n",
      "Epoch 248/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0675 - acc: 0.0000e+00 - val_loss: 0.0534 - val_acc: 0.0500\n",
      "Epoch 249/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0669 - acc: 0.0000e+00 - val_loss: 0.0531 - val_acc: 0.0500\n",
      "Epoch 250/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0663 - acc: 0.0000e+00 - val_loss: 0.0529 - val_acc: 0.0500\n",
      "Epoch 251/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0657 - acc: 0.0000e+00 - val_loss: 0.0528 - val_acc: 0.0500\n",
      "Epoch 252/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0652 - acc: 0.0000e+00 - val_loss: 0.0529 - val_acc: 0.0500\n",
      "Epoch 253/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0646 - acc: 0.0000e+00 - val_loss: 0.0527 - val_acc: 0.0500\n",
      "Epoch 254/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0641 - acc: 0.0000e+00 - val_loss: 0.0521 - val_acc: 0.0500\n",
      "Epoch 255/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0635 - acc: 0.0000e+00 - val_loss: 0.0516 - val_acc: 0.0500\n",
      "Epoch 256/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0630 - acc: 0.0000e+00 - val_loss: 0.0514 - val_acc: 0.0500\n",
      "Epoch 257/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0624 - acc: 0.0000e+00 - val_loss: 0.0513 - val_acc: 0.0500\n",
      "Epoch 258/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0619 - acc: 0.0000e+00 - val_loss: 0.0513 - val_acc: 0.0500\n",
      "Epoch 259/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0614 - acc: 0.0000e+00 - val_loss: 0.0512 - val_acc: 0.0500\n",
      "Epoch 260/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0609 - acc: 0.0000e+00 - val_loss: 0.0506 - val_acc: 0.0500\n",
      "Epoch 261/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0603 - acc: 0.0000e+00 - val_loss: 0.0498 - val_acc: 0.0500\n",
      "Epoch 262/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0599 - acc: 0.0000e+00 - val_loss: 0.0495 - val_acc: 0.0500\n",
      "Epoch 263/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0596 - acc: 0.0000e+00 - val_loss: 0.0493 - val_acc: 0.0500\n",
      "Epoch 264/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0591 - acc: 0.0000e+00 - val_loss: 0.0490 - val_acc: 0.0500\n",
      "Epoch 265/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0584 - acc: 0.0000e+00 - val_loss: 0.0489 - val_acc: 0.0500\n",
      "Epoch 266/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0579 - acc: 0.0000e+00 - val_loss: 0.0489 - val_acc: 0.0500\n",
      "Epoch 267/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0575 - acc: 0.0000e+00 - val_loss: 0.0489 - val_acc: 0.0500\n",
      "Epoch 268/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0570 - acc: 0.0000e+00 - val_loss: 0.0484 - val_acc: 0.0500\n",
      "Epoch 269/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0565 - acc: 0.0000e+00 - val_loss: 0.0480 - val_acc: 0.0500\n",
      "Epoch 270/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0560 - acc: 0.0000e+00 - val_loss: 0.0477 - val_acc: 0.0500\n",
      "Epoch 271/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0556 - acc: 0.0000e+00 - val_loss: 0.0474 - val_acc: 0.0500\n",
      "Epoch 272/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0554 - acc: 0.0000e+00 - val_loss: 0.0472 - val_acc: 0.0500\n",
      "Epoch 273/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0549 - acc: 0.0000e+00 - val_loss: 0.0468 - val_acc: 0.0500\n",
      "Epoch 274/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0543 - acc: 0.0000e+00 - val_loss: 0.0466 - val_acc: 0.0500\n",
      "Epoch 275/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0537 - acc: 0.0000e+00 - val_loss: 0.0465 - val_acc: 0.0500\n",
      "Epoch 276/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0535 - acc: 0.0000e+00 - val_loss: 0.0464 - val_acc: 0.0500\n",
      "Epoch 277/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0531 - acc: 0.0000e+00 - val_loss: 0.0460 - val_acc: 0.0500\n",
      "Epoch 278/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0525 - acc: 0.0000e+00 - val_loss: 0.0458 - val_acc: 0.0500\n",
      "Epoch 279/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0522 - acc: 0.0000e+00 - val_loss: 0.0458 - val_acc: 0.0500\n",
      "Epoch 280/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0519 - acc: 0.0000e+00 - val_loss: 0.0456 - val_acc: 0.0500\n",
      "Epoch 281/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0514 - acc: 0.0000e+00 - val_loss: 0.0451 - val_acc: 0.0500\n",
      "Epoch 282/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0509 - acc: 0.0000e+00 - val_loss: 0.0450 - val_acc: 0.0500\n",
      "Epoch 283/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0505 - acc: 0.0000e+00 - val_loss: 0.0447 - val_acc: 0.0500\n",
      "Epoch 284/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0502 - acc: 0.0000e+00 - val_loss: 0.0444 - val_acc: 0.0500\n",
      "Epoch 285/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0498 - acc: 0.0000e+00 - val_loss: 0.0443 - val_acc: 0.0500\n",
      "Epoch 286/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0493 - acc: 0.0000e+00 - val_loss: 0.0440 - val_acc: 0.0500\n",
      "Epoch 287/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0490 - acc: 0.0000e+00 - val_loss: 0.0439 - val_acc: 0.0500\n",
      "Epoch 288/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0486 - acc: 0.0000e+00 - val_loss: 0.0438 - val_acc: 0.0500\n",
      "Epoch 289/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0483 - acc: 0.0000e+00 - val_loss: 0.0437 - val_acc: 0.0500\n",
      "Epoch 290/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0479 - acc: 0.0000e+00 - val_loss: 0.0436 - val_acc: 0.0500\n",
      "Epoch 291/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0475 - acc: 0.0000e+00 - val_loss: 0.0435 - val_acc: 0.0500\n",
      "Epoch 292/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0471 - acc: 0.0000e+00 - val_loss: 0.0434 - val_acc: 0.0500\n",
      "Epoch 293/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0468 - acc: 0.0000e+00 - val_loss: 0.0433 - val_acc: 0.0500\n",
      "Epoch 294/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0465 - acc: 0.0000e+00 - val_loss: 0.0431 - val_acc: 0.0500\n",
      "Epoch 295/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0460 - acc: 0.0000e+00 - val_loss: 0.0430 - val_acc: 0.0500\n",
      "Epoch 296/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0457 - acc: 0.0000e+00 - val_loss: 0.0429 - val_acc: 0.0500\n",
      "Epoch 297/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0453 - acc: 0.0000e+00 - val_loss: 0.0427 - val_acc: 0.0500\n",
      "Epoch 298/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0450 - acc: 0.0000e+00 - val_loss: 0.0426 - val_acc: 0.0500\n",
      "Epoch 299/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0445 - acc: 0.0000e+00 - val_loss: 0.0423 - val_acc: 0.0500\n",
      "Epoch 300/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0445 - acc: 0.0000e+00 - val_loss: 0.0421 - val_acc: 0.0500\n",
      "Epoch 301/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0439 - acc: 0.0000e+00 - val_loss: 0.0420 - val_acc: 0.0500\n",
      "Epoch 302/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0436 - acc: 0.0000e+00 - val_loss: 0.0419 - val_acc: 0.0500\n",
      "Epoch 303/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0434 - acc: 0.0000e+00 - val_loss: 0.0417 - val_acc: 0.0500\n",
      "Epoch 304/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0431 - acc: 0.0000e+00 - val_loss: 0.0415 - val_acc: 0.0500\n",
      "Epoch 305/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0426 - acc: 0.0000e+00 - val_loss: 0.0413 - val_acc: 0.0500\n",
      "Epoch 306/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0424 - acc: 0.0000e+00 - val_loss: 0.0412 - val_acc: 0.0500\n",
      "Epoch 307/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0421 - acc: 0.0000e+00 - val_loss: 0.0410 - val_acc: 0.0500\n",
      "Epoch 308/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0419 - acc: 0.0000e+00 - val_loss: 0.0409 - val_acc: 0.0500\n",
      "Epoch 309/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0417 - acc: 0.0000e+00 - val_loss: 0.0407 - val_acc: 0.0500\n",
      "Epoch 310/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0414 - acc: 0.0000e+00 - val_loss: 0.0406 - val_acc: 0.0500\n",
      "Epoch 311/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0411 - acc: 0.0000e+00 - val_loss: 0.0404 - val_acc: 0.0500\n",
      "Epoch 312/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0406 - acc: 0.0000e+00 - val_loss: 0.0401 - val_acc: 0.0500\n",
      "Epoch 313/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0404 - acc: 0.0000e+00 - val_loss: 0.0399 - val_acc: 0.0500\n",
      "Epoch 314/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0402 - acc: 0.0000e+00 - val_loss: 0.0396 - val_acc: 0.0500\n",
      "Epoch 315/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0399 - acc: 0.0000e+00 - val_loss: 0.0393 - val_acc: 0.0500\n",
      "Epoch 316/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0396 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0500\n",
      "Epoch 317/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0394 - acc: 0.0000e+00 - val_loss: 0.0389 - val_acc: 0.0500\n",
      "Epoch 318/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0391 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0500\n",
      "Epoch 319/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0389 - acc: 0.0000e+00 - val_loss: 0.0386 - val_acc: 0.0500\n",
      "Epoch 320/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0387 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0500\n",
      "Epoch 321/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0385 - acc: 0.0000e+00 - val_loss: 0.0386 - val_acc: 0.0500\n",
      "Epoch 322/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0382 - acc: 0.0000e+00 - val_loss: 0.0385 - val_acc: 0.0500\n",
      "Epoch 323/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0380 - acc: 0.0000e+00 - val_loss: 0.0385 - val_acc: 0.0500\n",
      "Epoch 324/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0377 - acc: 0.0000e+00 - val_loss: 0.0384 - val_acc: 0.0500\n",
      "Epoch 325/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0375 - acc: 0.0000e+00 - val_loss: 0.0383 - val_acc: 0.0500\n",
      "Epoch 326/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0374 - acc: 0.0000e+00 - val_loss: 0.0383 - val_acc: 0.0500\n",
      "Epoch 327/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0372 - acc: 0.0000e+00 - val_loss: 0.0382 - val_acc: 0.0500\n",
      "Epoch 328/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0370 - acc: 0.0000e+00 - val_loss: 0.0380 - val_acc: 0.0500\n",
      "Epoch 329/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0367 - acc: 0.0000e+00 - val_loss: 0.0377 - val_acc: 0.0500\n",
      "Epoch 330/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0363 - acc: 0.0000e+00 - val_loss: 0.0376 - val_acc: 0.0500\n",
      "Epoch 331/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0362 - acc: 0.0000e+00 - val_loss: 0.0374 - val_acc: 0.0500\n",
      "Epoch 332/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0360 - acc: 0.0000e+00 - val_loss: 0.0372 - val_acc: 0.0500\n",
      "Epoch 333/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0357 - acc: 0.0000e+00 - val_loss: 0.0368 - val_acc: 0.0500\n",
      "Epoch 334/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0358 - acc: 0.0000e+00 - val_loss: 0.0367 - val_acc: 0.0500\n",
      "Epoch 335/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0356 - acc: 0.0000e+00 - val_loss: 0.0365 - val_acc: 0.0500\n",
      "Epoch 336/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0352 - acc: 0.0000e+00 - val_loss: 0.0363 - val_acc: 0.0500\n",
      "Epoch 337/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0351 - acc: 0.0000e+00 - val_loss: 0.0368 - val_acc: 0.0500\n",
      "Epoch 338/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0349 - acc: 0.0000e+00 - val_loss: 0.0366 - val_acc: 0.0500\n",
      "Epoch 339/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0347 - acc: 0.0000e+00 - val_loss: 0.0358 - val_acc: 0.0500\n",
      "Epoch 340/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0344 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0500\n",
      "Epoch 341/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0343 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0500\n",
      "Epoch 342/400\n",
      "80/80 [==============================] - 0s 200us/step - loss: 0.0341 - acc: 0.0000e+00 - val_loss: 0.0350 - val_acc: 0.0500\n",
      "Epoch 343/400\n",
      "80/80 [==============================] - 0s 174us/step - loss: 0.0338 - acc: 0.0000e+00 - val_loss: 0.0352 - val_acc: 0.0500\n",
      "Epoch 344/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0336 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0500\n",
      "Epoch 345/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0335 - acc: 0.0000e+00 - val_loss: 0.0352 - val_acc: 0.0500\n",
      "Epoch 346/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0333 - acc: 0.0000e+00 - val_loss: 0.0348 - val_acc: 0.0500\n",
      "Epoch 347/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0332 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0500\n",
      "Epoch 348/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0330 - acc: 0.0000e+00 - val_loss: 0.0343 - val_acc: 0.0500\n",
      "Epoch 349/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0328 - acc: 0.0000e+00 - val_loss: 0.0341 - val_acc: 0.0500\n",
      "Epoch 350/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0327 - acc: 0.0000e+00 - val_loss: 0.0341 - val_acc: 0.0500\n",
      "Epoch 351/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0325 - acc: 0.0000e+00 - val_loss: 0.0345 - val_acc: 0.0500\n",
      "Epoch 352/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0323 - acc: 0.0000e+00 - val_loss: 0.0348 - val_acc: 0.0500\n",
      "Epoch 353/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0322 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0500\n",
      "Epoch 354/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0322 - acc: 0.0000e+00 - val_loss: 0.0351 - val_acc: 0.0500\n",
      "Epoch 355/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0320 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0500\n",
      "Epoch 356/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0317 - acc: 0.0000e+00 - val_loss: 0.0340 - val_acc: 0.0500\n",
      "Epoch 357/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0315 - acc: 0.0000e+00 - val_loss: 0.0335 - val_acc: 0.0500\n",
      "Epoch 358/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0314 - acc: 0.0000e+00 - val_loss: 0.0332 - val_acc: 0.0500\n",
      "Epoch 359/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0312 - acc: 0.0000e+00 - val_loss: 0.0332 - val_acc: 0.0500\n",
      "Epoch 360/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0310 - acc: 0.0000e+00 - val_loss: 0.0334 - val_acc: 0.0500\n",
      "Epoch 361/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0309 - acc: 0.0000e+00 - val_loss: 0.0333 - val_acc: 0.0500\n",
      "Epoch 362/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0308 - acc: 0.0000e+00 - val_loss: 0.0331 - val_acc: 0.0500\n",
      "Epoch 363/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0307 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0500\n",
      "Epoch 364/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0305 - acc: 0.0000e+00 - val_loss: 0.0322 - val_acc: 0.0500\n",
      "Epoch 365/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0305 - acc: 0.0000e+00 - val_loss: 0.0319 - val_acc: 0.0500\n",
      "Epoch 366/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0304 - acc: 0.0000e+00 - val_loss: 0.0319 - val_acc: 0.0500\n",
      "Epoch 367/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0300 - acc: 0.0000e+00 - val_loss: 0.0322 - val_acc: 0.0500\n",
      "Epoch 368/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0300 - acc: 0.0000e+00 - val_loss: 0.0328 - val_acc: 0.0500\n",
      "Epoch 369/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0300 - acc: 0.0000e+00 - val_loss: 0.0322 - val_acc: 0.0500\n",
      "Epoch 370/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0299 - acc: 0.0000e+00 - val_loss: 0.0315 - val_acc: 0.0500\n",
      "Epoch 371/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0295 - acc: 0.0000e+00 - val_loss: 0.0312 - val_acc: 0.0500\n",
      "Epoch 372/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0295 - acc: 0.0000e+00 - val_loss: 0.0310 - val_acc: 0.0500\n",
      "Epoch 373/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0294 - acc: 0.0000e+00 - val_loss: 0.0310 - val_acc: 0.0500\n",
      "Epoch 374/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0292 - acc: 0.0000e+00 - val_loss: 0.0312 - val_acc: 0.0500\n",
      "Epoch 375/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0292 - acc: 0.0000e+00 - val_loss: 0.0315 - val_acc: 0.0500\n",
      "Epoch 376/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0290 - acc: 0.0000e+00 - val_loss: 0.0312 - val_acc: 0.0500\n",
      "Epoch 377/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0288 - acc: 0.0000e+00 - val_loss: 0.0312 - val_acc: 0.0500\n",
      "Epoch 378/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0287 - acc: 0.0000e+00 - val_loss: 0.0308 - val_acc: 0.0500\n",
      "Epoch 379/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0287 - acc: 0.0000e+00 - val_loss: 0.0307 - val_acc: 0.0500\n",
      "Epoch 380/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0285 - acc: 0.0000e+00 - val_loss: 0.0309 - val_acc: 0.0500\n",
      "Epoch 381/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0283 - acc: 0.0000e+00 - val_loss: 0.0306 - val_acc: 0.0500\n",
      "Epoch 382/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0282 - acc: 0.0000e+00 - val_loss: 0.0303 - val_acc: 0.0500\n",
      "Epoch 383/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0283 - acc: 0.0000e+00 - val_loss: 0.0299 - val_acc: 0.0500\n",
      "Epoch 384/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0279 - acc: 0.0000e+00 - val_loss: 0.0304 - val_acc: 0.0500\n",
      "Epoch 385/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0280 - acc: 0.0000e+00 - val_loss: 0.0310 - val_acc: 0.0500\n",
      "Epoch 386/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0279 - acc: 0.0000e+00 - val_loss: 0.0306 - val_acc: 0.0500\n",
      "Epoch 387/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0277 - acc: 0.0000e+00 - val_loss: 0.0299 - val_acc: 0.0500\n",
      "Epoch 388/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0275 - acc: 0.0000e+00 - val_loss: 0.0294 - val_acc: 0.0500\n",
      "Epoch 389/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0274 - acc: 0.0000e+00 - val_loss: 0.0294 - val_acc: 0.0500\n",
      "Epoch 390/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0273 - acc: 0.0000e+00 - val_loss: 0.0293 - val_acc: 0.0500\n",
      "Epoch 391/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0272 - acc: 0.0000e+00 - val_loss: 0.0297 - val_acc: 0.0500\n",
      "Epoch 392/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0272 - acc: 0.0000e+00 - val_loss: 0.0302 - val_acc: 0.0500\n",
      "Epoch 393/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0270 - acc: 0.0000e+00 - val_loss: 0.0296 - val_acc: 0.0500\n",
      "Epoch 394/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0269 - acc: 0.0000e+00 - val_loss: 0.0292 - val_acc: 0.0500\n",
      "Epoch 395/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0270 - acc: 0.0000e+00 - val_loss: 0.0288 - val_acc: 0.0500\n",
      "Epoch 396/400\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.0267 - acc: 0.0000e+00 - val_loss: 0.0295 - val_acc: 0.0500\n",
      "Epoch 397/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.0267 - acc: 0.0000e+00 - val_loss: 0.0295 - val_acc: 0.0500\n",
      "Epoch 398/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0266 - acc: 0.0000e+00 - val_loss: 0.0296 - val_acc: 0.0500\n",
      "Epoch 399/400\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.0266 - acc: 0.0000e+00 - val_loss: 0.0291 - val_acc: 0.0500\n",
      "Epoch 400/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0265 - acc: 0.0000e+00 - val_loss: 0.0279 - val_acc: 0.0500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=400, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23735362],\n",
       "       [0.15980798],\n",
       "       [0.89524114],\n",
       "       [0.20401925],\n",
       "       [0.70231354],\n",
       "       [0.27382064],\n",
       "       [0.5987093 ],\n",
       "       [0.8982782 ],\n",
       "       [0.45840645],\n",
       "       [0.52941734],\n",
       "       [0.48206243],\n",
       "       [0.11093833],\n",
       "       [0.892097  ],\n",
       "       [0.2932052 ],\n",
       "       [0.18106152],\n",
       "       [0.41172668],\n",
       "       [0.18852207],\n",
       "       [0.323652  ],\n",
       "       [0.3889158 ],\n",
       "       [0.6098641 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.predict(x_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2155f36e5f8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(20), results, c=\"r\")\n",
    "plt.scatter(range(20), y_test, c=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2155f402e80>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Variable size input - Simple solution => padding to make inputs of same size or.,.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = [[[(i + j)/100] for i in range(7)] for j in range(100)] #7 input sequence\n",
    "target = [(i+7)/100 for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(Data, dtype=float)\n",
    "target = np.array(target, dtype=float)\n",
    "data.shape\n",
    "target.shape\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size = 0.2, random_state = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 7, 1)              12        \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 24\n",
      "Trainable params: 24\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#RNN Model\n",
    "model = Sequential()\n",
    "model.add(LSTM((1), batch_input_shape=(None, 7, 1), return_sequences=True)) \n",
    "#input (NONE = you don't know input size, batch size 1)\n",
    "model.add(LSTM((1),return_sequences=False))\n",
    "model.compile(loss=\"MAE\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/400\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.6163 - acc: 0.0000e+00 - val_loss: 0.5025 - val_acc: 0.0000e+00\n",
      "Epoch 2/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.6139 - acc: 0.0000e+00 - val_loss: 0.5001 - val_acc: 0.0000e+00\n",
      "Epoch 3/400\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.6115 - acc: 0.0000e+00 - val_loss: 0.4978 - val_acc: 0.0000e+00\n",
      "Epoch 4/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.6091 - acc: 0.0000e+00 - val_loss: 0.4955 - val_acc: 0.0000e+00\n",
      "Epoch 5/400\n",
      "80/80 [==============================] - 0s 225us/step - loss: 0.6068 - acc: 0.0000e+00 - val_loss: 0.4932 - val_acc: 0.0000e+00\n",
      "Epoch 6/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.6045 - acc: 0.0000e+00 - val_loss: 0.4910 - val_acc: 0.0000e+00\n",
      "Epoch 7/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.6022 - acc: 0.0000e+00 - val_loss: 0.4887 - val_acc: 0.0000e+00\n",
      "Epoch 8/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.6000 - acc: 0.0000e+00 - val_loss: 0.4866 - val_acc: 0.0000e+00\n",
      "Epoch 9/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5978 - acc: 0.0000e+00 - val_loss: 0.4844 - val_acc: 0.0000e+00\n",
      "Epoch 10/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5957 - acc: 0.0000e+00 - val_loss: 0.4823 - val_acc: 0.0000e+00\n",
      "Epoch 11/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.5935 - acc: 0.0000e+00 - val_loss: 0.4802 - val_acc: 0.0000e+00\n",
      "Epoch 12/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5914 - acc: 0.0000e+00 - val_loss: 0.4781 - val_acc: 0.0000e+00\n",
      "Epoch 13/400\n",
      "80/80 [==============================] - 0s 225us/step - loss: 0.5893 - acc: 0.0000e+00 - val_loss: 0.4760 - val_acc: 0.0000e+00\n",
      "Epoch 14/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.5873 - acc: 0.0000e+00 - val_loss: 0.4740 - val_acc: 0.0000e+00\n",
      "Epoch 15/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.5852 - acc: 0.0000e+00 - val_loss: 0.4720 - val_acc: 0.0000e+00\n",
      "Epoch 16/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.5832 - acc: 0.0000e+00 - val_loss: 0.4699 - val_acc: 0.0000e+00\n",
      "Epoch 17/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5811 - acc: 0.0000e+00 - val_loss: 0.4679 - val_acc: 0.0000e+00\n",
      "Epoch 18/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.5791 - acc: 0.0000e+00 - val_loss: 0.4659 - val_acc: 0.0000e+00\n",
      "Epoch 19/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5771 - acc: 0.0000e+00 - val_loss: 0.4638 - val_acc: 0.0000e+00\n",
      "Epoch 20/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.5750 - acc: 0.0000e+00 - val_loss: 0.4618 - val_acc: 0.0000e+00\n",
      "Epoch 21/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.5730 - acc: 0.0000e+00 - val_loss: 0.4597 - val_acc: 0.0000e+00\n",
      "Epoch 22/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.5709 - acc: 0.0000e+00 - val_loss: 0.4576 - val_acc: 0.0000e+00\n",
      "Epoch 23/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5688 - acc: 0.0000e+00 - val_loss: 0.4555 - val_acc: 0.0000e+00\n",
      "Epoch 24/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.5667 - acc: 0.0000e+00 - val_loss: 0.4534 - val_acc: 0.0000e+00\n",
      "Epoch 25/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.5645 - acc: 0.0000e+00 - val_loss: 0.4512 - val_acc: 0.0000e+00\n",
      "Epoch 26/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5623 - acc: 0.0000e+00 - val_loss: 0.4490 - val_acc: 0.0000e+00\n",
      "Epoch 27/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.5601 - acc: 0.0000e+00 - val_loss: 0.4467 - val_acc: 0.0000e+00\n",
      "Epoch 28/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5578 - acc: 0.0000e+00 - val_loss: 0.4445 - val_acc: 0.0000e+00\n",
      "Epoch 29/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5555 - acc: 0.0000e+00 - val_loss: 0.4421 - val_acc: 0.0000e+00\n",
      "Epoch 30/400\n",
      "80/80 [==============================] - 0s 275us/step - loss: 0.5532 - acc: 0.0000e+00 - val_loss: 0.4397 - val_acc: 0.0000e+00\n",
      "Epoch 31/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.5508 - acc: 0.0000e+00 - val_loss: 0.4373 - val_acc: 0.0000e+00\n",
      "Epoch 32/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.5484 - acc: 0.0000e+00 - val_loss: 0.4348 - val_acc: 0.0000e+00\n",
      "Epoch 33/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5459 - acc: 0.0000e+00 - val_loss: 0.4323 - val_acc: 0.0000e+00\n",
      "Epoch 34/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.5433 - acc: 0.0000e+00 - val_loss: 0.4297 - val_acc: 0.0000e+00\n",
      "Epoch 35/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.5407 - acc: 0.0000e+00 - val_loss: 0.4271 - val_acc: 0.0000e+00\n",
      "Epoch 36/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.5381 - acc: 0.0000e+00 - val_loss: 0.4244 - val_acc: 0.0000e+00\n",
      "Epoch 37/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.5354 - acc: 0.0000e+00 - val_loss: 0.4217 - val_acc: 0.0000e+00\n",
      "Epoch 38/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.5327 - acc: 0.0000e+00 - val_loss: 0.4189 - val_acc: 0.0000e+00\n",
      "Epoch 39/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.5300 - acc: 0.0000e+00 - val_loss: 0.4161 - val_acc: 0.0000e+00\n",
      "Epoch 40/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.5272 - acc: 0.0000e+00 - val_loss: 0.4132 - val_acc: 0.0000e+00\n",
      "Epoch 41/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.5244 - acc: 0.0000e+00 - val_loss: 0.4103 - val_acc: 0.0000e+00\n",
      "Epoch 42/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5216 - acc: 0.0000e+00 - val_loss: 0.4073 - val_acc: 0.0000e+00\n",
      "Epoch 43/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5187 - acc: 0.0000e+00 - val_loss: 0.4043 - val_acc: 0.0000e+00\n",
      "Epoch 44/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5159 - acc: 0.0000e+00 - val_loss: 0.4013 - val_acc: 0.0000e+00\n",
      "Epoch 45/400\n",
      "80/80 [==============================] - 0s 300us/step - loss: 0.5129 - acc: 0.0000e+00 - val_loss: 0.3984 - val_acc: 0.0000e+00\n",
      "Epoch 46/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.5099 - acc: 0.0000e+00 - val_loss: 0.3956 - val_acc: 0.0000e+00\n",
      "Epoch 47/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.5068 - acc: 0.0000e+00 - val_loss: 0.3926 - val_acc: 0.0000e+00\n",
      "Epoch 48/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.5036 - acc: 0.0000e+00 - val_loss: 0.3896 - val_acc: 0.0000e+00\n",
      "Epoch 49/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.5005 - acc: 0.0000e+00 - val_loss: 0.3865 - val_acc: 0.0000e+00\n",
      "Epoch 50/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.4974 - acc: 0.0000e+00 - val_loss: 0.3834 - val_acc: 0.0000e+00\n",
      "Epoch 51/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.4942 - acc: 0.0000e+00 - val_loss: 0.3803 - val_acc: 0.0000e+00\n",
      "Epoch 52/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.4909 - acc: 0.0000e+00 - val_loss: 0.3771 - val_acc: 0.0000e+00\n",
      "Epoch 53/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.4877 - acc: 0.0000e+00 - val_loss: 0.3739 - val_acc: 0.0000e+00\n",
      "Epoch 54/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4845 - acc: 0.0000e+00 - val_loss: 0.3706 - val_acc: 0.0000e+00\n",
      "Epoch 55/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.4812 - acc: 0.0000e+00 - val_loss: 0.3673 - val_acc: 0.0000e+00\n",
      "Epoch 56/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.4779 - acc: 0.0000e+00 - val_loss: 0.3638 - val_acc: 0.0000e+00\n",
      "Epoch 57/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4745 - acc: 0.0000e+00 - val_loss: 0.3604 - val_acc: 0.0000e+00\n",
      "Epoch 58/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4711 - acc: 0.0000e+00 - val_loss: 0.3568 - val_acc: 0.0000e+00\n",
      "Epoch 59/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.4678 - acc: 0.0000e+00 - val_loss: 0.3532 - val_acc: 0.0000e+00\n",
      "Epoch 60/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.4643 - acc: 0.0000e+00 - val_loss: 0.3496 - val_acc: 0.0000e+00\n",
      "Epoch 61/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.4607 - acc: 0.0000e+00 - val_loss: 0.3459 - val_acc: 0.0000e+00\n",
      "Epoch 62/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4573 - acc: 0.0000e+00 - val_loss: 0.3421 - val_acc: 0.0000e+00\n",
      "Epoch 63/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.4539 - acc: 0.0000e+00 - val_loss: 0.3383 - val_acc: 0.0000e+00\n",
      "Epoch 64/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.4503 - acc: 0.0000e+00 - val_loss: 0.3344 - val_acc: 0.0000e+00\n",
      "Epoch 65/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.4468 - acc: 0.0000e+00 - val_loss: 0.3303 - val_acc: 0.0000e+00\n",
      "Epoch 66/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.4434 - acc: 0.0000e+00 - val_loss: 0.3263 - val_acc: 0.0000e+00\n",
      "Epoch 67/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.4397 - acc: 0.0000e+00 - val_loss: 0.3223 - val_acc: 0.0000e+00\n",
      "Epoch 68/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4361 - acc: 0.0000e+00 - val_loss: 0.3185 - val_acc: 0.0000e+00\n",
      "Epoch 69/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.4325 - acc: 0.0000e+00 - val_loss: 0.3148 - val_acc: 0.0000e+00\n",
      "Epoch 70/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.4288 - acc: 0.0000e+00 - val_loss: 0.3110 - val_acc: 0.0000e+00\n",
      "Epoch 71/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.4250 - acc: 0.0000e+00 - val_loss: 0.3072 - val_acc: 0.0000e+00\n",
      "Epoch 72/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.4214 - acc: 0.0000e+00 - val_loss: 0.3032 - val_acc: 0.0000e+00\n",
      "Epoch 73/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.4177 - acc: 0.0000e+00 - val_loss: 0.2992 - val_acc: 0.0000e+00\n",
      "Epoch 74/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.4141 - acc: 0.0000e+00 - val_loss: 0.2952 - val_acc: 0.0000e+00\n",
      "Epoch 75/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.4101 - acc: 0.0000e+00 - val_loss: 0.2916 - val_acc: 0.0000e+00\n",
      "Epoch 76/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.4064 - acc: 0.0000e+00 - val_loss: 0.2879 - val_acc: 0.0000e+00\n",
      "Epoch 77/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.4024 - acc: 0.0000e+00 - val_loss: 0.2847 - val_acc: 0.0000e+00\n",
      "Epoch 78/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.3983 - acc: 0.0000e+00 - val_loss: 0.2813 - val_acc: 0.0000e+00\n",
      "Epoch 79/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3944 - acc: 0.0000e+00 - val_loss: 0.2779 - val_acc: 0.0000e+00\n",
      "Epoch 80/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.3902 - acc: 0.0000e+00 - val_loss: 0.2745 - val_acc: 0.0000e+00\n",
      "Epoch 81/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3862 - acc: 0.0000e+00 - val_loss: 0.2714 - val_acc: 0.0000e+00\n",
      "Epoch 82/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3820 - acc: 0.0000e+00 - val_loss: 0.2683 - val_acc: 0.0000e+00\n",
      "Epoch 83/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3778 - acc: 0.0000e+00 - val_loss: 0.2652 - val_acc: 0.0000e+00\n",
      "Epoch 84/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.3733 - acc: 0.0000e+00 - val_loss: 0.2620 - val_acc: 0.0000e+00\n",
      "Epoch 85/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.3690 - acc: 0.0000e+00 - val_loss: 0.2588 - val_acc: 0.0000e+00\n",
      "Epoch 86/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3650 - acc: 0.0000e+00 - val_loss: 0.2555 - val_acc: 0.0000e+00\n",
      "Epoch 87/400\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.4259 - acc: 0.0000e+0 - 0s 287us/step - loss: 0.3610 - acc: 0.0000e+00 - val_loss: 0.2523 - val_acc: 0.0000e+00\n",
      "Epoch 88/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.3566 - acc: 0.0000e+00 - val_loss: 0.2495 - val_acc: 0.0000e+00\n",
      "Epoch 89/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.3526 - acc: 0.0000e+00 - val_loss: 0.2467 - val_acc: 0.0000e+00\n",
      "Epoch 90/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.3480 - acc: 0.0000e+00 - val_loss: 0.2440 - val_acc: 0.0000e+00\n",
      "Epoch 91/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3441 - acc: 0.0000e+00 - val_loss: 0.2412 - val_acc: 0.0000e+00\n",
      "Epoch 92/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.3402 - acc: 0.0000e+00 - val_loss: 0.2384 - val_acc: 0.0000e+00\n",
      "Epoch 93/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.3362 - acc: 0.0000e+00 - val_loss: 0.2355 - val_acc: 0.0000e+00\n",
      "Epoch 94/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.3320 - acc: 0.0000e+00 - val_loss: 0.2333 - val_acc: 0.0000e+00\n",
      "Epoch 95/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.3284 - acc: 0.0000e+00 - val_loss: 0.2309 - val_acc: 0.0000e+00\n",
      "Epoch 96/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.3242 - acc: 0.0000e+00 - val_loss: 0.2287 - val_acc: 0.0000e+00\n",
      "Epoch 97/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.3204 - acc: 0.0000e+00 - val_loss: 0.2271 - val_acc: 0.0000e+00\n",
      "Epoch 98/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.3164 - acc: 0.0000e+00 - val_loss: 0.2255 - val_acc: 0.0000e+00\n",
      "Epoch 99/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.3127 - acc: 0.0000e+00 - val_loss: 0.2238 - val_acc: 0.0000e+00\n",
      "Epoch 100/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.3088 - acc: 0.0000e+00 - val_loss: 0.2222 - val_acc: 0.0000e+00\n",
      "Epoch 101/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.3051 - acc: 0.0000e+00 - val_loss: 0.2210 - val_acc: 0.0000e+00\n",
      "Epoch 102/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.3015 - acc: 0.0000e+00 - val_loss: 0.2200 - val_acc: 0.0000e+00\n",
      "Epoch 103/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2979 - acc: 0.0000e+00 - val_loss: 0.2189 - val_acc: 0.0000e+00\n",
      "Epoch 104/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2943 - acc: 0.0000e+00 - val_loss: 0.2179 - val_acc: 0.0000e+00\n",
      "Epoch 105/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.2911 - acc: 0.0000e+00 - val_loss: 0.2169 - val_acc: 0.0000e+00\n",
      "Epoch 106/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.2879 - acc: 0.0000e+00 - val_loss: 0.2159 - val_acc: 0.0000e+00\n",
      "Epoch 107/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2847 - acc: 0.0000e+00 - val_loss: 0.2149 - val_acc: 0.0000e+00\n",
      "Epoch 108/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2817 - acc: 0.0000e+00 - val_loss: 0.2140 - val_acc: 0.0000e+00\n",
      "Epoch 109/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.2791 - acc: 0.0000e+00 - val_loss: 0.2132 - val_acc: 0.0000e+00\n",
      "Epoch 110/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2767 - acc: 0.0000e+00 - val_loss: 0.2129 - val_acc: 0.0000e+00\n",
      "Epoch 111/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2734 - acc: 0.0000e+00 - val_loss: 0.2126 - val_acc: 0.0000e+00\n",
      "Epoch 112/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2711 - acc: 0.0000e+00 - val_loss: 0.2126 - val_acc: 0.0000e+00\n",
      "Epoch 113/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2688 - acc: 0.0000e+00 - val_loss: 0.2129 - val_acc: 0.0000e+00\n",
      "Epoch 114/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2662 - acc: 0.0000e+00 - val_loss: 0.2132 - val_acc: 0.0000e+00\n",
      "Epoch 115/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2637 - acc: 0.0000e+00 - val_loss: 0.2135 - val_acc: 0.0000e+00\n",
      "Epoch 116/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.2617 - acc: 0.0000e+00 - val_loss: 0.2138 - val_acc: 0.0000e+00\n",
      "Epoch 117/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2598 - acc: 0.0000e+00 - val_loss: 0.2141 - val_acc: 0.0000e+00\n",
      "Epoch 118/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.2579 - acc: 0.0000e+00 - val_loss: 0.2146 - val_acc: 0.0000e+00\n",
      "Epoch 119/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.2557 - acc: 0.0000e+00 - val_loss: 0.2153 - val_acc: 0.0000e+00\n",
      "Epoch 120/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2541 - acc: 0.0125 - val_loss: 0.2161 - val_acc: 0.0000e+00\n",
      "Epoch 121/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2525 - acc: 0.0125 - val_loss: 0.2169 - val_acc: 0.0000e+00\n",
      "Epoch 122/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.2507 - acc: 0.0125 - val_loss: 0.2181 - val_acc: 0.0000e+00\n",
      "Epoch 123/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2492 - acc: 0.0125 - val_loss: 0.2193 - val_acc: 0.0000e+00\n",
      "Epoch 124/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2475 - acc: 0.0125 - val_loss: 0.2206 - val_acc: 0.0000e+00\n",
      "Epoch 125/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2460 - acc: 0.0125 - val_loss: 0.2217 - val_acc: 0.0000e+00\n",
      "Epoch 126/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2448 - acc: 0.0125 - val_loss: 0.2228 - val_acc: 0.0000e+00\n",
      "Epoch 127/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2433 - acc: 0.0125 - val_loss: 0.2237 - val_acc: 0.0000e+00\n",
      "Epoch 128/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.2422 - acc: 0.0125 - val_loss: 0.2245 - val_acc: 0.0000e+00\n",
      "Epoch 129/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2412 - acc: 0.0125 - val_loss: 0.2255 - val_acc: 0.0000e+00\n",
      "Epoch 130/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.2400 - acc: 0.0125 - val_loss: 0.2266 - val_acc: 0.0000e+00\n",
      "Epoch 131/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.2390 - acc: 0.0125 - val_loss: 0.2277 - val_acc: 0.0000e+00\n",
      "Epoch 132/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.2380 - acc: 0.0125 - val_loss: 0.2286 - val_acc: 0.0000e+00\n",
      "Epoch 133/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.2370 - acc: 0.0125 - val_loss: 0.2295 - val_acc: 0.0000e+00\n",
      "Epoch 134/400\n",
      "80/80 [==============================] - 0s 300us/step - loss: 0.2359 - acc: 0.0125 - val_loss: 0.2301 - val_acc: 0.0000e+00\n",
      "Epoch 135/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.2349 - acc: 0.0125 - val_loss: 0.2305 - val_acc: 0.0000e+00\n",
      "Epoch 136/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2341 - acc: 0.0125 - val_loss: 0.2308 - val_acc: 0.0000e+00\n",
      "Epoch 137/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.2332 - acc: 0.0125 - val_loss: 0.2311 - val_acc: 0.0000e+00\n",
      "Epoch 138/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2324 - acc: 0.0125 - val_loss: 0.2314 - val_acc: 0.0000e+00\n",
      "Epoch 139/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.2315 - acc: 0.0125 - val_loss: 0.2315 - val_acc: 0.0000e+00\n",
      "Epoch 140/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2306 - acc: 0.0125 - val_loss: 0.2315 - val_acc: 0.0000e+00\n",
      "Epoch 141/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2298 - acc: 0.0125 - val_loss: 0.2315 - val_acc: 0.0000e+00\n",
      "Epoch 142/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.2288 - acc: 0.0125 - val_loss: 0.2311 - val_acc: 0.0000e+00\n",
      "Epoch 143/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.2280 - acc: 0.0125 - val_loss: 0.2309 - val_acc: 0.0000e+00\n",
      "Epoch 144/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2270 - acc: 0.0125 - val_loss: 0.2305 - val_acc: 0.0000e+00\n",
      "Epoch 145/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.2262 - acc: 0.0125 - val_loss: 0.2302 - val_acc: 0.0000e+00\n",
      "Epoch 146/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2251 - acc: 0.0125 - val_loss: 0.2295 - val_acc: 0.0000e+00\n",
      "Epoch 147/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2241 - acc: 0.0125 - val_loss: 0.2287 - val_acc: 0.0000e+00\n",
      "Epoch 148/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2230 - acc: 0.0125 - val_loss: 0.2280 - val_acc: 0.0000e+00\n",
      "Epoch 149/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.2220 - acc: 0.0125 - val_loss: 0.2273 - val_acc: 0.0000e+00\n",
      "Epoch 150/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.2209 - acc: 0.0125 - val_loss: 0.2264 - val_acc: 0.0000e+00\n",
      "Epoch 151/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2197 - acc: 0.0125 - val_loss: 0.2253 - val_acc: 0.0000e+00\n",
      "Epoch 152/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.2185 - acc: 0.0125 - val_loss: 0.2240 - val_acc: 0.0000e+00\n",
      "Epoch 153/400\n",
      "80/80 [==============================] - 0s 275us/step - loss: 0.2171 - acc: 0.0125 - val_loss: 0.2230 - val_acc: 0.0000e+00\n",
      "Epoch 154/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2158 - acc: 0.0125 - val_loss: 0.2219 - val_acc: 0.0000e+00\n",
      "Epoch 155/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2144 - acc: 0.0125 - val_loss: 0.2207 - val_acc: 0.0000e+00\n",
      "Epoch 156/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2129 - acc: 0.0125 - val_loss: 0.2192 - val_acc: 0.0000e+00\n",
      "Epoch 157/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2114 - acc: 0.0125 - val_loss: 0.2175 - val_acc: 0.0000e+00\n",
      "Epoch 158/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2098 - acc: 0.0125 - val_loss: 0.2156 - val_acc: 0.0000e+00\n",
      "Epoch 159/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2080 - acc: 0.0125 - val_loss: 0.2138 - val_acc: 0.0000e+00\n",
      "Epoch 160/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.2063 - acc: 0.0125 - val_loss: 0.2119 - val_acc: 0.0000e+00\n",
      "Epoch 161/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.2044 - acc: 0.0125 - val_loss: 0.2099 - val_acc: 0.0000e+00\n",
      "Epoch 162/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.2025 - acc: 0.0125 - val_loss: 0.2076 - val_acc: 0.0000e+00\n",
      "Epoch 163/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.2006 - acc: 0.0125 - val_loss: 0.2054 - val_acc: 0.0000e+00\n",
      "Epoch 164/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.1984 - acc: 0.0125 - val_loss: 0.2036 - val_acc: 0.0000e+00\n",
      "Epoch 165/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1962 - acc: 0.0125 - val_loss: 0.2015 - val_acc: 0.0000e+00\n",
      "Epoch 166/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.1938 - acc: 0.0125 - val_loss: 0.1993 - val_acc: 0.0000e+00\n",
      "Epoch 167/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1914 - acc: 0.0125 - val_loss: 0.1969 - val_acc: 0.0000e+00\n",
      "Epoch 168/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.1889 - acc: 0.0125 - val_loss: 0.1941 - val_acc: 0.0000e+00\n",
      "Epoch 169/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1862 - acc: 0.0125 - val_loss: 0.1909 - val_acc: 0.0000e+00\n",
      "Epoch 170/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.1836 - acc: 0.0125 - val_loss: 0.1875 - val_acc: 0.0000e+00\n",
      "Epoch 171/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1806 - acc: 0.0125 - val_loss: 0.1844 - val_acc: 0.0000e+00\n",
      "Epoch 172/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1776 - acc: 0.0125 - val_loss: 0.1815 - val_acc: 0.0000e+00\n",
      "Epoch 173/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1746 - acc: 0.0125 - val_loss: 0.1783 - val_acc: 0.0000e+00\n",
      "Epoch 174/400\n",
      "80/80 [==============================] - 0s 225us/step - loss: 0.1713 - acc: 0.0125 - val_loss: 0.1752 - val_acc: 0.0000e+00\n",
      "Epoch 175/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.1679 - acc: 0.0125 - val_loss: 0.1719 - val_acc: 0.0000e+00\n",
      "Epoch 176/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.1644 - acc: 0.0125 - val_loss: 0.1687 - val_acc: 0.0000e+00\n",
      "Epoch 177/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.1609 - acc: 0.0125 - val_loss: 0.1653 - val_acc: 0.0000e+00\n",
      "Epoch 178/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.1572 - acc: 0.0125 - val_loss: 0.1608 - val_acc: 0.0000e+00\n",
      "Epoch 179/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1532 - acc: 0.0125 - val_loss: 0.1560 - val_acc: 0.0000e+00\n",
      "Epoch 180/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.1492 - acc: 0.0125 - val_loss: 0.1513 - val_acc: 0.0000e+00\n",
      "Epoch 181/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1449 - acc: 0.0125 - val_loss: 0.1464 - val_acc: 0.0000e+00\n",
      "Epoch 182/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1408 - acc: 0.0125 - val_loss: 0.1407 - val_acc: 0.0000e+00\n",
      "Epoch 183/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1362 - acc: 0.0125 - val_loss: 0.1349 - val_acc: 0.0000e+00\n",
      "Epoch 184/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1319 - acc: 0.0125 - val_loss: 0.1284 - val_acc: 0.0000e+00\n",
      "Epoch 185/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1273 - acc: 0.0125 - val_loss: 0.1225 - val_acc: 0.0000e+00\n",
      "Epoch 186/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1225 - acc: 0.0125 - val_loss: 0.1172 - val_acc: 0.0000e+00\n",
      "Epoch 187/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.1178 - acc: 0.0125 - val_loss: 0.1120 - val_acc: 0.0000e+00\n",
      "Epoch 188/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.1129 - acc: 0.0125 - val_loss: 0.1067 - val_acc: 0.0000e+00\n",
      "Epoch 189/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.1081 - acc: 0.0125 - val_loss: 0.1016 - val_acc: 0.0000e+00\n",
      "Epoch 190/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.1030 - acc: 0.0125 - val_loss: 0.0957 - val_acc: 0.0000e+00\n",
      "Epoch 191/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0981 - acc: 0.0125 - val_loss: 0.0898 - val_acc: 0.0000e+00\n",
      "Epoch 192/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0930 - acc: 0.0125 - val_loss: 0.0839 - val_acc: 0.0000e+00\n",
      "Epoch 193/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0878 - acc: 0.0125 - val_loss: 0.0777 - val_acc: 0.0000e+00\n",
      "Epoch 194/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.0824 - acc: 0.0125 - val_loss: 0.0709 - val_acc: 0.0000e+00\n",
      "Epoch 195/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0768 - acc: 0.0125 - val_loss: 0.0650 - val_acc: 0.0000e+00\n",
      "Epoch 196/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0721 - acc: 0.0125 - val_loss: 0.0616 - val_acc: 0.0000e+00\n",
      "Epoch 197/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0684 - acc: 0.0125 - val_loss: 0.0593 - val_acc: 0.0000e+00\n",
      "Epoch 198/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0652 - acc: 0.0125 - val_loss: 0.0575 - val_acc: 0.0000e+00\n",
      "Epoch 199/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0628 - acc: 0.0125 - val_loss: 0.0566 - val_acc: 0.0000e+00\n",
      "Epoch 200/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0612 - acc: 0.0125 - val_loss: 0.0560 - val_acc: 0.0000e+00\n",
      "Epoch 201/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0594 - acc: 0.0125 - val_loss: 0.0555 - val_acc: 0.0000e+00\n",
      "Epoch 202/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0582 - acc: 0.0125 - val_loss: 0.0550 - val_acc: 0.0000e+00\n",
      "Epoch 203/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0571 - acc: 0.0125 - val_loss: 0.0548 - val_acc: 0.0000e+00\n",
      "Epoch 204/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0564 - acc: 0.0125 - val_loss: 0.0546 - val_acc: 0.0000e+00\n",
      "Epoch 205/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0555 - acc: 0.0125 - val_loss: 0.0544 - val_acc: 0.0000e+00\n",
      "Epoch 206/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0548 - acc: 0.0125 - val_loss: 0.0541 - val_acc: 0.0000e+00\n",
      "Epoch 207/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0543 - acc: 0.0125 - val_loss: 0.0536 - val_acc: 0.0000e+00\n",
      "Epoch 208/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0539 - acc: 0.0125 - val_loss: 0.0534 - val_acc: 0.0000e+00\n",
      "Epoch 209/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0534 - acc: 0.0125 - val_loss: 0.0533 - val_acc: 0.0000e+00\n",
      "Epoch 210/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0527 - acc: 0.0125 - val_loss: 0.0531 - val_acc: 0.0000e+00\n",
      "Epoch 211/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0522 - acc: 0.0125 - val_loss: 0.0530 - val_acc: 0.0000e+00\n",
      "Epoch 212/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0518 - acc: 0.0125 - val_loss: 0.0528 - val_acc: 0.0000e+00\n",
      "Epoch 213/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0514 - acc: 0.0125 - val_loss: 0.0526 - val_acc: 0.0000e+00\n",
      "Epoch 214/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0510 - acc: 0.0125 - val_loss: 0.0523 - val_acc: 0.0000e+00\n",
      "Epoch 215/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0505 - acc: 0.0125 - val_loss: 0.0520 - val_acc: 0.0000e+00\n",
      "Epoch 216/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.0500 - acc: 0.0125 - val_loss: 0.0516 - val_acc: 0.0000e+00\n",
      "Epoch 217/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0496 - acc: 0.0125 - val_loss: 0.0513 - val_acc: 0.0000e+00\n",
      "Epoch 218/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0492 - acc: 0.0125 - val_loss: 0.0510 - val_acc: 0.0000e+00\n",
      "Epoch 219/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0487 - acc: 0.0125 - val_loss: 0.0506 - val_acc: 0.0000e+00\n",
      "Epoch 220/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0483 - acc: 0.0125 - val_loss: 0.0502 - val_acc: 0.0000e+00\n",
      "Epoch 221/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0479 - acc: 0.0125 - val_loss: 0.0499 - val_acc: 0.0000e+00\n",
      "Epoch 222/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0476 - acc: 0.0125 - val_loss: 0.0496 - val_acc: 0.0000e+00\n",
      "Epoch 223/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0472 - acc: 0.0125 - val_loss: 0.0494 - val_acc: 0.0000e+00\n",
      "Epoch 224/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0468 - acc: 0.0125 - val_loss: 0.0491 - val_acc: 0.0000e+00\n",
      "Epoch 225/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0464 - acc: 0.0125 - val_loss: 0.0489 - val_acc: 0.0000e+00\n",
      "Epoch 226/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0460 - acc: 0.0125 - val_loss: 0.0489 - val_acc: 0.0000e+00\n",
      "Epoch 227/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0456 - acc: 0.0125 - val_loss: 0.0489 - val_acc: 0.0000e+00\n",
      "Epoch 228/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0453 - acc: 0.0125 - val_loss: 0.0489 - val_acc: 0.0000e+00\n",
      "Epoch 229/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0449 - acc: 0.0125 - val_loss: 0.0486 - val_acc: 0.0000e+00\n",
      "Epoch 230/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0446 - acc: 0.0125 - val_loss: 0.0483 - val_acc: 0.0000e+00\n",
      "Epoch 231/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0442 - acc: 0.0125 - val_loss: 0.0479 - val_acc: 0.0000e+00\n",
      "Epoch 232/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0439 - acc: 0.0125 - val_loss: 0.0477 - val_acc: 0.0000e+00\n",
      "Epoch 233/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0436 - acc: 0.0125 - val_loss: 0.0476 - val_acc: 0.0000e+00\n",
      "Epoch 234/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0433 - acc: 0.0125 - val_loss: 0.0474 - val_acc: 0.0000e+00\n",
      "Epoch 235/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0430 - acc: 0.0125 - val_loss: 0.0471 - val_acc: 0.0000e+00\n",
      "Epoch 236/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0427 - acc: 0.0125 - val_loss: 0.0467 - val_acc: 0.0000e+00\n",
      "Epoch 237/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0423 - acc: 0.0125 - val_loss: 0.0460 - val_acc: 0.0000e+00\n",
      "Epoch 238/400\n",
      "80/80 [==============================] - 0s 275us/step - loss: 0.0422 - acc: 0.0125 - val_loss: 0.0454 - val_acc: 0.0000e+00\n",
      "Epoch 239/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0420 - acc: 0.0125 - val_loss: 0.0453 - val_acc: 0.0000e+00\n",
      "Epoch 240/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0416 - acc: 0.0125 - val_loss: 0.0453 - val_acc: 0.0000e+00\n",
      "Epoch 241/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0412 - acc: 0.0125 - val_loss: 0.0454 - val_acc: 0.0000e+00\n",
      "Epoch 242/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.0410 - acc: 0.0125 - val_loss: 0.0457 - val_acc: 0.0000e+00\n",
      "Epoch 243/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0409 - acc: 0.0125 - val_loss: 0.0456 - val_acc: 0.0000e+00\n",
      "Epoch 244/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0406 - acc: 0.0125 - val_loss: 0.0450 - val_acc: 0.0000e+00\n",
      "Epoch 245/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0402 - acc: 0.0125 - val_loss: 0.0446 - val_acc: 0.0000e+00\n",
      "Epoch 246/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0399 - acc: 0.0125 - val_loss: 0.0443 - val_acc: 0.0000e+00\n",
      "Epoch 247/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0397 - acc: 0.0125 - val_loss: 0.0441 - val_acc: 0.0000e+00\n",
      "Epoch 248/400\n",
      "80/80 [==============================] - 0s 275us/step - loss: 0.0394 - acc: 0.0125 - val_loss: 0.0438 - val_acc: 0.0000e+00\n",
      "Epoch 249/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0393 - acc: 0.0125 - val_loss: 0.0434 - val_acc: 0.0000e+00\n",
      "Epoch 250/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.0389 - acc: 0.0125 - val_loss: 0.0434 - val_acc: 0.0000e+00\n",
      "Epoch 251/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0387 - acc: 0.0125 - val_loss: 0.0435 - val_acc: 0.0000e+00\n",
      "Epoch 252/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0384 - acc: 0.0125 - val_loss: 0.0434 - val_acc: 0.0000e+00\n",
      "Epoch 253/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0383 - acc: 0.0125 - val_loss: 0.0432 - val_acc: 0.0000e+00\n",
      "Epoch 254/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0380 - acc: 0.0125 - val_loss: 0.0429 - val_acc: 0.0000e+00\n",
      "Epoch 255/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0378 - acc: 0.0125 - val_loss: 0.0425 - val_acc: 0.0000e+00\n",
      "Epoch 256/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0376 - acc: 0.0125 - val_loss: 0.0421 - val_acc: 0.0000e+00\n",
      "Epoch 257/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0374 - acc: 0.0125 - val_loss: 0.0419 - val_acc: 0.0000e+00\n",
      "Epoch 258/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0373 - acc: 0.0125 - val_loss: 0.0420 - val_acc: 0.0000e+00\n",
      "Epoch 259/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0371 - acc: 0.0125 - val_loss: 0.0418 - val_acc: 0.0000e+00\n",
      "Epoch 260/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0368 - acc: 0.0125 - val_loss: 0.0415 - val_acc: 0.0000e+00\n",
      "Epoch 261/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0367 - acc: 0.0125 - val_loss: 0.0411 - val_acc: 0.0000e+00\n",
      "Epoch 262/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0365 - acc: 0.0125 - val_loss: 0.0412 - val_acc: 0.0000e+00\n",
      "Epoch 263/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0363 - acc: 0.0125 - val_loss: 0.0411 - val_acc: 0.0000e+00\n",
      "Epoch 264/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0361 - acc: 0.0125 - val_loss: 0.0407 - val_acc: 0.0000e+00\n",
      "Epoch 265/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0359 - acc: 0.0125 - val_loss: 0.0407 - val_acc: 0.0000e+00\n",
      "Epoch 266/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0357 - acc: 0.0125 - val_loss: 0.0408 - val_acc: 0.0000e+00\n",
      "Epoch 267/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0356 - acc: 0.0125 - val_loss: 0.0410 - val_acc: 0.0000e+00\n",
      "Epoch 268/400\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0375 - acc: 0.0000e+0 - 0s 237us/step - loss: 0.0355 - acc: 0.0125 - val_loss: 0.0408 - val_acc: 0.0000e+00\n",
      "Epoch 269/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0353 - acc: 0.0125 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 270/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0351 - acc: 0.0125 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 271/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0348 - acc: 0.0125 - val_loss: 0.0392 - val_acc: 0.0000e+00\n",
      "Epoch 272/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0348 - acc: 0.0125 - val_loss: 0.0388 - val_acc: 0.0000e+00\n",
      "Epoch 273/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0346 - acc: 0.0125 - val_loss: 0.0388 - val_acc: 0.0000e+00\n",
      "Epoch 274/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0344 - acc: 0.0125 - val_loss: 0.0387 - val_acc: 0.0000e+00\n",
      "Epoch 275/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0343 - acc: 0.0125 - val_loss: 0.0389 - val_acc: 0.0000e+00\n",
      "Epoch 276/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.0340 - acc: 0.0125 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
      "Epoch 277/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0339 - acc: 0.0125 - val_loss: 0.0387 - val_acc: 0.0000e+00\n",
      "Epoch 278/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0337 - acc: 0.0125 - val_loss: 0.0386 - val_acc: 0.0000e+00\n",
      "Epoch 279/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0336 - acc: 0.0125 - val_loss: 0.0385 - val_acc: 0.0000e+00\n",
      "Epoch 280/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0334 - acc: 0.0125 - val_loss: 0.0384 - val_acc: 0.0000e+00\n",
      "Epoch 281/400\n",
      "80/80 [==============================] - 0s 311us/step - loss: 0.0333 - acc: 0.0125 - val_loss: 0.0384 - val_acc: 0.0000e+00\n",
      "Epoch 282/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0332 - acc: 0.0125 - val_loss: 0.0383 - val_acc: 0.0000e+00\n",
      "Epoch 283/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0332 - acc: 0.0125 - val_loss: 0.0380 - val_acc: 0.0000e+00\n",
      "Epoch 284/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0329 - acc: 0.0125 - val_loss: 0.0380 - val_acc: 0.0000e+00\n",
      "Epoch 285/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0328 - acc: 0.0125 - val_loss: 0.0378 - val_acc: 0.0000e+00\n",
      "Epoch 286/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0326 - acc: 0.0125 - val_loss: 0.0373 - val_acc: 0.0000e+00\n",
      "Epoch 287/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0325 - acc: 0.0125 - val_loss: 0.0367 - val_acc: 0.0000e+00\n",
      "Epoch 288/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0324 - acc: 0.0125 - val_loss: 0.0363 - val_acc: 0.0000e+00\n",
      "Epoch 289/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0323 - acc: 0.0125 - val_loss: 0.0362 - val_acc: 0.0000e+00\n",
      "Epoch 290/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.0321 - acc: 0.0125 - val_loss: 0.0364 - val_acc: 0.0000e+00\n",
      "Epoch 291/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0319 - acc: 0.0125 - val_loss: 0.0366 - val_acc: 0.0000e+00\n",
      "Epoch 292/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0319 - acc: 0.0125 - val_loss: 0.0370 - val_acc: 0.0000e+00\n",
      "Epoch 293/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0319 - acc: 0.0125 - val_loss: 0.0368 - val_acc: 0.0000e+00\n",
      "Epoch 294/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0316 - acc: 0.0125 - val_loss: 0.0362 - val_acc: 0.0000e+00\n",
      "Epoch 295/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0314 - acc: 0.0125 - val_loss: 0.0354 - val_acc: 0.0000e+00\n",
      "Epoch 296/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0314 - acc: 0.0125 - val_loss: 0.0352 - val_acc: 0.0000e+00\n",
      "Epoch 297/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0313 - acc: 0.0125 - val_loss: 0.0351 - val_acc: 0.0000e+00\n",
      "Epoch 298/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0313 - acc: 0.0125 - val_loss: 0.0352 - val_acc: 0.0000e+00\n",
      "Epoch 299/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.0311 - acc: 0.0125 - val_loss: 0.0351 - val_acc: 0.0000e+00\n",
      "Epoch 300/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0309 - acc: 0.0125 - val_loss: 0.0350 - val_acc: 0.0000e+00\n",
      "Epoch 301/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0308 - acc: 0.0125 - val_loss: 0.0349 - val_acc: 0.0000e+00\n",
      "Epoch 302/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0307 - acc: 0.0125 - val_loss: 0.0349 - val_acc: 0.0000e+00\n",
      "Epoch 303/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0306 - acc: 0.0125 - val_loss: 0.0347 - val_acc: 0.0000e+00\n",
      "Epoch 304/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0305 - acc: 0.0125 - val_loss: 0.0349 - val_acc: 0.0000e+00\n",
      "Epoch 305/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0303 - acc: 0.0125 - val_loss: 0.0348 - val_acc: 0.0000e+00\n",
      "Epoch 306/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0302 - acc: 0.0125 - val_loss: 0.0346 - val_acc: 0.0000e+00\n",
      "Epoch 307/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0301 - acc: 0.0125 - val_loss: 0.0344 - val_acc: 0.0000e+00\n",
      "Epoch 308/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0300 - acc: 0.0125 - val_loss: 0.0343 - val_acc: 0.0000e+00\n",
      "Epoch 309/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0299 - acc: 0.0125 - val_loss: 0.0341 - val_acc: 0.0000e+00\n",
      "Epoch 310/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0298 - acc: 0.0125 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
      "Epoch 311/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0297 - acc: 0.0125 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
      "Epoch 312/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0295 - acc: 0.0125 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
      "Epoch 313/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0295 - acc: 0.0125 - val_loss: 0.0337 - val_acc: 0.0000e+00\n",
      "Epoch 314/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0294 - acc: 0.0125 - val_loss: 0.0338 - val_acc: 0.0000e+00\n",
      "Epoch 315/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0293 - acc: 0.0125 - val_loss: 0.0335 - val_acc: 0.0000e+00\n",
      "Epoch 316/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0292 - acc: 0.0125 - val_loss: 0.0334 - val_acc: 0.0000e+00\n",
      "Epoch 317/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0291 - acc: 0.0125 - val_loss: 0.0331 - val_acc: 0.0000e+00\n",
      "Epoch 318/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0290 - acc: 0.0125 - val_loss: 0.0331 - val_acc: 0.0000e+00\n",
      "Epoch 319/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0288 - acc: 0.0125 - val_loss: 0.0333 - val_acc: 0.0000e+00\n",
      "Epoch 320/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0288 - acc: 0.0125 - val_loss: 0.0333 - val_acc: 0.0000e+00\n",
      "Epoch 321/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0287 - acc: 0.0125 - val_loss: 0.0330 - val_acc: 0.0000e+00\n",
      "Epoch 322/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0286 - acc: 0.0125 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 323/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0285 - acc: 0.0125 - val_loss: 0.0324 - val_acc: 0.0000e+00\n",
      "Epoch 324/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0285 - acc: 0.0125 - val_loss: 0.0323 - val_acc: 0.0000e+00\n",
      "Epoch 325/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0284 - acc: 0.0125 - val_loss: 0.0326 - val_acc: 0.0000e+00\n",
      "Epoch 326/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.0283 - acc: 0.0125 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 327/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0282 - acc: 0.0125 - val_loss: 0.0324 - val_acc: 0.0000e+00\n",
      "Epoch 328/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0283 - acc: 0.0125 - val_loss: 0.0319 - val_acc: 0.0000e+00\n",
      "Epoch 329/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0282 - acc: 0.0125 - val_loss: 0.0319 - val_acc: 0.0000e+00\n",
      "Epoch 330/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0281 - acc: 0.0125 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
      "Epoch 331/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0280 - acc: 0.0125 - val_loss: 0.0322 - val_acc: 0.0000e+00\n",
      "Epoch 332/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0279 - acc: 0.0125 - val_loss: 0.0325 - val_acc: 0.0000e+00\n",
      "Epoch 333/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0279 - acc: 0.0125 - val_loss: 0.0324 - val_acc: 0.0000e+00\n",
      "Epoch 334/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0278 - acc: 0.0125 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
      "Epoch 335/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0277 - acc: 0.0125 - val_loss: 0.0317 - val_acc: 0.0000e+00\n",
      "Epoch 336/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0277 - acc: 0.0125 - val_loss: 0.0315 - val_acc: 0.0000e+00\n",
      "Epoch 337/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0277 - acc: 0.0125 - val_loss: 0.0316 - val_acc: 0.0000e+00\n",
      "Epoch 338/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0276 - acc: 0.0125 - val_loss: 0.0317 - val_acc: 0.0000e+00\n",
      "Epoch 339/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0274 - acc: 0.0125 - val_loss: 0.0320 - val_acc: 0.0000e+00\n",
      "Epoch 340/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0275 - acc: 0.0125 - val_loss: 0.0322 - val_acc: 0.0000e+00\n",
      "Epoch 341/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0274 - acc: 0.0125 - val_loss: 0.0320 - val_acc: 0.0000e+00\n",
      "Epoch 342/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0273 - acc: 0.0125 - val_loss: 0.0318 - val_acc: 0.0000e+00\n",
      "Epoch 343/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0272 - acc: 0.0125 - val_loss: 0.0317 - val_acc: 0.0000e+00\n",
      "Epoch 344/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0273 - acc: 0.0125 - val_loss: 0.0315 - val_acc: 0.0000e+00\n",
      "Epoch 345/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0271 - acc: 0.0125 - val_loss: 0.0316 - val_acc: 0.0000e+00\n",
      "Epoch 346/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0270 - acc: 0.0125 - val_loss: 0.0317 - val_acc: 0.0000e+00\n",
      "Epoch 347/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0271 - acc: 0.0125 - val_loss: 0.0316 - val_acc: 0.0000e+00\n",
      "Epoch 348/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0269 - acc: 0.0125 - val_loss: 0.0312 - val_acc: 0.0000e+00\n",
      "Epoch 349/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0270 - acc: 0.0125 - val_loss: 0.0310 - val_acc: 0.0000e+00\n",
      "Epoch 350/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0270 - acc: 0.0125 - val_loss: 0.0311 - val_acc: 0.0000e+00\n",
      "Epoch 351/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0268 - acc: 0.0125 - val_loss: 0.0313 - val_acc: 0.0000e+00\n",
      "Epoch 352/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0268 - acc: 0.0125 - val_loss: 0.0315 - val_acc: 0.0000e+00\n",
      "Epoch 353/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0268 - acc: 0.0125 - val_loss: 0.0317 - val_acc: 0.0000e+00\n",
      "Epoch 354/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0268 - acc: 0.0125 - val_loss: 0.0322 - val_acc: 0.0000e+00\n",
      "Epoch 355/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0269 - acc: 0.0125 - val_loss: 0.0324 - val_acc: 0.0000e+00\n",
      "Epoch 356/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0269 - acc: 0.0125 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
      "Epoch 357/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0268 - acc: 0.0125 - val_loss: 0.0314 - val_acc: 0.0000e+00\n",
      "Epoch 358/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0265 - acc: 0.0125 - val_loss: 0.0307 - val_acc: 0.0000e+00\n",
      "Epoch 359/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0266 - acc: 0.0125 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 360/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0268 - acc: 0.0125 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 361/400\n",
      "80/80 [==============================] - 0s 300us/step - loss: 0.0266 - acc: 0.0125 - val_loss: 0.0306 - val_acc: 0.0000e+00\n",
      "Epoch 362/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0263 - acc: 0.0125 - val_loss: 0.0312 - val_acc: 0.0000e+00\n",
      "Epoch 363/400\n",
      "80/80 [==============================] - 0s 337us/step - loss: 0.0264 - acc: 0.0125 - val_loss: 0.0318 - val_acc: 0.0000e+00\n",
      "Epoch 364/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0265 - acc: 0.0125 - val_loss: 0.0319 - val_acc: 0.0000e+00\n",
      "Epoch 365/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0265 - acc: 0.0125 - val_loss: 0.0314 - val_acc: 0.0000e+00\n",
      "Epoch 366/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0263 - acc: 0.0125 - val_loss: 0.0308 - val_acc: 0.0000e+00\n",
      "Epoch 367/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0262 - acc: 0.0125 - val_loss: 0.0305 - val_acc: 0.0000e+00\n",
      "Epoch 368/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0262 - acc: 0.0125 - val_loss: 0.0306 - val_acc: 0.0000e+00\n",
      "Epoch 369/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0261 - acc: 0.0125 - val_loss: 0.0307 - val_acc: 0.0000e+00\n",
      "Epoch 370/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0261 - acc: 0.0125 - val_loss: 0.0308 - val_acc: 0.0000e+00\n",
      "Epoch 371/400\n",
      "80/80 [==============================] - 0s 312us/step - loss: 0.0261 - acc: 0.0125 - val_loss: 0.0306 - val_acc: 0.0000e+00\n",
      "Epoch 372/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0260 - acc: 0.0125 - val_loss: 0.0307 - val_acc: 0.0000e+00\n",
      "Epoch 373/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0259 - acc: 0.0125 - val_loss: 0.0307 - val_acc: 0.0000e+00\n",
      "Epoch 374/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0263 - acc: 0.0125 - val_loss: 0.0309 - val_acc: 0.0000e+00\n",
      "Epoch 375/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0259 - acc: 0.0125 - val_loss: 0.0305 - val_acc: 0.0000e+00\n",
      "Epoch 376/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0259 - acc: 0.0125 - val_loss: 0.0298 - val_acc: 0.0000e+00\n",
      "Epoch 377/400\n",
      "80/80 [==============================] - 0s 324us/step - loss: 0.0263 - acc: 0.0125 - val_loss: 0.0296 - val_acc: 0.0000e+00\n",
      "Epoch 378/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0261 - acc: 0.0125 - val_loss: 0.0301 - val_acc: 0.0000e+00\n",
      "Epoch 379/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0258 - acc: 0.0125 - val_loss: 0.0307 - val_acc: 0.0000e+00\n",
      "Epoch 380/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0258 - acc: 0.0125 - val_loss: 0.0309 - val_acc: 0.0000e+00\n",
      "Epoch 381/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0258 - acc: 0.0125 - val_loss: 0.0306 - val_acc: 0.0000e+00\n",
      "Epoch 382/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0259 - acc: 0.0125 - val_loss: 0.0303 - val_acc: 0.0000e+00\n",
      "Epoch 383/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0256 - acc: 0.0125 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 384/400\n",
      "80/80 [==============================] - 0s 250us/step - loss: 0.0256 - acc: 0.0125 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 385/400\n",
      "80/80 [==============================] - 0s 261us/step - loss: 0.0256 - acc: 0.0125 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 386/400\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.0256 - acc: 0.0125 - val_loss: 0.0303 - val_acc: 0.0000e+00\n",
      "Epoch 387/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0256 - acc: 0.0125 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
      "Epoch 388/400\n",
      "80/80 [==============================] - 0s 286us/step - loss: 0.0254 - acc: 0.0125 - val_loss: 0.0297 - val_acc: 0.0000e+00\n",
      "Epoch 389/400\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.0254 - acc: 0.0125 - val_loss: 0.0298 - val_acc: 0.0000e+00\n",
      "Epoch 390/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0254 - acc: 0.0125 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
      "Epoch 391/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0253 - acc: 0.0125 - val_loss: 0.0300 - val_acc: 0.0000e+00\n",
      "Epoch 392/400\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.0253 - acc: 0.0125 - val_loss: 0.0300 - val_acc: 0.0000e+00\n",
      "Epoch 393/400\n",
      "80/80 [==============================] - 0s 299us/step - loss: 0.0253 - acc: 0.0125 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
      "Epoch 394/400\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.0252 - acc: 0.0125 - val_loss: 0.0296 - val_acc: 0.0000e+00\n",
      "Epoch 395/400\n",
      "80/80 [==============================] - 0s 236us/step - loss: 0.0252 - acc: 0.0125 - val_loss: 0.0294 - val_acc: 0.0000e+00\n",
      "Epoch 396/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0253 - acc: 0.0125 - val_loss: 0.0292 - val_acc: 0.0000e+00\n",
      "Epoch 397/400\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.0253 - acc: 0.0125 - val_loss: 0.0294 - val_acc: 0.0000e+00\n",
      "Epoch 398/400\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.0251 - acc: 0.0125 - val_loss: 0.0298 - val_acc: 0.0000e+00\n",
      "Epoch 399/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0251 - acc: 0.0125 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
      "Epoch 400/400\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.0252 - acc: 0.0125 - val_loss: 0.0297 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=400, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2635799 ],\n",
       "       [0.19971047],\n",
       "       [0.936014  ],\n",
       "       [0.23583236],\n",
       "       [0.7174888 ],\n",
       "       [0.2945104 ],\n",
       "       [0.6037762 ],\n",
       "       [0.9377012 ],\n",
       "       [0.46202624],\n",
       "       [0.53206444],\n",
       "       [0.48498392],\n",
       "       [0.16047183],\n",
       "       [0.9340506 ],\n",
       "       [0.31121537],\n",
       "       [0.2169867 ],\n",
       "       [0.41777542],\n",
       "       [0.22308888],\n",
       "       [0.33784434],\n",
       "       [0.39664063],\n",
       "       [0.61564386]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.predict(x_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x215638f4c50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(20), results, c=\"r\")\n",
    "plt.scatter(range(20), y_test, c=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27, 0.17, 1.03, 0.23, 0.7 , 0.31, 0.6 , 1.04, 0.48, 0.54, 0.5 ,\n",
       "       0.09, 1.02, 0.33, 0.2 , 0.44, 0.21, 0.36, 0.42, 0.61])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2635799 ],\n",
       "       [0.19971047],\n",
       "       [0.936014  ],\n",
       "       [0.23583236],\n",
       "       [0.7174888 ],\n",
       "       [0.2945104 ],\n",
       "       [0.6037762 ],\n",
       "       [0.9377012 ],\n",
       "       [0.46202624],\n",
       "       [0.53206444],\n",
       "       [0.48498392],\n",
       "       [0.16047183],\n",
       "       [0.9340506 ],\n",
       "       [0.31121537],\n",
       "       [0.2169867 ],\n",
       "       [0.41777542],\n",
       "       [0.22308888],\n",
       "       [0.33784434],\n",
       "       [0.39664063],\n",
       "       [0.61564386]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Predictions\n",
      "0      0.263580\n",
      "1      0.199710\n",
      "2      0.936014\n",
      "3      0.235832\n",
      "4      0.717489\n",
      "5      0.294510\n",
      "6      0.603776\n",
      "7      0.937701\n",
      "8      0.462026\n",
      "9      0.532064\n",
      "10     0.484984\n",
      "11     0.160472\n",
      "12     0.934051\n",
      "13     0.311215\n",
      "14     0.216987\n",
      "15     0.417775\n",
      "16     0.223089\n",
      "17     0.337844\n",
      "18     0.396641\n",
      "19     0.615644\n"
     ]
    }
   ],
   "source": [
    "preds = pd.DataFrame({'Predictions':results[:,0]})\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    20.000000\n",
      "mean      0.029730\n",
      "std       0.031187\n",
      "min       0.003776\n",
      "25%       0.011801\n",
      "50%       0.017731\n",
      "75%       0.024947\n",
      "max       0.102299\n",
      "Name: Error, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Error</th>\n",
       "      <th>RPrediction</th>\n",
       "      <th>RError</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.263580</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.000001e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.199710</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.029710</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.936014</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.093986</td>\n",
       "      <td>0.94</td>\n",
       "      <td>9.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.235832</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.005832</td>\n",
       "      <td>0.24</td>\n",
       "      <td>9.999995e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.717489</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.017489</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.000003e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.294510</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.015490</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.000001e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.603776</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.384186e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.937701</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.462026</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.017974</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.999999e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.532064</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.007936</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.000003e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.484984</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2.000001e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.160472</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.070472</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.934051</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.085949</td>\n",
       "      <td>0.93</td>\n",
       "      <td>8.999999e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.311215</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.018785</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.216987</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.016987</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.417775</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.022225</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.000001e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.223089</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.013089</td>\n",
       "      <td>0.22</td>\n",
       "      <td>9.999999e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.337844</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.022156</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.396641</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.999999e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.615644</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.000000e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prediction  Actual     Error  RPrediction        RError\n",
       "0     0.263580    0.27  0.006420         0.26  1.000001e-02\n",
       "1     0.199710    0.17  0.029710         0.20  3.000000e-02\n",
       "2     0.936014    1.03  0.093986         0.94  9.000000e-02\n",
       "3     0.235832    0.23  0.005832         0.24  9.999995e-03\n",
       "4     0.717489    0.70  0.017489         0.72  2.000003e-02\n",
       "5     0.294510    0.31  0.015490         0.29  2.000001e-02\n",
       "6     0.603776    0.60  0.003776         0.60  2.384186e-08\n",
       "7     0.937701    1.04  0.102299         0.94  1.000000e-01\n",
       "8     0.462026    0.48  0.017974         0.46  1.999999e-02\n",
       "9     0.532064    0.54  0.007936         0.53  1.000003e-02\n",
       "10    0.484984    0.50  0.015016         0.48  2.000001e-02\n",
       "11    0.160472    0.09  0.070472         0.16  7.000000e-02\n",
       "12    0.934051    1.02  0.085949         0.93  8.999999e-02\n",
       "13    0.311215    0.33  0.018785         0.31  2.000000e-02\n",
       "14    0.216987    0.20  0.016987         0.22  2.000000e-02\n",
       "15    0.417775    0.44  0.022225         0.42  2.000001e-02\n",
       "16    0.223089    0.21  0.013089         0.22  9.999999e-03\n",
       "17    0.337844    0.36  0.022156         0.34  2.000000e-02\n",
       "18    0.396641    0.42  0.023359         0.40  1.999999e-02\n",
       "19    0.615644    0.61  0.005644         0.62  1.000000e-02"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting predictions\n",
    "p_df = pd.DataFrame()\n",
    "p_df['Prediction'] = preds[\"Predictions\"]\n",
    "p_df['Actual'] = y_test\n",
    "p_df['Error'] = np.abs(p_df['Actual'] - p_df['Prediction'])\n",
    "p_df['RPrediction'] = np.round(results,2) \n",
    "p_df['RError'] = np.abs(p_df['Actual'] - p_df['RPrediction'])\n",
    "\n",
    "print(p_df['Error'].describe())\n",
    "\n",
    "p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21563bd6b70>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
